import os
import time
import io
import zipfile
import warnings
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import polars as pl
import duckdb
import streamlit as st

from concurrent.futures import ThreadPoolExecutor, as_completed

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

EXCEL_ROW_LIMIT = 1_000_000

class HighVolumeAutoPartsCatalog:
    def __init__(self):
        self.data_dir = Path("./auto_parts_data")
        self.data_dir.mkdir(exist_ok=True)
        self.db_path = self.data_dir / "catalog.duckdb"
        self.conn = duckdb.connect(database=str(self.db_path))
        self.setup_database()

        st.set_page_config(
            page_title="AutoParts Catalog 10M+", 
            layout="wide",
            page_icon="üöó"
        )

    def setup_database(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE, 
                width DOUBLE,
                height DOUBLE, 
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                —Ü–µ–Ω–∞ DOUBLE DEFAULT NULL,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS settings (
                key VARCHAR PRIMARY KEY,
                value FLOAT
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS brand_markups (
                brand_norm VARCHAR PRIMARY KEY,
                markup FLOAT
            )
        """)

    def create_indexes(self):
        st.info("–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞...")
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_oe_data_oe ON oe_data(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_parts_data_keys ON parts_data(artikul_norm, brand_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_oe ON cross_references(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_artikul ON cross_references(artikul_norm, brand_norm)",
        ]
        for index_sql in indexes:
            self.conn.execute(index_sql)
        st.success("–ò–Ω–¥–µ–∫—Å—ã —Å–æ–∑–¥–∞–Ω—ã.")

    @staticmethod
    def normalize_key(key_series: pl.Series) -> pl.Series:
        return (
            key_series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
            .str.to_lowercase()
        )

    @staticmethod
    def clean_values(value_series: pl.Series) -> pl.Series:
        return (
            value_series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
        )

    @staticmethod
    def determine_category_vectorized(name_series: pl.Series) -> pl.Series:
        categories_map = {
            '–§–∏–ª—å—Ç—Ä': '—Ñ–∏–ª—å—Ç—Ä|filter', '–¢–æ—Ä–º–æ–∑–∞': '—Ç–æ—Ä–º–æ–∑|brake|–∫–æ–ª–æ–¥–∫|–¥–∏—Å–∫|—Å—É–ø–ø–æ—Ä—Ç',
            '–ü–æ–¥–≤–µ—Å–∫–∞': '–∞–º–æ—Ä—Ç–∏–∑–∞—Ç–æ—Ä|—Å—Ç–æ–π–∫|spring|–ø–æ–¥–≤–µ—Å–∫|—Ä—ã—á–∞–≥', '–î–≤–∏–≥–∞—Ç–µ–ª—å': '–¥–≤–∏–≥–∞—Ç–µ–ª—å|engine|—Å–≤–µ—á|–ø–æ—Ä—à–µ–Ω—å|–∫–ª–∞–ø–∞–Ω',
            '–¢—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è': '—Ç—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è|—Å—Ü–µ–ø–ª–µ–Ω|–∫–æ—Ä–æ–±–∫|transmission', '–≠–ª–µ–∫—Ç—Ä–∏–∫–∞': '–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä|–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä|—Å—Ç–∞—Ä—Ç–µ—Ä|–ø—Ä–æ–≤–æ–¥|–ª–∞–º–ø',
            '–†—É–ª–µ–≤–æ–µ': '—Ä—É–ª–µ–≤–æ–π|—Ç—è–≥–∞|–Ω–∞–∫–æ–Ω–µ—á–Ω–∏–∫|steering', '–í—ã–ø—É—Å–∫': '–≥–ª—É—à–∏—Ç–µ–ª—å|–∫–∞—Ç–∞–ª–∏–∑–∞—Ç–æ—Ä|–≤—ã—Ö–ª–æ–ø|exhaust',
            '–û—Ö–ª–∞–∂–¥–µ–Ω–∏–µ': '—Ä–∞–¥–∏–∞—Ç–æ—Ä|–≤–µ–Ω—Ç–∏–ª—è—Ç–æ—Ä|—Ç–µ—Ä–º–æ—Å—Ç–∞—Ç|cooling', '–¢–æ–ø–ª–∏–≤–æ': '—Ç–æ–ø–ª–∏–≤–Ω—ã–π|–±–µ–Ω–∑–æ–Ω–∞—Å–æ—Å|—Ñ–æ—Ä—Å—É–Ω–∫|fuel'
        }
        name_lower = name_series.str.to_lowercase()
        categorization_expr = pl.when(pl.lit(False)).then(pl.lit(None))
        for category, pattern in categories_map.items():
            categorization_expr = categorization_expr.when(name_lower.str.contains(pattern)).then(pl.lit(category))
        return categorization_expr.otherwise(pl.lit('–†–∞–∑–Ω–æ–µ')).alias('category')

    def detect_columns(self, actual_columns: List[str], expected_columns: List[str]) -> Dict[str, str]:
        mapping = {}
        column_variants = {
            'oe_number': ['oe –Ω–æ–º–µ—Ä', 'oe', '–æe', '–Ω–æ–º–µ—Ä', 'code', 'OE'], 
            'artikul': ['–∞—Ä—Ç–∏–∫—É–ª', 'article', 'sku'],
            'brand': ['–±—Ä–µ–Ω–¥', 'brand', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', 'manufacturer'], 
            'name': ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ', 'name', '–æ–ø–∏—Å–∞–Ω–∏–µ', 'description'],
            'applicability': ['–ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', 'vehicle', 'applicability'], 
            'barcode': ['—à—Ç—Ä–∏—Ö-–∫–æ–¥', 'barcode', '—à—Ç—Ä–∏—Ö–∫–æ–¥', 'ean', 'eac13'],
            'multiplicity': ['–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å —à—Ç', '–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å', 'multiplicity'], 
            'length': ['–¥–ª–∏–Ω–∞ (—Å–º)', '–¥–ª–∏–Ω–∞', 'length', '–¥–ª–∏–Ω–Ω–∞'],
            'width': ['—à–∏—Ä–∏–Ω–∞ (—Å–º)', '—à–∏—Ä–∏–Ω–∞', 'width'], 
            'height': ['–≤—ã—Å–æ—Ç–∞ (—Å–º)', '–≤—ã—Å–æ—Ç–∞', 'height'],
            'weight': ['–≤–µ—Å (–∫–≥)', '–≤–µ—Å, –∫–≥', '–≤–µ—Å', 'weight'], 
            'image_url': ['—Å—Å—ã–ª–∫–∞', 'url', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 'image', '–∫–∞—Ä—Ç–∏–Ω–∫–∞'],
            'dimensions_str': ['–≤–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã', '—Ä–∞–∑–º–µ—Ä—ã', 'dimensions', 'size']
        }
        actual_lower = {col.lower(): col for col in actual_columns}
        for expected in expected_columns:
            variants = [v.lower() for v in column_variants.get(expected, [expected])]
            for variant in variants:
                for actual_l, actual_orig in actual_lower.items():
                    if variant in actual_l:
                        mapping[actual_orig] = expected
                        break
                if expected in mapping.values():
                    break
        return mapping

    def read_and_prepare_file(self, file_path: str, file_type: str) -> pl.DataFrame:
        logger.info(f"–ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞: {file_type} ({file_path})")
        try:
            if not os.path.exists(file_path):
                logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                return pl.DataFrame()
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logger.warning(f"–§–∞–π–ª –ø—É—Å—Ç: {file_path}")
                return pl.DataFrame()
            df = pl.read_excel(file_path, engine='calamine')
            if df.is_empty():
                logger.warning(f"–§–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω, –Ω–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã—Ö: {file_path}")
                return pl.DataFrame()
        except Exception as e:
            logger.exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {file_path}: {e}")
            return pl.DataFrame()

        schemas = {
            'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
            'barcode': ['brand', 'artikul', 'barcode', 'multiplicity'],
            'dimensions': ['artikul', 'brand', 'length', 'width', 'height', 'weight', 'dimensions_str'],
            'images': ['artikul', 'brand', 'image_url'],
            'cross': ['oe_number', 'artikul', 'brand']
        }
        expected_cols = schemas.get(file_type, [])
        column_mapping = self.detect_columns(df.columns, expected_cols)
        if not column_mapping:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ñ–∞–π–ª–∞ {file_type}. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {df.columns}")
            return pl.DataFrame()
        df = df.rename(column_mapping)
        # –û—á–∏—Å—Ç–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π
        if 'artikul' in df.columns:
            df = df.with_columns(artikul=self.clean_values(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand=self.clean_values(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number=self.clean_values(pl.col('oe_number')))
        key_cols = [col for col in ['oe_number', 'artikul', 'brand'] if col in df.columns]
        if key_cols:
            df = df.unique(subset=key_cols, keep='first')
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if 'artikul' in df.columns:
            df = df.with_columns(artikul_norm=self.normalize_key(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand_norm=self.normalize_key(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number_norm=self.normalize_key(pl.col('oe_number')))
        return df

    def upsert_data(self, table_name: str, df: pl.DataFrame, pk: List[str]):
        if df.is_empty():
            return
        df = df.unique(keep='first')
        cols = df.columns
        pk_str = ", ".join(f'"{c}"' for c in pk)
        temp_view_name = f"temp_{table_name}_{int(time.time())}"
        self.conn.register(temp_view_name, df.to_arrow())
        update_cols = [col for col in cols if col not in pk]
        if not update_cols:
            on_conflict_action = "DO NOTHING"
        else:
            update_clause = ", ".join([f'"{col}" = excluded."{col}"' for col in update_cols])
            on_conflict_action = f"DO UPDATE SET {update_clause}"

        sql = f"""
        INSERT INTO {table_name}
        SELECT * FROM {temp_view_name}
        ON CONFLICT ({pk_str}) {on_conflict_action};
        """
        try:
            self.conn.execute(sql)
            logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–æ/–≤—Å—Ç–∞–≤–ª–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ UPSERT –≤ {table_name}: {e}")
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}. –î–µ—Ç–∞–ª–∏ –≤ –ª–æ–≥–µ.")
        finally:
            self.conn.unregister(temp_view_name)

    def process_and_load(self, dataframes: Dict[str, pl.DataFrame]):
        st.info("üîÑ –ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑–µ...")
        steps = [s for s in ['oe', 'cross', 'parts'] if s in dataframes or s == 'parts']
        num_steps = len(steps)
        progress_bar = st.progress(0, text="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        step_counter = 0
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ oe
        if 'oe' in dataframes:
            step_counter += 1
            progress_bar.progress(step_counter / (num_steps + 1), text=f"({step_counter}/{num_steps}) –û–±—Ä–∞–±–æ—Ç–∫–∞ OE –¥–∞–Ω–Ω—ã—Ö...")
            df = dataframes['oe'].filter(pl.col('oe_number_norm') != "")
            oe_df = df.select(['oe_number_norm', 'oe_number', 'name', 'applicability']).unique(subset=['oe_number_norm'], keep='first')
            if 'name' in oe_df.columns:
                oe_df = oe_df.with_columns(self.determine_category_vectorized(pl.col('name')))
            else:
                oe_df = oe_df.with_columns(category=pl.lit('–†–∞–∑–Ω–æ–µ'))
            self.upsert_data('oe_data', oe_df, ['oe_number_norm'])
            cross_df_from_oe = df.filter(pl.col('artikul_norm') != "").select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df_from_oe, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ cross
        if 'cross' in dataframes:
            step_counter += 1
            progress_bar.progress(step_counter / (num_steps + 1), text=f"({step_counter}/{num_steps}) –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫—Ä–æ—Å—Å–æ–≤...")
            df = dataframes['cross'].filter((pl.col('oe_number_norm') != "") & (pl.col('artikul_norm') != ""))
            cross_df_from_cross = df.select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df_from_cross, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ parts
        step_counter += 1
        progress_bar.progress(step_counter / (num_steps + 1), text=f"({step_counter}/{num_steps}) –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–µ—Ç–∞–ª–µ–π...")

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∞—Ä—Ç–∏–∫—É–ª–∞ –∏ –±—Ä–µ–Ω–¥—ã –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ —Ç–∏–ø–∞
        file_priority = ['oe', 'barcode', 'images', 'dimensions']
        key_files = {ftype: df for ftype, df in dataframes.items() if ftype in file_priority}
        if key_files:
            all_parts = pl.concat([
                df.select(['artikul', 'artikul_norm', 'brand', 'brand_norm']) 
                for ftype, df in key_files.items() if 'artikul_norm' in df.columns and 'brand_norm' in df.columns
            ]).filter(pl.col('artikul_norm') != "").unique(subset=['artikul_norm', 'brand_norm'], keep='first')
            parts_df = all_parts
            for ftype in file_priority:
                if ftype not in key_files: continue
                df = key_files[ftype]
                if df.is_empty() or 'artikul_norm' not in df.columns:
                    continue
                join_cols = [col for col in df.columns if col not in ['artikul', 'artikul_norm', 'brand', 'brand_norm']]
                if not join_cols:
                    continue
                existing_cols = set(parts_df.columns)
                join_cols = [col for col in join_cols if col not in existing_cols]
                if not join_cols:
                    continue
                df_subset = df.select(['artikul_norm', 'brand_norm'] + join_cols).unique(subset=['artikul_norm', 'brand_norm'], keep='first')
                parts_df = parts_df.join(df_subset, on=['artikul_norm', 'brand_norm'], how='left', coalesce=True)

        if 'parts_df' in locals() and not parts_df.is_empty():
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ dimensions –∏ description
            if 'multiplicity' not in parts_df.columns:
                parts_df = parts_df.with_columns(multiplicity=pl.lit(1).cast(pl.Int32))
            else:
                parts_df = parts_df.with_columns(pl.col('multiplicity').fill_null(1).cast(pl.Int32))
            for col in ['length', 'width', 'height']:
                if col not in parts_df.columns:
                    parts_df = parts_df.with_columns(pl.lit(None).cast(pl.Float64).alias(col))
            if 'dimensions_str' not in parts_df.columns:
                parts_df = parts_df.with_columns(dimensions_str=pl.lit(None).cast(pl.Utf8))
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Ä–∞–∑–º–µ—Ä–æ–≤
            parts_df = parts_df.with_columns([
                pl.col('length').cast(pl.Utf8).fill_null('').alias('_length_str'),
                pl.col('width').cast(pl.Utf8).fill_null('').alias('_width_str'),
                pl.col('height').cast(pl.Utf8).fill_null('').alias('_height_str'),
            ])
            parts_df = parts_df.with_columns(
                pl.when(pl.col('dimensions_str').is_not_null() & (pl.col('dimensions_str') != '')).then(pl.col('dimensions_str'))
                .otherwise(
                    pl.concat_str([pl.col('_length_str'), pl.lit('x'), pl.col('_width_str'), pl.lit('x'), pl.col('_height_str')], separator='')
                ).alias('dimensions_str')
            )
            parts_df = parts_df.drop(['_length_str', '_width_str', '_height_str'])
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ description
            if 'artikul' not in parts_df.columns:
                parts_df = parts_df.with_columns(artikul=pl.lit(''))
            if 'brand' not in parts_df.columns:
                parts_df = parts_df.with_columns(brand=pl.lit(''))
            parts_df = parts_df.with_columns([
                pl.col('artikul').cast(pl.Utf8).fill_null('').alias('_artikul_str'),
                pl.col('brand').cast(pl.Utf8).fill_null('').alias('_brand_str'),
                pl.col('multiplicity').cast(pl.Utf8).alias('_multiplicity_str'),
            ])
            for col in ['_artikul_str', '_brand_str', '_multiplicity_str']:
                if col not in parts_df.columns:
                    parts_df = parts_df.with_columns(pl.lit('').alias(col))
            parts_df = parts_df.with_columns(
                pl.concat_str([
                    '–ê—Ä—Ç–∏–∫—É–ª: ', pl.col('_artikul_str'),
                    ', –ë—Ä–µ–Ω–¥: ', pl.col('_brand_str'),
                    ', –ö—Ä–∞—Ç–Ω–æ—Å—Ç—å: ', pl.col('_multiplicity_str'), ' —à—Ç.'
                ], separator='').alias('description')
            )
            parts_df = parts_df.drop(['_artikul_str', '_brand_str', '_multiplicity_str'])

            # –í—ã–∑–æ–≤ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏
            # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ DataFrame
            final_columns = [
                'artikul_norm', 'brand_norm', 'artikul', 'brand', 'multiplicity', 'barcode', 
                'length', 'width', 'height', 'weight', 'image_url', 'dimensions_str', 'description'
            ]
            # –í–∞–∂–Ω–æ: –µ—Å–ª–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ —ç—Ç–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ
            select_exprs = [pl.col(c) if c in parts_df.columns else pl.lit(None).alias(c) for c in final_columns]
            # –¢–µ–ø–µ—Ä—å —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –∫–æ–ª–æ–Ω–∫–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
            # –î–ª—è —ç—Ç–æ–≥–æ —Å–æ–∑–¥–∞–¥–∏–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –≤ –ø–æ—Ä—è–¥–∫–µ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö
            # (–µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤—ã–±—Ä–∞–ª —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å, —Ç–æ –≤–æ–∑—å–º–µ–º –∏—Ö)
            # –í–∞—Ä–∏–∞–Ω—Ç: –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–ø–æ –ø–æ—Ä—è–¥–∫—É)
            # –í–∞—Ä–∏–∞–Ω—Ç: –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –µ—Å—Ç—å, –≤ –ø–æ—Ä—è–¥–∫–µ –≤—ã–±–æ—Ä–∞
            # –¢—É—Ç - –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ
            selected_columns_ordered = []
            for col_name in final_columns:
                if col_name in selected_columns:
                    selected_columns_ordered.append(pl.col(col_name))
            # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∏—Ç–æ–≥–æ–≤–æ–º DataFrame, –¥–æ–±–∞–≤–∏–º None
            # –ù–æ –ª—É—á—à–µ –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            df_final = parts_df.select(selected_columns)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–∏—Å–æ–∫ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            self.upsert_data('parts_data', df_final, ['artikul_norm', 'brand_norm'])
        progress_bar.progress(1.0, text="–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        time.sleep(1)
        progress_bar.empty()
        st.success("üíæ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—É –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    def merge_all_data_parallel(self, file_paths: Dict[str, str]) -> Dict[str, any]:
        start_time = time.time()
        stats = {}
        st.info("üöÄ –ù–∞—á–∞–ª–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Ñ–∞–π–ª–æ–≤...")
        n_files = len(file_paths)
        file_progress_bar = st.progress(0, text=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤...")
        dataframes = {}
        processed_files = 0
        with ThreadPoolExecutor() as executor:
            future_to_file = {executor.submit(self.read_and_prepare_file, path, ftype): ftype for ftype, path in file_paths.items()}
            for future in as_completed(future_to_file):
                ftype = future_to_file[future]
                try:
                    df = future.result()
                    if not df.is_empty():
                        dataframes[ftype] = df
                        st.success(f"‚úÖ –§–∞–π–ª '{ftype}' –ø—Ä–æ—á–∏—Ç–∞–Ω: {len(df):,} —Å—Ç—Ä–æ–∫.")
                        logger.info(f"–§–∞–π–ª '{ftype}' —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {len(df):,} —Å—Ç—Ä–æ–∫, –∫–æ–ª–æ–Ω–∫–∏: {df.columns}")
                    else:
                        logger.warning(f"–§–∞–π–ª '{ftype}' –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π DataFrame –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                        st.warning(f"‚ö†Ô∏è –§–∞–π–ª '{ftype}' –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å.")
                except Exception as e:
                    logger.exception(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {ftype}")
                    st.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ {ftype}: {e}")
                finally:
                    processed_files += 1
                    file_progress_bar.progress(processed_files / n_files, text=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {ftype} ({processed_files}/{n_files})")
        file_progress_bar.empty()
        if not dataframes:
            st.error("‚ùå –ù–∏ –æ–¥–∏–Ω —Ñ–∞–π–ª –Ω–µ –±—ã–ª –∑–∞–≥—Ä—É–∂–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")
            return {}
        self.process_and_load(dataframes)
        processing_time = time.time() - start_time
        total_records = self.get_total_records()
        stats['processing_time'] = processing_time
        stats['total_records'] = total_records
        st.success(f"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f} —Å–µ–∫")
        st.success(f"üìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞—Ä—Ç–∏–∫—É–ª–æ–≤ –≤ –±–∞–∑–µ: {total_records:,}")
        self.create_indexes()
        return stats

    def get_total_records(self) -> int:
        try:
            result = self.conn.execute("SELECT COUNT(*) FROM parts_data").fetchone()
            return result[0] if result else 0
        except (duckdb.Error, TypeError):
            return 0

    def get_export_query(self) -> str:
        return "SELECT * FROM parts_data"

    def generate_exclude_filter(self, exclude_input: str):
        """–°–æ–∑–¥–∞–µ—Ç SQL —É—Å–ª–æ–≤–∏–µ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—è–º."""
        terms = [term.strip() for term in exclude_input.split('|') if term.strip()]
        if not terms:
            return ""
        conditions = []
        for term in terms:
            escaped_term = term.replace("'", "''")
            conditions.append(f"(name LIKE '%{escaped_term}%' OR name = '{escaped_term}')")
        return " OR ".join(conditions)

    def export_to_csv_optimized(self, output_path: str, selected_columns: Optional[List[str]] = None, exclude_names: str = "") -> bool:
        total_records = self.conn.execute("SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data) AS t").fetchone()[0]
        if total_records == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False
        st.info(f"üì§ –≠–∫—Å–ø–æ—Ä—Ç {total_records:,} –∑–∞–ø–∏—Å–µ–π –≤ CSV...")
        try:
            query = self.get_export_query()

            # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä –∏—Å–∫–ª—é—á–µ–Ω–∏–π –ø–æ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è–º
            exclude_filter = self.generate_exclude_filter(exclude_names)
            if exclude_filter:
                query += f" WHERE NOT ({exclude_filter})"

            df = self.conn.execute(query).pl()

            # –í—ã–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫ –ø–æ —Å–ø–∏—Å–∫—É, –µ—Å–ª–∏ –æ–Ω –ø–µ—Ä–µ–¥–∞–Ω
            if selected_columns:
                existing_cols = [col for col in selected_columns if col in df.columns]
                df = df.select(existing_cols)

            buf = io.StringIO()
            df.write_csv(buf, separator=';')
            csv_text = buf.getvalue()
            with open(output_path, 'wb') as f:
                f.write(b'\xef\xbb\xbf')
                f.write(csv_text.encode('utf-8'))
            file_size = os.path.getsize(output_path) / (1024 * 1024)
            st.success(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã: {output_path} ({file_size:.1f} –ú–ë)")
            return True
        except Exception as e:
            logger.exception("–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ CSV")
            st.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ CSV: {e}")
            return False

    def export_to_excel(self, output_path: Path, selected_columns: Optional[List[str]] = None, exclude_names: str = "") -> Tuple[bool, Optional[Path]]:
        total_records = self.conn.execute("SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data) AS t").fetchone()[0]
        if total_records == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False, None
        st.info(f"üì§ –≠–∫—Å–ø–æ—Ä—Ç {total_records:,} –∑–∞–ø–∏—Å–µ–π –≤ Excel...")
        try:
            num_files = (total_records + EXCEL_ROW_LIMIT - 1) // EXCEL_ROW_LIMIT
            exported_files = []
            progress_bar = st.progress(0, text=f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —ç–∫—Å–ø–æ—Ä—Ç—É {num_files} —Ñ–∞–π–ª–∞(–æ–≤)...")
            for i in range(num_files):
                progress_bar.progress((i + 1) / num_files, text=f"–≠–∫—Å–ø–æ—Ä—Ç —á–∞—Å—Ç–∏ {i+1} –∏–∑ {num_files}...")
                offset = i * EXCEL_ROW_LIMIT
                query = f"{self.get_export_query()} LIMIT {EXCEL_ROW_LIMIT} OFFSET {offset}"

                # –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä –∏—Å–∫–ª—é—á–µ–Ω–∏–π
                exclude_filter = self.generate_exclude_filter(exclude_names)
                if exclude_filter:
                    query += f" WHERE NOT ({exclude_filter})"

                df = self.conn.execute(query).pl()

                # –í—ã–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫ –ø–æ —Å–ø–∏—Å–∫—É, –µ—Å–ª–∏ –æ–Ω –ø–µ—Ä–µ–¥–∞–Ω
                if selected_columns:
                    existing_cols = [col for col in selected_columns if col in df.columns]
                    df = df.select(existing_cols)

                file_part_path = output_path.with_name(f"{output_path.stem}_part_{i+1}.xlsx")
                df.write_excel(str(file_part_path))
                exported_files.append(file_part_path)
            progress_bar.empty()
            if len(exported_files) > 1:
                st.info("–ê—Ä—Ö–∏–≤–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤ –≤ ZIP...")
                zip_path = output_path.with_suffix('.zip')
                with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for file in exported_files:
                        zipf.write(file, file.name)
                        os.remove(file)
                final_path = zip_path
            else:
                final_path = exported_files[0]
                if final_path.name != output_path.name:
                    os.rename(final_path, output_path)
                    final_path = output_path
            file_size = os.path.getsize(final_path) / (1024 * 1024)
            st.success(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã: {final_path.name} ({file_size:.1f} –ú–ë)")
            return True, final_path
        except Exception as e:
            logger.exception("–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Excel")
            st.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Excel: {e}")
            return False, None

    def export_to_parquet(self, output_path: str, selected_columns: Optional[List[str]] = None, exclude_names: str = "") -> bool:
        total_records = self.conn.execute("SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data) AS t").fetchone()[0]
        if total_records == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False
        st.info(f"üì§ –≠–∫—Å–ø–æ—Ä—Ç {total_records:,} –∑–∞–ø–∏—Å–µ–π –≤ Parquet...")
        try:
            query = self.get_export_query()
            # –¥–æ–±–∞–≤–ª—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –ø–æ –∏–º–µ–Ω–∏
            exclude_filter = self.generate_exclude_filter(exclude_names)
            if exclude_filter:
                query += f" WHERE NOT ({exclude_filter})"

            df = self.conn.execute(query).pl()

            # –í—ã–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫ –ø–æ —Å–ø–∏—Å–∫—É, –µ—Å–ª–∏ –æ–Ω –ø–µ—Ä–µ–¥–∞–Ω
            if selected_columns:
                existing_cols = [col for col in selected_columns if col in df.columns]
                df = df.select(existing_cols)

            df.write_parquet(output_path)
            file_size = os.path.getsize(output_path) / (1024 * 1024)
            st.success(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ –ü–∞—Ä–∫–µ—Ç: {output_path} ({file_size:.1f} –ú–ë)")
            return True
        except Exception as e:
            logger.exception("–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Parquet")
            st.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Parquet: {e}")
            return False

    def show_export_interface(self):
        st.header("üì§ –£–º–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")
        total_records = self.conn.execute("SELECT COUNT(DISTINCT (artikul_norm, brand_norm)) FROM parts_data").fetchone()[0]
        st.info(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ (—Å—Ç—Ä–æ–∫): {total_records:,}")
        if total_records == 0:
            st.warning("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ—Ç —Å–≤—è–∑–µ–π –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ.")
            return
        # –í—ã–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫
        available_columns = [
            "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", "–ë—Ä–µ–Ω–¥", "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", "–û–ø–∏—Å–∞–Ω–∏–µ",
            "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", "–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞",
            "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "OE –Ω–æ–º–µ—Ä", "–∞–Ω–∞–ª–æ–≥–∏", "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
        ]
        selected_columns = st.multiselect(
            "–í—ã–±–µ—Ä–∏—Ç–µ –∏ —É–ø–æ—Ä—è–¥–æ—á–∏—Ç–µ —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞:",
            options=available_columns,
            default=available_columns,
            key='columns_select'
        )
        # –í–≤–æ–¥ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –ø–æ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—é
        exclude_names = st.text_input("–ò—Å–∫–ª—é—á–∏—Ç—å –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é (—á–µ—Ä–µ–∑ |):", key='exclude_names')

        export_format = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞:", ["CSV", "Excel (.xlsx)", "Parquet (–¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤)"], index=0)

        if export_format == "CSV":
            if st.button("üöÄ –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV", key='export_csv'):
                output_path = self.data_dir / "auto_parts_report.csv"
                with st.spinner("–ò–¥–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç –≤ CSV..."):
                    success = self.export_to_csv_optimized(str(output_path), selected_columns, exclude_names)
                if success:
                    with open(output_path, "rb") as f:
                        st.download_button("üì• –°–∫–∞—á–∞—Ç—å CSV —Ñ–∞–π–ª", f, "auto_parts_report.csv", "text/csv")
        elif export_format == "Excel (.xlsx)":
            st.info("‚ÑπÔ∏è –ï—Å–ª–∏ –∑–∞–ø–∏—Å–µ–π –±–æ–ª—å—à–µ 1 –º–ª–Ω, —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±—É–¥–µ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ –∏ —É–ø–∞–∫–æ–≤–∞–Ω –≤ ZIP-–∞—Ä—Ö–∏–≤.")
            if st.button("üìä –≠–∫—Å–ø–æ—Ä—Ç –≤ Excel", key='export_excel'):
                output_path = self.data_dir / "auto_parts_report.xlsx"
                with st.spinner("–ò–¥–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç –≤ Excel..."):
                    success, final_path = self.export_to_excel(str(output_path), selected_columns, exclude_names)
                if success and final_path and final_path.exists():
                    with open(final_path, "rb") as f:
                        mime = "application/zip" if final_path.suffix == ".zip" else "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        st.download_button(f"üì• –°–∫–∞—á–∞—Ç—å {final_path.name}", f, final_path.name, mime)
        elif export_format == "Parquet (–¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤)":
            if st.button("‚ö°Ô∏è –≠–∫—Å–ø–æ—Ä—Ç –≤ Parquet", key='export_parquet'):
                output_path = self.data_dir / "auto_parts_report.parquet"
                with st.spinner("–ò–¥–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç –≤ Parquet..."):
                    success = self.export_to_parquet(str(output_path), selected_columns, exclude_names)
                if success:
                    with open(output_path, "rb") as f:
                        st.download_button("üì• –°–∫–∞—á–∞—Ç—å Parquet —Ñ–∞–π–ª", f, "auto_parts_report.parquet", "application/octet-stream")
