import polars as pl
import duckdb
import streamlit as st
import os
import time
import logging
from pathlib import Path
from typing import Dict, List
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
EXCEL_ROW_LIMIT = 1_000_000

class HighVolumeAutoPartsCatalog:
    
    def __init__(self):
        self.data_dir = Path("./auto_parts_data")
        self.data_dir.mkdir(exist_ok=True)
        self.db_path = self.data_dir / "catalog.duckdb"
        self.conn = duckdb.connect(database=str(self.db_path))
        self.setup_database()
        
        st.set_page_config(
            page_title="AutoParts Catalog 10M+", 
            layout="wide",
            page_icon="üöó"
        )
    
    def setup_database(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE, 
                width DOUBLE,
                height DOUBLE, 
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS prices (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                price DOUBLE,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.create_indexes()

    def create_indexes(self):
        st.info("–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞...")
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_oe_data_oe ON oe_data(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_parts_data_keys ON parts_data(artikul_norm, brand_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_oe ON cross_references(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_artikul ON cross_references(artikul_norm, brand_norm)",
            "CREATE INDEX IF NOT EXISTS idx_prices ON prices(artikul_norm, brand_norm)"
        ]
        for index_sql in indexes:
            self.conn.execute(index_sql)
        st.success("–ò–Ω–¥–µ–∫—Å—ã —Å–æ–∑–¥–∞–Ω—ã.")

    @staticmethod
    def normalize_key(key_series: pl.Series) -> pl.Series:
        return (
            key_series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-zA-za-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
            .str.to_lowercase()
        )

    @staticmethod
    def clean_values(value_series: pl.Series) -> pl.Series:
        return (
            value_series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-zA-za-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
        )

    @staticmethod
    def determine_category_vectorized(name_series: pl.Series) -> pl.Series:
        categories_map = {
            '–§–∏–ª—å—Ç—Ä': '—Ñ–∏–ª—å—Ç—Ä|filter', 
            '–¢–æ—Ä–º–æ–∑–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞': '—Ç–æ—Ä–º–æ–∑|brake|–∫–æ–ª–æ–¥–∫|–¥–∏—Å–∫|—Å—É–ø–ø–æ—Ä—Ç',
            '–ü–æ–¥–≤–µ—Å–∫–∞': '–∞–º–æ—Ä—Ç–∏–∑–∞—Ç–æ—Ä|—Å—Ç–æ–π–∫|spring|–ø–æ–¥–≤–µ—Å–∫|–†—ã—á–∞–≥|–†—ã—á–∞–≥–∏|–®–∞—Ä–æ–≤–∞—è –æ–ø–æ—Ä–∞|–û–ø–æ—Ä–∞ —à–∞—Ä–æ–≤–∞—è|–°–∞–π–ª–µ–Ω—Ç–±–ª–æ–∫|–°—Ç—É–ø–∏—Ü|–ø–æ–¥—à–∏–ø–Ω–∏–∫ —Å—Ç—É–ø–∏—Ü—ã|–ø–æ–¥—à–∏–ø–Ω–∏–∫–∏ —Å—Ç—É–ø–∏—Ü—ã', 
            '–î–≤–∏–≥–∞—Ç–µ–ª—å': '–¥–≤–∏–≥–∞—Ç–µ–ª—å|engine|—Å–≤–µ—á|–ø–æ—Ä—à–µ–Ω—å|–∫–ª–∞–ø–∞–Ω',
            '–¢—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è': '—Ç—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è|—Å—Ü–µ–ø–ª–µ–Ω|–∫–æ—Ä–æ–±–∫|transmission', 
            '–≠–ª–µ–∫—Ç—Ä–∏–∫–∞': '–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä|–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä|—Å—Ç–∞—Ä—Ç–µ—Ä|–ø—Ä–æ–≤–æ–¥|–ª–∞–º–ø',
            '–†—É–ª–µ–≤–æ–µ': '—Ä—É–ª–µ–≤–æ–π|—Ç—è–≥–∞|–Ω–∞–∫–æ–Ω–µ—á–Ω–∏–∫|steering', 
            '–í—ã—Ö–ª–æ–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞': '–≥–ª—É—à–∏—Ç–µ–ª—å|–≥–ª—É—à–∏—Ç–µ–ª|–∫–∞—Ç–∞–ª–∏–∑–∞—Ç–æ—Ä|–≤—ã—Ö–ª–æ–ø|exhaust|',
            '–û—Ö–ª–∞–∂–¥–µ–Ω–∏–µ': '—Ä–∞–¥–∏–∞—Ç–æ—Ä|–≤–µ–Ω—Ç–∏–ª—è—Ç–æ—Ä|—Ç–µ—Ä–º–æ—Å—Ç–∞—Ç|cooling', 
            '–¢–æ–ø–ª–∏–≤–æ': '—Ç–æ–ø–ª–∏–≤–Ω—ã–π|–±–µ–Ω–∑–æ–Ω–∞—Å–æ—Å|—Ñ–æ—Ä—Å—É–Ω–∫|fuel',
        }
        name_lower = name_series.str.to_lowercase()
        categorization_expr = pl.when(pl.lit(False)).then(pl.lit(None))
        for category, pattern in categories_map.items():
            categorization_expr = categorization_expr.when(name_lower.str.contains(pattern)).then(pl.lit(category))
        return categorization_expr.otherwise(pl.lit('–†–∞–∑–Ω–æ–µ')).alias('category')

    def detect_columns(self, actual_columns: List[str], expected_columns: List[str]) -> Dict[str, str]:
        mapping = {}
        column_variants = {
            'oe_number': ['oe –Ω–æ–º–µ—Ä', 'oe', '–æe', '–Ω–æ–º–µ—Ä', 'code', 'OE'], 
            'artikul': ['–∞—Ä—Ç–∏–∫—É–ª', 'article', 'sku'],
            'brand': ['–±—Ä–µ–Ω–¥', 'brand', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', 'manufacturer'], 
            'name': ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ', 'name', '–æ–ø–∏—Å–∞–Ω–∏–µ', 'description'],
            'applicability': ['–ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', 'vehicle', 'applicability'], 
            'barcode': ['—à—Ç—Ä–∏—Ö-–∫–æ–¥', 'barcode', '—à—Ç—Ä–∏—Ö–∫–æ–¥', 'ean', 'eac13'],
            'multiplicity': ['–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å —à—Ç', '–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å', 'multiplicity'], 
            'length': ['–¥–ª–∏–Ω–∞ (—Å–º)', '–¥–ª–∏–Ω–∞', 'length', '–¥–ª–∏–Ω–Ω–∞'],
            'width': ['—à–∏—Ä–∏–Ω–∞ (—Å–º)', '—à–∏—Ä–∏–Ω–∞', 'width'], 
            'height': ['–≤—ã—Å–æ—Ç–∞ (—Å–º)', '–≤—ã—Å–æ—Ç–∞', 'height'],
            'weight': ['–≤–µ—Å (–∫–≥)', '–≤–µ—Å, –∫–≥', '–≤–µ—Å', 'weight'], 
            'image_url': ['—Å—Å—ã–ª–∫–∞', 'url', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 'image', '–∫–∞—Ä—Ç–∏–Ω–∫–∞'],
            'dimensions_str': ['–≤–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã', '—Ä–∞–∑–º–µ—Ä—ã', 'dimensions', 'size'],
            'price': ['—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞', '—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è —Ü–µ–Ω–∞', 'price']
        }
        actual_lower = {col.lower(): col for col in actual_columns}
        for expected in expected_columns:
            variants = [v.lower() for v in column_variants.get(expected, [expected])]
            for variant in variants:
                for actual_l, actual_orig in actual_lower.items():
                    if variant in actual_l:
                        mapping[actual_orig] = expected
                        break
                if expected in mapping.values():
                    break
        return mapping

    def read_and_prepare_file(self, file_path: str, file_type: str) -> pl.DataFrame:
        logger.info(f"–ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞: {file_type} ({file_path})")
        try:
            df = pl.read_excel(file_path, engine='calamine')
        except Exception as e:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {file_path}: {e}")
            return pl.DataFrame()

        schemas = {
            'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
            'barcode': ['brand', 'artikul', 'barcode', 'multiplicity'],
            'dimensions': ['artikul', 'brand', 'length', 'width', 'height', 'weight', 'dimensions_str'],
            'images': ['artikul', 'brand', 'image_url'],
            'cross': ['oe_number', 'artikul', 'brand'],
            'prices': ['artikul', 'brand', 'price']
        }
        expected_cols = schemas.get(file_type, [])
        column_mapping = self.detect_columns(df.columns, expected_cols)
        df = df.rename(column_mapping)
        
        # –û—á–∏—Å—Ç–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç –∞–ø–æ—Å—Ç—Ä–æ—Ñ–æ–≤ –∏ –º—É—Å–æ—Ä–∞ –Ω–∞ –≤—Ö–æ–¥–µ
        if 'artikul' in df.columns:
            df = df.with_columns(artikul=self.clean_values(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand=self.clean_values(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number=self.clean_values(pl.col('oe_number')))
        
        key_cols = [col for col in ['oe_number', 'artikul', 'brand'] if col in df.columns]
        if key_cols:
            df = df.unique(subset=key_cols, keep='first')

        # –°–æ–∑–¥–∞—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –¥–ª—è –∫–ª—é—á–µ–π (–Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä)
        if 'artikul' in df.columns:
            df = df.with_columns(artikul_norm=self.normalize_key(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand_norm=self.normalize_key(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number_norm=self.normalize_key(pl.col('oe_number')))
            
        return df

    def upsert_data(self, table_name: str, df: pl.DataFrame, pk: List[str]):
        if df.is_empty():
            return
        
        df = df.unique(keep='first')
        
        cols = df.columns
        pk_str = ", ".join(f'"{c}"' for c in pk)
        
        temp_view_name = f"temp_{table_name}_{int(time.time())}"
        self.conn.register(temp_view_name, df.to_arrow())
        
        update_cols = [col for col in cols if col not in pk]
        
        if not update_cols:
            on_conflict_action = "DO NOTHING"
        else:
            update_clause = ", ".join([f'"{col}" = excluded."{col}"' for col in update_cols])
            on_conflict_action = f"DO UPDATE SET {update_clause}"

        sql = f"""
        INSERT INTO {table_name}
        SELECT * FROM {temp_view_name}
        ON CONFLICT ({pk_str}) {on_conflict_action};
        """
        
        try:
            self.conn.execute(sql)
            logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–æ/–≤—Å—Ç–∞–≤–ª–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ UPSERT –≤ {table_name}: {e}")
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}. –î–µ—Ç–∞–ª–∏ –≤ –ª–æ–≥–µ.")
        finally:
            self.conn.unregister(temp_view_name)

    def load_prices(self, file_path: str) -> pl.DataFrame:
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ü–µ–Ω –∏–∑ —Ñ–∞–π–ª–∞: {file_path}")
        df = pl.read_excel(file_path, engine='calamine')
        df = df.rename({"artikul": "artikul", "brand": "brand", "price": "price"})  # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∏–º–µ–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç
        df = df.with_columns(
            artikul_norm=self.normalize_key(pl.col('artikul')),
            brand_norm=self.normalize_key(pl.col('brand'))
        )
        return df

    def upsert_prices(self, df: pl.DataFrame, general_markup: float, brand_markup: Dict[str, float]):
        df = df.with_columns([
            (pl.col('price') * (1 + general_markup)).alias('final_price'),
        ])
        for brand, markup in brand_markup.items():
            df = df.with_columns(
                pl.when(pl.col('brand_norm') == brand)
                .then(pl.col('price') * (1 + markup))
                .otherwise(pl.col('final_price'))
                .alias('final_price')
            )

        self.upsert_data('prices', df.select(['artikul_norm', 'brand_norm', 'final_price']), ['artikul_norm', 'brand_norm'])

    def process_and_load_data(self, dataframes: Dict[str, pl.DataFrame]):
        st.info("üîÑ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑–µ...")

        steps = [s for s in ['oe', 'cross', 'parts', 'prices'] if s in dataframes or s == 'parts']
        num_steps = len(steps)
        progress_bar = st.progress(0, text="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        step_counter = 0

        if 'oe' in dataframes:
            step_counter += 1
            progress_bar.progress(step_counter / (num_steps + 1), text=f"({step_counter}/{num_steps}) –û–±—Ä–∞–±–æ—Ç–∫–∞ OE –¥–∞–Ω–Ω—ã—Ö...")
            df = dataframes['oe'].filter(pl.col('oe_number_norm') != "")
            
            oe_df = df.select(['oe_number_norm', 'oe_number', 'name', 'applicability']).unique(subset=['oe_number_norm'], keep='first')
            if 'name' in oe_df.columns:
                oe_df = oe_df.with_columns(self.determine_category_vectorized(pl.col('name')))
            else:
                oe_df = oe_df.with_columns(category=pl.lit('–†–∞–∑–Ω–æ–µ'))
            self.upsert_data('oe_data', oe_df, ['oe_number_norm'])
            
            cross_df_from_oe = df.filter(pl.col('artikul_norm') != "").select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df_from_oe, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        if 'cross' in dataframes:
            step_counter += 1
            progress_bar.progress(step_counter / (num_steps + 1), text=f"({step_counter}/{num_steps}) –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫—Ä–æ—Å—Å–æ–≤...")
            df = dataframes['cross'].filter((pl.col('oe_number_norm') != "") & (pl.col('artikul_norm') != ""))
            cross_df_from_cross = df.select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df_from_cross, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        step_counter += 1
        progress_bar.progress(step_counter / (num_steps + 1), text=f"({step_counter}/{num_steps}) –°–±–æ—Ä–∫–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∞—Ä—Ç–∏–∫—É–ª–∞–º...")
        parts_df = None

        file_priority = ['oe', 'barcode', 'images', 'dimensions']
        key_files = {ftype: df for ftype, df in dataframes.items() if ftype in file_priority}
        
        if key_files:
            all_parts = pl.concat([ 
                df.select(['artikul', 'artikul_norm', 'brand', 'brand_norm']) 
                for df in key_files.values() if 'artikul_norm' in df.columns and 'brand_norm' in df.columns
            ]).filter(pl.col('artikul_norm') != "").unique(subset=['artikul_norm', 'brand_norm'], keep='first')

            parts_df = all_parts

            for ftype in file_priority:
                if ftype not in key_files: continue
                df = key_files[ftype]
                if df.is_empty() or 'artikul_norm' not in df.columns: continue
                
                join_cols = [col for col in df.columns if col not in ['artikul', 'artikul_norm', 'brand', 'brand_norm']]
                if not join_cols: continue
                
                existing_cols = set(parts_df.columns)
                join_cols = [col for col in join_cols if col not in existing_cols]
                if not join_cols: continue
                
                df_subset = df.select(['artikul_norm', 'brand_norm'] + join_cols).unique(subset=['artikul_norm', 'brand_norm'], keep='first')
                parts_df = parts_df.join(df_subset, on=['artikul_norm', 'brand_norm'], how='left', coalesce=True)

        if parts_df is not None and not parts_df.is_empty():
            if 'multiplicity' not in parts_df.columns:
                parts_df = parts_df.with_columns(multiplicity=pl.lit(1).cast(pl.Int32))
            else:
                parts_df = parts_df.with_columns(
                    pl.col('multiplicity').fill_null(1).cast(pl.Int32)
                )
            
            for col in ['length', 'width', 'height']:
                if col not in parts_df.columns:
                    parts_df = parts_df.with_columns(pl.lit(None).cast(pl.Float64).alias(col))
            
            if 'dimensions_str' not in parts_df.columns:
                parts_df = parts_df.with_columns(dimensions_str=pl.lit(None).cast(pl.Utf8))
            parts_df = parts_df.with_columns([
                pl.col('length').cast(pl.Utf8).fill_null('').alias('_length_str'),
                pl.col('width').cast(pl.Utf8).fill_null('').alias('_width_str'),
                pl.col('height').cast(pl.Utf8).fill_null('').alias('_height_str'),
            ])
            
            parts_df = parts_df.with_columns(
                dimensions_str=pl.when(
                    (pl.col('dimensions_str').is_not_null()) & 
                    (pl.col('dimensions_str').cast(pl.Utf8) != '')
                ).then(
                    pl.col('dimensions_str').cast(pl.Utf8)
                ).otherwise(
                    pl.concat_str([
                        pl.col('_length_str'), pl.lit('x'), 
                        pl.col('_width_str'), pl.lit('x'), 
                        pl.col('_height_str')
                    ], separator='')
                )
            )
            
            parts_df = parts_df.drop(['_length_str', '_width_str', '_height_str'])
            
            if 'artikul' not in parts_df.columns:
                parts_df = parts_df.with_columns(artikul=pl.lit(''))
            if 'brand' not in parts_df.columns:
                parts_df = parts_df.with_columns(brand=pl.lit(''))

            parts_df = parts_df.with_columns([
                pl.col('artikul').cast(pl.Utf8).fill_null('').alias('_artikul_str'),
                pl.col('brand').cast(pl.Utf8).fill_null('').alias('_brand_str'),
                pl.col('multiplicity').cast(pl.Utf8).alias('_multiplicity_str'),
            ])
            
            parts_df = parts_df.with_columns(
                description=pl.concat_str([
                    pl.lit('–ê—Ä—Ç–∏–∫—É–ª: '), pl.col('_artikul_str'),
                    pl.lit(', –ë—Ä–µ–Ω–¥: '), pl.col('_brand_str'),
                    pl.lit(', –ö—Ä–∞—Ç–Ω–æ—Å—Ç—å: '), pl.col('_multiplicity_str'), pl.lit(' —à—Ç.')
                ], separator='')
            )
            
            parts_df = parts_df.drop(['_artikul_str', '_brand_str', '_multiplicity_str'])
            final_columns = [
                'artikul_norm', 'brand_norm', 'artikul', 'brand', 'multiplicity', 'barcode', 
                'length', 'width', 'height', 'weight', 'image_url', 'dimensions_str', 'description'
            ]
            select_exprs = [pl.col(c) if c in parts_df.columns else pl.lit(None).alias(c) for c in final_columns]
            parts_df = parts_df.select(select_exprs)
            
            self.upsert_data('parts_data', parts_df, ['artikul_norm', 'brand_norm'])

        if 'prices' in dataframes:
            price_df = dataframes['prices']
            general_markup = st.number_input("–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞ (%)", min_value=0.0, step=0.01) / 100.0
            brand_markup = {}
            brand_list = price_df['brand_norm'].unique().to_list()
            for brand in brand_list:
                markup = st.number_input(f"–ù–∞—Ü–µ–Ω–∫–∞ –¥–ª—è {brand} (%)", min_value=0.0, step=0.01) / 100.0
                brand_markup[brand] = markup
            self.upsert_prices(price_df, general_markup, brand_markup)

        progress_bar.progress(1.0, text="–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        time.sleep(1)
        progress_bar.empty()
        st.success("üíæ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—É –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    def merge_all_data_parallel(self, file_paths: Dict[str, str]) -> Dict[str, any]:
        start_time = time.time()
        stats = {}
        
        st.info("üöÄ –ù–∞—á–∞–ª–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Ñ–∞–π–ª–æ–≤...")
        n_files = len(file_paths)
        file_progress_bar = st.progress(0, text="–û–∂–∏–¥–∞–Ω–∏–µ...")
        
        dataframes = {}
        processed_files = 0
        with ThreadPoolExecutor() as executor:
            future_to_file = {executor.submit(self.read_and_prepare_file, path, ftype): ftype for ftype, path in file_paths.items()}
            for future in as_completed(future_to_file):
                ftype = future_to_file[future]
                try:
                    df = future.result()
                    if not df.is_empty():
                        dataframes[ftype] = df
                        st.success(f"‚úÖ –§–∞–π–ª '{ftype}' –ø—Ä–æ—á–∏—Ç–∞–Ω: {len(df):,} —Å—Ç—Ä–æ–∫.")
                    else:
                        st.warning(f"‚ö†Ô∏è –§–∞–π–ª '{ftype}' –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å.")
                except Exception as e:
                    logger.exception(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {ftype}")
                    st.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ {ftype}: {e}")
                finally:
                    processed_files += 1
                    file_progress_bar.progress(processed_files / n_files, text=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {ftype} ({processed_files}/{n_files})")
        
        file_progress_bar.empty()

        if not dataframes:
            st.error("‚ùå –ù–∏ –æ–¥–∏–Ω —Ñ–∞–π–ª –Ω–µ –±—ã–ª –∑–∞–≥—Ä—É–∂–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")
            return {}

        self.process_and_load_data(dataframes)
        
        processing_time = time.time() - start_time
        total_records = self.get_total_records()
        
        stats['processing_time'] = processing_time
        stats['total_records'] = total_records
        
        st.success(f"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f} —Å–µ–∫—É–Ω–¥")
        st.success(f"üìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞—Ä—Ç–∏–∫—É–ª–æ–≤ –≤ –±–∞–∑–µ: {total_records:,}")
        
        self.create_indexes()
        return stats
    
    def get_total_records(self) -> int:
        try:
            result = self.conn.execute("SELECT COUNT(*) FROM parts_data").fetchone()
            return result[0] if result else 0
        except (duckdb.Error, TypeError):
            return 0

    def get_export_query(self, selected_columns: List[str], exclusions: str) -> str:
        exclusions_condition = " AND ".join([
            f'NOT (brand_norm LIKE "%{exclusion}%" OR artikul_norm LIKE "%{exclusion}%")'
            for exclusion in exclusions.split('|') if exclusion
        ])
        
        query = f"""
        SELECT {', '.join(selected_columns)}
        FROM parts_data
        WHERE {exclusions_condition}
        """
        return query

    def show_pricing_interface(self):
        st.header("üí∏ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞—Ü–µ–Ω–æ–∫")
        general_markup = st.number_input("–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞ (%)", min_value=0.0, step=0.01) / 100.0
        brand_markup = {}
        brand_list = self.conn.execute("SELECT DISTINCT brand_norm FROM prices").fetchall()
        for brand in brand_list:
            markup = st.number_input(f"–ù–∞—Ü–µ–Ω–∫–∞ –¥–ª—è {brand[0]} (%)", min_value=0.0, step=0.01) / 100.0
            brand_markup[brand[0]] = markup

        if st.button("–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ü–µ–Ω—ã"):
            price_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª —Å —Ü–µ–Ω–∞–º–∏", type=['xlsx', 'xls'])
            if price_file:
                df_prices = self.load_prices(price_file)
                self.upsert_prices(df_prices, general_markup, brand_markup)

        exclusions = st.text_input("–ò—Å–∫–ª—é—á–µ–Ω–∏—è (—Ä–∞–∑–¥–µ–ª—è–π—Ç–µ '|' –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π)")
        selected_columns = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞", options=["artikul", "brand", "price", "description"], default=["artikul", "brand", "price"])

        if st.button("–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å"):
            query = self.get_export_query(selected_columns, exclusions)
            df_export = self.conn.execute(query).pl()
            output_path = self.data_dir / "exported_data.csv"
            df_export.write_csv(str(output_path))
            st.success(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã: {output_path.name}")

def main():
    st.title("üöó AutoParts Catalog - –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è 10+ –º–ª–Ω –∑–∞–ø–∏—Å–µ–π")
    st.markdown("""
    ### üí™ –ú–æ—â–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–æ–ª—å—à–∏–º–∏ –æ–±—ä–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–µ–π
    - **–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è**: –ë–µ–∑–æ–ø–∞—Å–Ω–æ –¥–æ–±–∞–≤–ª—è–π—Ç–µ –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–∞—Ç–∞–ª–æ–≥–∞.
    - **–ù–∞–¥–µ–∂–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ**: –î–∞–Ω–Ω—ã–µ –∏–∑ 5-—Ç–∏ —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–ª–∏–≤–∞—é—Ç—Å—è –≤ –µ–¥–∏–Ω—É—é –±–∞–∑—É.
    - **–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ DuckDB –¥–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞.
    - **–£–º–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç**: –ë—ã—Å—Ç—Ä—ã–π –∏ –Ω–∞–¥–µ–∂–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç –≤ CSV, Excel –∏–ª–∏ Parquet —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
    """)
    
    catalog = HighVolumeAutoPartsCatalog()
    
    st.sidebar.title("üß≠ –ù–∞–≤–∏–≥–∞—Ü–∏—è")
    menu_option = st.sidebar.radio("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:", ["–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö", "–≠–∫—Å–ø–æ—Ä—Ç", "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏"])
    
    if menu_option == "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö":
        st.header("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        st.info("""
        **–ü–æ—Ä—è–¥–æ–∫ —Ä–∞–±–æ—Ç—ã:**
        1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ Excel (`.xlsx`, `.xls`). –ù–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–∞—Ç—å –≤—Å–µ —Å—Ä–∞–∑—É.
        2. –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É "–ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É".
        3. –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ—á–∏—Ç–∞–µ—Ç, –æ–±—ä–µ–¥–∏–Ω–∏—Ç –¥–∞–Ω–Ω—ã–µ –∏ –æ–±–Ω–æ–≤–∏—Ç/–¥–æ–ø–æ–ª–Ω–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É.
        
        **üí° –î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:**
        - –í—ã –º–æ–∂–µ—Ç–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å —Ñ–∞–π–ª—ã **–ø–æ –æ–¥–Ω–æ–º—É** –∏–ª–∏ **–ø–∞—á–∫–∞–º–∏** (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ).
        - –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º UPSERT: –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è, —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è.
        - –ü—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞ —Å —Ç–µ–º–∏ –∂–µ –∞—Ä—Ç–∏–∫—É–ª–∞–º–∏/–±—Ä–µ–Ω–¥–∞–º–∏ –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –æ–±–Ω–æ–≤–ª–µ–Ω—ã, –∞ –Ω–µ –ø—Ä–æ–¥—É–±–ª–∏—Ä–æ–≤–∞–Ω—ã.
        - –ú–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —É –≤–∞—Å –µ—Å—Ç—å - –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç—Å—è.
        
        **–¢–∏–ø—ã –§–∞–π–ª–æ–≤:**
        - **–û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**: OE –Ω–æ–º–µ—Ä–∞, –∞—Ä—Ç–∏–∫—É–ª—ã, –±—Ä–µ–Ω–¥, –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ.
        - **–ö—Ä–æ—Å—Å—ã (OE -> –ê—Ä—Ç–∏–∫—É–ª)**: –°–≤—è–∑—å OE –Ω–æ–º–µ—Ä–æ–≤ —Å –∞—Ä—Ç–∏–∫—É–ª–∞–º–∏ –∏ –±—Ä–µ–Ω–¥–∞–º–∏.
        - **–®—Ç—Ä–∏—Ö-–∫–æ–¥—ã**: –°–≤—è–∑—å –∞—Ä—Ç–∏–∫—É–ª–æ–≤ —Å–æ —à—Ç—Ä–∏—Ö-–∫–æ–¥–∞–º–∏ –∏ –∫—Ä–∞—Ç–Ω–æ—Å—Ç—å—é.
        - **–í–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã**: –†–∞–∑–º–µ—Ä—ã –∏ –≤–µ—Å —Ç–æ–≤–∞—Ä–æ–≤.
        - **–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è**: –°—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        - **–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ —Ü–µ–Ω—ã**: –¶–µ–Ω—ã –Ω–∞ –∞—Ä—Ç–∏–∫—É–ª—ã.
        """)
        
        col1, col2 = st.columns(2)
        
        with col1:
            oe_file = st.file_uploader("1. –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (OE)", type=['xlsx', 'xls'])
            cross_file = st.file_uploader("2. –ö—Ä–æ—Å—Å—ã (OE -> –ê—Ä—Ç–∏–∫—É–ª)", type=['xlsx', 'xls'])
            barcode_file = st.file_uploader("3. –®—Ç—Ä–∏—Ö-–∫–æ–¥—ã –∏ –∫—Ä–∞—Ç–Ω–æ—Å—Ç—å", type=['xlsx', 'xls'])
        with col2:
            dimensions_file = st.file_uploader("4. –í–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", type=['xlsx', 'xls'])
            images_file = st.file_uploader("5. –°—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", type=['xlsx', 'xls'])
            prices_file = st.file_uploader("6. –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ —Ü–µ–Ω—ã", type=['xlsx', 'xls'])

        file_map = {
            'oe': oe_file, 
            'cross': cross_file, 
            'barcode': barcode_file,
            'dimensions': dimensions_file,
            'images': images_file,
            'prices': prices_file
        }
        
        if st.button("üöÄ –ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö", type="primary"):
            paths_to_process = {}
            any_file_uploaded = False
            for ftype, uploaded_file in file_map.items():
                if uploaded_file:
                    any_file_uploaded = True
                    path = catalog.data_dir / f"{ftype}_data_{int(time.time())}_{uploaded_file.name}"
                    with open(path, "wb") as f: f.write(uploaded_file.getvalue())
                    paths_to_process[ftype] = str(path)
            
            if any_file_uploaded:
                stats = catalog.merge_all_data_parallel(paths_to_process)
                if stats:
                    st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                    col1, col2, col3 = st.columns(3)
                    col1.metric("–û–±—â–µ–µ –≤—Ä–µ–º—è", f"{stats.get('processing_time', 0):.2f} —Å–µ–∫")
                    col2.metric("–í—Å–µ–≥–æ –∞—Ä—Ç–∏–∫—É–ª–æ–≤ –≤ –±–∞–∑–µ", f"{stats.get('total_records', 0):,}")
                    col3.metric("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤", f"{len(paths_to_process)}")
            else:
                st.warning("‚ö†Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–π–ª –¥–ª—è –Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏.")

    elif menu_option == "–≠–∫—Å–ø–æ—Ä—Ç":
        catalog.show_pricing_interface()
    
    elif menu_option == "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞":
        st.header("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–∞–ª–æ–≥—É")
        with st.spinner("–°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏..."):
            stats = catalog.get_statistics()
        
        if stats.get('total_parts', 0) > 0:
            col1, col2, col3 = st.columns(3)
            col1.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞—Ä—Ç–∏–∫—É–ª–æ–≤", f"{stats.get('total_parts', 0):,}")
            col2.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö OE", f"{stats.get('total_oe', 0):,}")
            col3.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤", f"{stats.get('total_brands', 0):,}")
            
            st.subheader("üèÜ –¢–æ–ø-10 –±—Ä–µ–Ω–¥–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∞—Ä—Ç–∏–∫—É–ª–æ–≤")
            if 'top_brands' in stats and not stats['top_brands'].is_empty():
                st.dataframe(stats['top_brands'].to_pandas(), width='stretch')
            else:
                st.write("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –±—Ä–µ–Ω–¥–∞–º.")

            st.subheader("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
            if 'categories' in stats and not stats['categories'].is_empty():
                st.bar_chart(stats['categories'].to_pandas().set_index('category'))
            else:
                st.write("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º.")
        else:
            st.info("–î–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ —Ä–∞–∑–¥–µ–ª '–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö', —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å.")
    
    elif menu_option == "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏":
        st.header("üóëÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏ –≤ –±–∞–∑–µ")
        st.warning("‚ö†Ô∏è –ë—É–¥—å—Ç–µ –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã! –û–ø–µ—Ä–∞—Ü–∏–∏ —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ–æ–±—Ä–∞—Ç–∏–º—ã.")
        
        management_option = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ –æ–ø–µ—Ä–∞—Ü–∏—é:", ["–£–¥–∞–ª–∏—Ç—å –ø–æ –±—Ä–µ–Ω–¥—É", "–£–¥–∞–ª–∏—Ç—å –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É"])
        
        if management_option == "–£–¥–∞–ª–∏—Ç—å –ø–æ –±—Ä–µ–Ω–¥—É":
            st.subheader("üè≠ –£–¥–∞–ª–∏—Ç—å –≤—Å–µ –∞—Ä—Ç–∏–∫—É–ª—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –±—Ä–µ–Ω–¥–∞")
            
            # Get list of available brands
            brands_result = catalog.conn.execute("SELECT DISTINCT brand FROM parts_data WHERE brand IS NOT NULL ORDER BY brand").pl()
            available_brands = brands_result['brand'].to_list() if not brands_result.is_empty() else []
            
            if available_brands:
                selected_brand = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –±—Ä–µ–Ω–¥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:", available_brands)
                
                col1, col2 = st.columns(2)
                
                with col1:
                    brand_norm_result = catalog.conn.execute("SELECT brand_norm FROM parts_data WHERE brand = ? LIMIT 1", [selected_brand]).fetchone()
                    if brand_norm_result:
                        brand_norm = brand_norm_result[0]
                    else:
                        brand_series = pl.Series([selected_brand])
                        normalized_series = catalog.normalize_key(brand_series)
                        brand_norm = normalized_series[0] if len(normalized_series) > 0 else ""
                    
                    count_result = catalog.conn.execute("SELECT COUNT(*) FROM parts_data WHERE brand_norm = ?", [brand_norm]).fetchone()
                    count_to_delete = count_result[0] if count_result else 0
                    
                    st.info(f"–ö —É–¥–∞–ª–µ–Ω–∏—é: **{count_to_delete}** –∑–∞–ø–∏—Å–µ–π –∏–∑ –±—Ä–µ–Ω–¥–∞ '{selected_brand}'")
                
                with col2:
                    confirm_delete_brand = st.checkbox("–Ø –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é —É–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π —ç—Ç–æ–≥–æ –±—Ä–µ–Ω–¥–∞", key=f"confirm_brand_{selected_brand}")
                    if st.button("‚ùå –£–¥–∞–ª–∏—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏ –±—Ä–µ–Ω–¥–∞", type="secondary", disabled=not confirm_delete_brand):
                        try:
                            deleted = catalog.delete_by_brand(brand_norm)
                            st.success(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–æ {deleted} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –±—Ä–µ–Ω–¥–∞ '{selected_brand}'")
                            st.rerun()
                        except Exception as e:
                            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {e}")
                    if not confirm_delete_brand:
                        st.caption("‚ö†Ô∏è –û—Ç–º–µ—Ç—å—Ç–µ —á–µ–∫–±–æ–∫—Å –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∫–Ω–æ–ø–∫–∏ —É–¥–∞–ª–µ–Ω–∏—è")
            else:
                st.warning("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è.")
        
        elif management_option == "–£–¥–∞–ª–∏—Ç—å –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É":
            st.subheader("üì¶ –£–¥–∞–ª–∏—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –∞—Ä—Ç–∏–∫—É–ª–∞")
            st.info("üí° –í–≤–µ–¥–∏—Ç–µ –∞—Ä—Ç–∏–∫—É–ª (–ø–æ–∏—Å–∫ –±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤)")
            
            input_artikul = st.text_input("–í–≤–µ–¥–∏—Ç–µ –∞—Ä—Ç–∏–∫—É–ª –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:")
            
            if input_artikul:
                col1, col2 = st.columns(2)
                
                with col1:
                    if input_artikul:
                        input_series = pl.Series([input_artikul])
                        normalized_series = catalog.normalize_key(input_series)
                        artikul_norm = normalized_series[0] if len(normalized_series) > 0 else ""
                    
                    count_result = catalog.conn.execute("SELECT COUNT(*) FROM parts_data WHERE artikul_norm = ?", [artikul_norm]).fetchone()
                    count_to_delete = count_result[0] if count_result else 0
                    
                    if count_to_delete > 0:
                        st.info(f"–ö —É–¥–∞–ª–µ–Ω–∏—é: **{count_to_delete}** –∑–∞–ø–∏—Å–µ–π –∞—Ä—Ç–∏–∫—É–ª–∞ '{input_artikul}'")
                    else:
                        st.warning(f"–ê—Ä—Ç–∏–∫—É–ª '{input_artikul}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –±–∞–∑–µ")
                
                with col2:
                    if count_to_delete > 0:
                        confirm_delete_artikul = st.checkbox("–Ø –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é —É–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π —ç—Ç–æ–≥–æ –∞—Ä—Ç–∏–∫—É–ª–∞", key=f"confirm_artikul_{artikul_norm}")
                        if st.button("‚ùå –£–¥–∞–ª–∏—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏ –∞—Ä—Ç–∏–∫—É–ª–∞", type="secondary", disabled=not confirm_delete_artikul):
                            try:
                                deleted = catalog.delete_by_artikul(artikul_norm)
                                st.success(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–æ {deleted} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞—Ä—Ç–∏–∫—É–ª–∞ '{input_artikul}'")
                                st.rerun()
                            except Exception as e:
                                st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {e}")
                        if not confirm_delete_artikul:
                            st.caption("‚ö†Ô∏è –û—Ç–º–µ—Ç—å—Ç–µ —á–µ–∫–±–æ–∫—Å –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∫–Ω–æ–ø–∫–∏ —É–¥–∞–ª–µ–Ω–∏—è")

if __name__ == "__main__":
    main()
