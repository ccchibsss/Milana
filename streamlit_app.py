import streamlit as st
import duckdb
import polars as pl
import io
import os
import time
import json
from pathlib import Path
from difflib import get_close_matches

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
DATA_DIR = Path("./auto_parts_data")
DATA_DIR.mkdir(exist_ok=True)
DB_PATH = DATA_DIR / "catalog.duckdb"

class AutoPartsCatalog:
    def __init__(self):
        self.conn = duckdb.connect(str(DB_PATH))
        self._setup_database()
        self._create_indexes()

    def _setup_database(self):
        # –¢–∞–±–ª–∏—Ü—ã
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE,
                width DOUBLE,
                height DOUBLE,
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                price_with_markup DOUBLE,
                category VARCHAR,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS recommended_prices (
                artikul_norm VARCHAR PRIMARY KEY,
                price DOUBLE
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS price_list (
                artikul VARCHAR,
                brand VARCHAR,
                quantity INTEGER,
                price DOUBLE,
                PRIMARY KEY (artikul, brand)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS markup_settings (
                id INTEGER PRIMARY KEY,
                total_markup DOUBLE,
                brand_markup JSON
            )
        """)
        if not self.conn.execute("SELECT 1 FROM markup_settings").fetchone():
            self.conn.execute("INSERT INTO markup_settings (id, total_markup, brand_markup) VALUES (1, 0, '{}')")
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS categories (
                name VARCHAR PRIMARY KEY,
                description VARCHAR
            )
        """)

    def _create_indexes(self):
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_oe ON oe_data(oe_number_norm)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_parts ON parts_data(artikul_norm, brand_norm)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_cross ON cross_references(oe_number_norm)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_cross_art ON cross_references(artikul_norm, brand_norm)")

    @staticmethod
    def normalize_key(series):
        return (
            series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
            .str.to_lowercase()
        )

    @staticmethod
    def clean_values(series):
        return (
            series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
        )

    def detect_columns(self, actual_cols, expected_cols):
        mapping = {}
        col_variants = {
            'oe_number': ['oe –Ω–æ–º–µ—Ä', 'oe', '–æe', '–Ω–æ–º–µ—Ä', 'code', 'OE'],
            'artikul': ['–∞—Ä—Ç–∏–∫—É–ª', 'article', 'sku'],
            'brand': ['–±—Ä–µ–Ω–¥', 'brand', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', 'manufacturer'],
            'name': ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ', 'name', '–æ–ø–∏—Å–∞–Ω–∏–µ', 'description'],
            'applicability': ['–ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', 'vehicle', 'applicability'],
            'barcode': ['—à—Ç—Ä–∏—Ö-–∫–æ–¥', 'barcode', '—à—Ç—Ä–∏—Ö–∫–æ–¥', 'ean', 'eac13'],
            'multiplicity': ['–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å —à—Ç', '–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å', 'multiplicity'],
            'length': ['–¥–ª–∏–Ω–∞ (—Å–º)', '–¥–ª–∏–Ω–∞', 'length', '–¥–ª–∏–Ω–Ω–∞'],
            'width': ['—à–∏—Ä–∏–Ω–∞ (—Å–º)', '—à–∏—Ä–∏–Ω–∞', 'width'],
            'height': ['–≤—ã—Å–æ—Ç–∞ (—Å–º)', '–≤—ã—Å–æ—Ç–∞', 'height'],
            'weight': ['–≤–µ—Å (–∫–≥)', '–≤–µ—Å, –∫–≥', '–≤–µ—Å', 'weight'],
            'image_url': ['—Å—Å—ã–ª–∫–∞', 'url', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 'image', '–∫–∞—Ä—Ç–∏–Ω–∫–∞'],
            'dimensions_str': ['–≤–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã', '—Ä–∞–∑–º–µ—Ä—ã', 'dimensions', 'size']
        }
        actual_lower = {c.lower(): c for c in actual_cols}
        for exp_col in expected_cols:
            variants = [v.lower() for v in col_variants.get(exp_col, [exp_col])]
            for var in variants:
                for act_lower, act_orig in actual_lower.items():
                    if var in act_lower:
                        mapping[act_orig] = exp_col
                        break
                if exp_col in mapping.values():
                    break
        return mapping

    def read_and_prepare_file(self, path, file_type):
        try:
            df = pl.read_excel(path, engine='calamine')
            if df.is_empty():
                return pl.DataFrame()
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {path}: {e}")
            return pl.DataFrame()
        schemas = {
            'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
            'cross': ['oe_number', 'artikul', 'brand']
        }
        expected_cols = schemas.get(file_type, [])
        col_mapping = self.detect_columns(df.columns, expected_cols)
        if not col_mapping:
            return pl.DataFrame()
        df = df.rename(col_mapping)
        if 'artikul' in df.columns:
            df = df.with_columns(artikul=self.clean_values(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand=self.clean_values(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number=self.clean_values(pl.col('oe_number')))
        key_cols = [c for c in ['oe_number', 'artikul', 'brand'] if c in df.columns]
        if key_cols:
            df = df.unique(subset=key_cols, keep='first')
        if 'artikul' in df.columns:
            df = df.with_columns(artikul_norm=self.normalize_key(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand_norm=self.normalize_key(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number_norm=self.normalize_key(pl.col('oe_number')))
        return df

    def upsert_data(self, table_name, df, pk):
        if df.is_empty():
            return
        df_unique = df.unique(keep='first')
        timestamp = int(time.time())
        self.conn.register(f"temp_{table_name}_{timestamp}", df_unique.to_arrow())
        pk_str = ", ".join([f'"{col}"' for col in pk])
        update_cols = [col for col in df_unique.columns if col not in pk]
        if update_cols:
            update_clause = ", ".join([f'"{col}"=excluded."{col}"' for col in update_cols])
        else:
            update_clause = "DO NOTHING"
        sql = f"""
            INSERT INTO {table_name}
            SELECT * FROM "temp_{table_name}_{timestamp}"
            ON CONFLICT ({pk_str}) {update_clause}
        """
        try:
            self.conn.execute(sql)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ {table_name}: {e}")
        finally:
            self.conn.unregister(f"temp_{table_name}_{timestamp}")

    def process_and_load(self, dataframes):
        st.info("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        total_steps = 2
        progress = st.progress(0)
        step = 0

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ OE
        if 'oe' in dataframes:
            step += 1
            progress.progress(step / total_steps, text=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ OE ({step}/{total_steps})")
            df_oe = dataframes['oe'].filter(pl.col('oe_number_norm') != "")
            oe_df = df_oe.select(['oe_number_norm', 'oe_number', 'name', 'applicability']).unique(subset=['oe_number_norm'])
            # –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é (–∑–¥–µ—Å—å —É–±—Ä–∞–ª–∏, —Ç–∞–∫ –∫–∞–∫ –ø—Ä–æ—Å–∏–ª–∏)
            # if 'name' in oe_df.columns:
            #     oe_df = oe_df.with_columns(self._category_by_name(pl.col('name')).alias('category'))
            # else:
            #     oe_df = oe_df.with_columns(pl.lit('–†–∞–∑–Ω–æ–µ').alias('category'))
            self.upsert_data('oe_data', oe_df, ['oe_number_norm'])
            cross_df = df_oe.filter(pl.col('artikul_norm') != "").select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ cross
        if 'cross' in dataframes:
            step += 1
            progress.progress(step / total_steps, text=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫—Ä–æ—Å—Å–æ–≤ ({step}/{total_steps})")
            df_cross = dataframes['cross'].filter(
                (pl.col('oe_number_norm') != "") & (pl.col('artikul_norm') != "")
            )
            self.upsert_data('cross_references', df_cross, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ parts
        step += 1
        progress.progress(step / total_steps, text=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—Ä—Ç–∏–∫—É–ª–∞ ({step}/{total_steps})")
        # –ú–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É
        progress.progress(1)
        time.sleep(0.5)
        st.success("üóÉÔ∏è –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

    def load_category_data(self, file_bytes):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Å –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è–º–∏ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏"""
        df = pl.read_excel(io.BytesIO(file_bytes))
        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        if '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ' not in df.columns or '–∫–∞—Ç–µ–≥–æ—Ä–∏—è' not in df.columns:
            st.error("–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ' –∏ '–∫–∞—Ç–µ–≥–æ—Ä–∏—è'")
            return
        df = df.select([
            pl.col('–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ'),
            pl.col('–∫–∞—Ç–µ–≥–æ—Ä–∏—è')
        ])
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç—å
        self._category_data = df

    def assign_categories(self, df_names):
        """–ü–æ–∏—Å–∫ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –∏ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        if not hasattr(self, '_category_data'):
            return pl.Series([''] * len(df_names))
        category_series = []
        categories = self._category_data['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ'].to_list()
        categories_lower = [cat.lower() for cat in categories]
        for name in df_names:
            name_lower = name.lower() if name else ''
            matches = get_close_matches(name_lower, categories_lower, n=1, cutoff=0.6)
            if matches:
                idx = categories_lower.index(matches[0])
                category_series.append(self._category_data['–∫–∞—Ç–µ–≥–æ—Ä–∏—è'][idx])
            else:
                category_series.append('')
        return pl.Series(category_series)

    def build_export_query(self, selected_columns=None, category_filter=None):
        desc_text = """–°–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: –Ω–æ–≤—ã–π (–≤ —É–ø–∞–∫–æ–≤–∫–µ).
–í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã ‚Äî –Ω–∞–¥–µ–∂–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è. 
–û–±–µ—Å–ø–µ—á—å—Ç–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –¥–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç—å –∏ –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ —Å –ø–æ–º–æ—â—å—é –Ω–∞—à–µ–≥–æ —à–∏—Ä–æ–∫–æ–≥–æ –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–µ–π.

–í –Ω–∞—à–µ–º –∫–∞—Ç–∞–ª–æ–≥–µ –≤—ã –Ω–∞–π–¥–µ—Ç–µ —Ç–æ—Ä–º–æ–∑–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, —Ñ–∏–ª—å—Ç—Ä—ã (–º–∞—Å–ª—è–Ω—ã–µ, –≤–æ–∑–¥—É—à–Ω—ã–µ, —Å–∞–ª–æ–Ω–Ω—ã–µ), —Å–≤–µ—á–∏ –∑–∞–∂–∏–≥–∞–Ω–∏—è, —Ä–∞—Å—Ö–æ–¥–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –∞–≤—Ç–æ—Ö–∏–º–∏—é, —ç–ª–µ–∫—Ç—Ä–∏–∫—É, –∞–≤—Ç–æ–º–∞—Å–ª–∞, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∞ —Ç–∞–∫–∂–µ –¥—Ä—É–≥–∏–µ –∫–æ–º–ø–ª–µ–∫—Ç—É—é—â–∏–µ, –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. 

–ú—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –±—ã—Å—Ç—Ä—É—é –¥–æ—Å—Ç–∞–≤–∫—É, –≤—ã–≥–æ–¥–Ω—ã–µ —Ü–µ–Ω—ã –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –¥–ª—è –ª—é–±–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ ‚Äî –∞–≤—Ç–æ–ª—é–±–∏—Ç–µ–ª—è, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –∏–ª–∏ –∞–≤—Ç–æ—Å–µ—Ä–≤–∏—Å–∞. 

–í—ã–±–∏—Ä–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –ª—É—á—à–µ–µ ‚Äî –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç –≤–µ–¥—É—â–∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π."""
        ctes = f"""
        WITH DescriptionTemplate AS (
            SELECT '{desc_text}' AS text
        ),
        PartDetails AS (
            SELECT
                cr.artikul_norm,
                cr.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(o.oe_number, '''', ''), '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]', '', 'g'), ', ') AS oe_list,
                ANY_VALUE(o.name) AS representative_name,
                ANY_VALUE(o.applicability) AS representative_applicability,
                ANY_VALUE(o.category) AS representative_category,
                ANY_VALUE(p.description) AS description,
                ANY_VALUE(p.category) AS category,
                ANY_VALUE(p.length) AS length,
                ANY_VALUE(p.width) AS width,
                ANY_VALUE(p.height) AS height,
                ANY_VALUE(p.weight) AS weight,
                ANY_VALUE(p.dimensions_str) AS dimensions_str,
                ANY_VALUE(p.analog_list) AS analog_list,
                ANY_VALUE(p.image_url) AS image_url,
                ANY_VALUE(p.oe_list) AS oe_list,
                ANY_VALUE(p.price_with_markup) AS price_with_markup,
                ROW_NUMBER() OVER (PARTITION BY cr.artikul_norm, cr.brand_norm ORDER BY o.oe_number) AS rn
            FROM cross_references cr
            LEFT JOIN oe_data o ON cr.oe_number_norm = o.oe_number_norm
            LEFT JOIN parts_data p ON cr.artikul_norm = p.artikul_norm AND cr.brand_norm = p.brand_norm
        """
        # –§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        if category_filter:
            categories_str = "', '".join(category_filter)
            ctes += f"\nWHERE p.category IN ('{categories_str}')\n"
        ctes += """
            GROUP BY cr.artikul_norm, cr.brand_norm
        )
        SELECT
        """
        if selected_columns:
            select_cols = ', '.join(selected_columns)
        else:
            select_cols = '*'
        ctes += f" {select_cols} FROM PartDetails p WHERE p.rn=1"
        return ctes

    def get_markups(self):
        row = self.conn.execute("SELECT total_markup, brand_markup FROM markup_settings WHERE id=1").fetchone()
        if row:
            total_markup, brand_markup_json = row
            brand_markup = json.loads(brand_markup_json) if brand_markup_json else {}
            return total_markup, brand_markup
        return 0, {}

    def set_markups(self, total_markup, brand_markup):
        self.conn.execute("""
            UPDATE markup_settings SET total_markup=?, brand_markup=?
            WHERE id=1
        """, [total_markup, json.dumps(brand_markup)])
        st.success("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞—Ü–µ–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")

    def load_recommended_prices(self, file_bytes):
        df = pl.read_excel(io.BytesIO(file_bytes))
        if '–∞—Ä—Ç–∏–∫—É–ª' not in df.columns or '—Ü–µ–Ω–∞' not in df.columns:
            st.error("–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å '–∞—Ä—Ç–∏–∫—É–ª' –∏ '—Ü–µ–Ω–∞'")
            return
        df = df.select([
            pl.col('–∞—Ä—Ç–∏–∫—É–ª').alias('artikul'),
            pl.col('—Ü–µ–Ω–∞').cast(pl.Float64)
        ])
        for row in df.iter_rows():
            artikul, price = row
            norm_series = self.normalize_key(pl.Series([artikul]))
            artikul_norm = norm_series[0]
            self.conn.execute("""
                INSERT INTO recommended_prices (artikul_norm, price)
                VALUES (?, ?)
                ON CONFLICT (artikul_norm) DO UPDATE SET price=excluded.price
            """, [artikul_norm, price])
        st.success("–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ —Ü–µ–Ω—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã!")

    def load_price_list(self, file_bytes):
        df = pl.read_excel(io.BytesIO(file_bytes))
        required_cols = ['–∞—Ä—Ç–∏–∫—É–ª', '–±—Ä–µ–Ω–¥', '–∫–æ–ª-–≤–æ', '—Ü–µ–Ω–∞']
        if not all(c in df.columns for c in required_cols):
            st.error("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–æ–∫ '–∞—Ä—Ç–∏–∫—É–ª', '–±—Ä–µ–Ω–¥', '–∫–æ–ª-–≤–æ', '—Ü–µ–Ω–∞'")
            return
        df = df.select([
            pl.col('–∞—Ä—Ç–∏–∫—É–ª'),
            pl.col('–±—Ä–µ–Ω–¥'),
            pl.col('–∫–æ–ª-–≤–æ').cast(pl.Int32),
            pl.col('—Ü–µ–Ω–∞').cast(pl.Float64)
        ])
        for row in df.iter_rows():
            artikul, brand, qty, price = row
            norm_artikul = self.normalize_key(pl.Series([artikul]))[0]
            norm_brand = self.normalize_key(pl.Series([brand]))[0]
            self.conn.execute("""
                INSERT INTO price_list (artikul, brand, quantity, price)
                VALUES (?, ?, ?, ?)
                ON CONFLICT (artikul, brand) DO UPDATE SET quantity=excluded.quantity, price=excluded.price
            """, [artikul, brand, qty, price])
        st.success("–ü—Ä–∞–π—Å-–ª–∏—Å—Ç –∑–∞–≥—Ä—É–∂–µ–Ω!")

    def get_filtered_exclusions(self, exclude_terms):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –∏—Å–∫–ª—é—á–∏—Ç—å –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—è–º"""
        exclude_list = [term.strip() for term in exclude_terms.split('|') if term.strip()]
        return exclude_list

    def filter_exclusions(self, df, exclude_terms):
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç DataFrame, –∏—Å–∫–ª—é—á–∞—è —Å—Ç—Ä–æ–∫–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—è–º"""
        exclude_list = self.get_filtered_exclusions(exclude_terms)
        if not exclude_list:
            return df
        mask = pl.Series([False] * len(df))
        for term in exclude_list:
            mask = mask | df['name'].str.contains(term, case=False)
        return df.filter(~mask)

    def export_data(self, columns=None, exclude_terms=None, category_filter=None):
        """
        –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –∏—Å–∫–ª—é—á–µ–Ω–∏–π –∏ –≤—ã–±–æ—Ä–∞ –∫–æ–ª–æ–Ω–æ–∫.
        columns - —Å–ø–∏—Å–æ–∫ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ –∂–µ–ª–∞–µ–º–æ–º –ø–æ—Ä—è–¥–∫–µ.
        exclude_terms - —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è (—á–µ—Ä–µ–∑ |), –º–æ–≥—É—Ç –±—ã—Ç—å —Ç–æ—á–Ω—ã–µ –∏–ª–∏ —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è.
        category_filter - —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.
        """
        query = self.build_export_query(selected_columns=columns, category_filter=category_filter)
        df = self._run_query(query)
        if df is None or df.is_empty():
            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞.")
            return None
        if exclude_terms:
            df = self.filter_exclusions(df, exclude_terms)
        return df

    def _run_query(self, query):
        try:
            return pl.read_sql(query, self.conn)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return None

    def get_statistics(self):
        total_parts = self.conn.execute("SELECT COUNT(*) FROM parts_data").fetchone()[0]
        total_oe = self.conn.execute("SELECT COUNT(*) FROM oe_data").fetchone()[0]
        total_brands = self.conn.execute("SELECT COUNT(DISTINCT brand) FROM parts_data").fetchone()[0]
        top_brands = self.conn.execute("""
            SELECT brand, COUNT(*) as cnt FROM parts_data GROUP BY brand ORDER BY cnt DESC LIMIT 10
        """).fetchdf()
        categories = self.conn.execute("""
            SELECT category, COUNT(*) as cnt FROM parts_data GROUP BY category ORDER BY cnt DESC
        """).fetchdf()
        return {
            'total_parts': total_parts,
            'total_oe': total_oe,
            'total_brands': total_brands,
            'top_brands': top_brands,
            'categories': categories
        }

    def show_export_interface(self):
        st.subheader("–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")
        all_cols = [
            'artikul', 'brand', 'category', 'length', 'width', 'height', 'weight',
            'image_url', 'description', 'oe_list', 'price_with_markup'
        ]
        selected_cols = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ (–≤ –∂–µ–ª–∞–µ–º–æ–º –ø–æ—Ä—è–¥–∫–µ)", all_cols, default=all_cols)
        selected_cols = list(selected_cols)

        exclude_input = st.text_input("–ò—Å–∫–ª—é—á–∏—Ç—å –ø–æ–∑–∏—Ü–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é (—á–µ—Ä–µ–∑ |)", "")

        categories = self.conn.execute("SELECT DISTINCT category FROM parts_data").fetchdf()['category'].tolist()
        categories.insert(0, '–í—Å–µ')
        category_filter = st.multiselect("–§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º", categories, default=['–í—Å–µ'])
        if '–í—Å–µ' in category_filter:
            category_filter = None
        elif not category_filter:
            category_filter = None
        else:
            category_filter = category_filter

        total_markup_value = st.number_input("–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞ (%)", value=0.0, step=0.1)
        brand_markup_df = self.conn.execute("SELECT brand, COUNT(*) as cnt FROM parts_data GROUP BY brand").fetchdf()
        brand_markup_dict = {}
        st.write("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞—Ü–µ–Ω–æ–∫ –ø–æ –±—Ä–µ–Ω–¥–∞–º:")
        for index, row in brand_markup_df.iterrows():
            brand = row['brand']
            default_markup = 0.0
            markup_value = st.number_input(f"{brand}", value=default_markup, step=0.1, key=f"markup_{brand}")
            brand_markup_dict[brand] = markup_value

        if st.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞—Ü–µ–Ω–∫–∏"):
            self.set_markups(total_markup_value, brand_markup_dict)

        if st.button("–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ"):
            df = self.export_data(
                columns=selected_cols,
                exclude_terms=exclude_input,
                category_filter=None if category_filter is None or '–í—Å–µ' in category_filter else category_filter
            )
            if df is not None:
                total_markup, brand_markup = self.get_markups()
                if 'price_with_markup' in df.columns:
                    df = df.with_columns(
                        pl.col('price_with_markup').apply(
                            lambda p: self.apply_markup(p, total_markup, brand_markup, None)
                        ).alias('price_with_markup')
                    )
                buffer = io.BytesIO()
                df.write_excel(buffer)
                buffer.seek(0)
                filename = f"export_{int(time.time())}.xlsx"
                st.download_button("–°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª", data=buffer, file_name=filename, mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

    def apply_markup(self, price, total_markup, brand_markup_dict, brand):
        markup = total_markup
        if brand and brand in brand_markup_dict:
            markup += brand_markup_dict[brand]
        return price * (1 + markup / 100)

    def get_markups(self):
        row = self.conn.execute("SELECT total_markup, brand_markup FROM markup_settings WHERE id=1").fetchone()
        if row:
            total_markup, brand_markup_json = row
            brand_markup = json.loads(brand_markup_json) if brand_markup_json else {}
            return total_markup, brand_markup
        return 0, {}

    def set_markups(self, total_markup, brand_markup):
        self.conn.execute("""
            UPDATE markup_settings SET total_markup=?, brand_markup=?
            WHERE id=1
        """, [total_markup, json.dumps(brand_markup)])
        st.success("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞—Ü–µ–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")

# –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
def main():
    st.set_page_config(page_title="AutoParts Catalog", layout="wide")
    st.title("üöó AutoParts Catalog ‚Äî –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ —ç–∫—Å–ø–æ—Ä—Ç")
    catalog = AutoPartsCatalog()

    menu = st.sidebar.radio("–ú–µ–Ω—é", ["–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö", "–≠–∫—Å–ø–æ—Ä—Ç", "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Ü–µ–Ω", "–ü—Ä–∞–π—Å-–ª–∏—Å—Ç"])

    if menu == "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö":
        st.subheader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        col1, col2 = st.columns(2)
        with col1:
            file_oe = st.file_uploader("–û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (OE)", type=['xlsx', 'xls'])
            file_cross = st.file_uploader("–ö—Ä–æ—Å—Å—ã (OE ‚Üí –ê—Ä—Ç–∏–∫—É–ª)", type=['xlsx', 'xls'])
            file_barcode = st.file_uploader("–®—Ç—Ä–∏—Ö-–∫–æ–¥—ã", type=['xlsx', 'xls'])
        with col2:
            file_dim = st.file_uploader("–í–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã", type=['xlsx', 'xls'])
            file_img = st.file_uploader("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", type=['xlsx', 'xls'])
            file_category = st.file_uploader("–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ (–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ, –∫–∞—Ç–µ–≥–æ—Ä–∏—è)", type=['xlsx', 'xls'])

        files_map = {
            'oe': file_oe,
            'cross': file_cross,
            'barcode': file_barcode,
            'dimensions': file_dim,
            'images': file_img,
            'categories': file_category
        }

        if st.button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª—ã"):
            dataframes = {}
            for key, uploaded in files_map.items():
                if uploaded:
                    filename = f"{key}_{int(time.time())}_{uploaded.name}"
                    path = DATA_DIR / filename
                    with open(path, "wb") as f:
                        f.write(uploaded.read())
                    df = catalog.read_and_prepare_file(str(path), key)
                    dataframes[key] = df
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
            if 'categories' in files_map and files_map['categories']:
                catalog.load_category_data(files_map['categories'].read())

            if dataframes:
                catalog.process_and_load(dataframes)
                # –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏, –ø—Ä–∏—Å–≤–æ–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
                if hasattr(catalog, '_category_data'):
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º
                    # –î–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–æ–∫ –≤ parts_data
                    df_parts = catalog.conn.execute("SELECT artikul_norm, brand_norm, artikul, brand FROM parts_data").fetchdf()
                    if not df_parts.empty:
                        categories_assigned = catalog.assign_categories(pl.Series(df_parts['artikul']))
                        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ parts_data
                        for idx, row in df_parts.iterrows():
                            category_name = categories_assigned[idx]
                            catalog.conn.execute("""
                                UPDATE parts_data SET category=? WHERE artikul_norm=? AND brand_norm=?
                            """, [category_name, row['artikul_norm'], row['brand_norm']])
                        st.success("–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è–º —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏—Å–≤–æ–µ–Ω—ã!")
            else:
                st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")

    elif menu == "–≠–∫—Å–ø–æ—Ä—Ç":
        catalog.show_export_interface()

    elif menu == "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞":
        stats = catalog.get_statistics()
        st.metric("–ê—Ä—Ç–∏–∫—É–ª–æ–≤", stats['total_parts'])
        st.metric("OE", stats['total_oe'])
        st.metric("–ë—Ä–µ–Ω–¥–æ–≤", stats['total_brands'])
        st.subheader("–¢–æ–ø –±—Ä–µ–Ω–¥–æ–≤")
        st.dataframe(stats['top_brands'])
        st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
        st.dataframe(stats['categories'])
        st.bar_chart(stats['categories'].set_index('category')['cnt'])

    elif menu == "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Ü–µ–Ω":
        st.subheader("–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —Ü–µ–Ω–∞–º")
        uploaded = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å —Ü–µ–Ω–∞–º–∏", type=['xlsx', 'xls'])
        if uploaded:
            catalog.load_recommended_prices(uploaded.read())

    elif menu == "–ü—Ä–∞–π—Å-–ª–∏—Å—Ç":
        st.subheader("–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–∞")
        uploaded = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç", type=['xlsx', 'xls'])
        if uploaded:
            catalog.load_price_list(uploaded.read())

if __name__ == "__main__":
    main()
