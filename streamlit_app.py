import polars as pl
import duckdb
import streamlit as st
import os
import time
import io
import zipfile
import logging
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

EXCEL_ROW_LIMIT = 1_000_000

class AutoPartsCatalog:
    def __init__(self):
        self.data_dir = Path("./auto_parts_data")
        self.data_dir.mkdir(exist_ok=True)
        self.db_path = self.data_dir / "catalog.duckdb"
        self.conn = duckdb.connect(database=str(self.db_path))
        self.setup_database()

        # –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤
        self._setup_file_processing_functions()

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ü–µ–Ω
        self.overall_markup = 1.2  # 20% –Ω–∞—Ü–µ–Ω–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.brand_markups = {}  # –ø–æ –±—Ä–µ–Ω–¥–∞–º

        st.set_page_config(
            page_title="AutoParts Catalog 10M+", 
            layout="wide",
            page_icon="üöó"
        )

    def _setup_file_processing_functions(self):
        def read_and_prepare_file(file_path: str, file_type: str):
            logger.info(f"–ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞: {file_type} ({file_path})")
            try:
                if not os.path.exists(file_path):
                    logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                    return pl.DataFrame()
                if os.path.getsize(file_path) == 0:
                    logger.warning(f"–§–∞–π–ª –ø—É—Å—Ç: {file_path}")
                    return pl.DataFrame()
                df = pl.read_excel(file_path, engine='calamine')
                if df.is_empty():
                    logger.warning(f"–§–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω, –Ω–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã—Ö: {file_path}")
                    return pl.DataFrame()
            except Exception as e:
                logger.exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {file_path}: {e}")
                return pl.DataFrame()

            schemas = {
                'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
                'barcode': ['brand', 'artikul', 'barcode', 'multiplicity'],
                'dimensions': ['artikul', 'brand', 'length', 'width', 'height', 'weight', 'dimensions_str'],
                'images': ['artikul', 'brand', 'image_url'],
                'cross': ['oe_number', 'artikul', 'brand'],
                'price': ['artikul', 'brand', 'quantity', 'price']
            }
            expected_cols = schemas.get(file_type, [])
            column_mapping = self.detect_columns(df.columns, expected_cols)
            if not column_mapping:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ñ–∞–π–ª–∞ {file_type}. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {df.columns}")
                return pl.DataFrame()
            df = df.rename(column_mapping)

            # –û—á–∏—Å—Ç–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π
            for col in ['artikul', 'brand', 'oe_number', 'name', 'applicability']:
                if col in df.columns:
                    df = df.with_columns(**{col: self.clean_values(pl.col(col))})

            # –î–ª—è —Ñ–∞–π–ª–∞ —Ü–µ–Ω, –ø—Ä–∏–≤–µ–¥–µ–º —Ü–µ–Ω—É –∫ float
            if 'price' in df.columns:
                df = df.with_columns(price=pl.col('price').cast(pl.Float64))
            if 'quantity' in df.columns:
                df = df.with_columns(quantity=pl.col('quantity').cast(pl.Int64))

            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∫–ª—é—á—É
            key_cols = [col for col in ['artikul', 'brand', 'oe_number'] if col in df.columns]
            if key_cols:
                df = df.unique(subset=key_cols, keep='first')
            return df

        def detect_columns(self, actual_columns, expected_columns):
            mapping = {}
            column_variants = {
                'oe_number': ['oe –Ω–æ–º–µ—Ä', 'oe', '–æe', '–Ω–æ–º–µ—Ä', 'code', 'OE'],
                'artikul': ['–∞—Ä—Ç–∏–∫—É–ª', 'article', 'sku'],
                'brand': ['–±—Ä–µ–Ω–¥', 'brand', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', 'manufacturer'],
                'name': ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ', 'name', '–æ–ø–∏—Å–∞–Ω–∏–µ', 'description'],
                'applicability': ['–ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', 'vehicle', 'applicability'],
                'barcode': ['—à—Ç—Ä–∏—Ö-–∫–æ–¥', 'barcode', '—à—Ç—Ä–∏—Ö–∫–æ–¥', 'ean', 'eac13'],
                'multiplicity': ['–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å —à—Ç', '–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å', 'multiplicity'],
                'length': ['–¥–ª–∏–Ω–∞ (—Å–º)', '–¥–ª–∏–Ω–∞', 'length', '–¥–ª–∏–Ω–Ω–∞'],
                'width': ['—à–∏—Ä–∏–Ω–∞ (—Å–º)', '—à–∏—Ä–∏–Ω–∞', 'width'],
                'height': ['–≤—ã—Å–æ—Ç–∞ (—Å–º)', '–≤—ã—Å–æ—Ç–∞', 'height'],
                'weight': ['–≤–µ—Å (–∫–≥)', '–≤–µ—Å, –∫–≥', '–≤–µ—Å', 'weight'],
                'image_url': ['—Å—Å—ã–ª–∫–∞', 'url', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 'image', '–∫–∞—Ä—Ç–∏–Ω–∫–∞'],
                'dimensions_str': ['–≤–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã', '—Ä–∞–∑–º–µ—Ä—ã', 'dimensions', 'size'],
                'quantity': ['–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', 'quantity', 'qty'],
                'price': ['—Ü–µ–Ω–∞', 'price']
            }
            actual_lower = {col.lower(): col for col in actual_columns}
            for expected in expected_columns:
                variants = [v.lower() for v in column_variants.get(expected, [expected])]
                for variant in variants:
                    for actual_l, actual_orig in actual_lower.items():
                        if variant in actual_l:
                            mapping[actual_orig] = expected
                            break
                    if expected in mapping.values():
                        break
            return mapping

        def clean_values(self, value_series: pl.Series) -> pl.Series:
            return (
                value_series
                .fill_null("")
                .cast(pl.Utf8)
                .str.replace_all("'", "")
                .str.replace_all(r"[^0-9A-Za-zA-za-—è–Å—ë`\-\s]", "")
                .str.replace_all(r"\s+", " ")
                .str.strip_chars()
            )

        def normalize_key(self, key_series: pl.Series) -> pl.Series:
            return (
                key_series
                .fill_null("")
                .cast(pl.Utf8)
                .str.replace_all("'", "")
                .str.replace_all(r"[^0-9A-Za-zA-za-—è–Å—ë`\-\s]", "")
                .str.replace_all(r"\s+", " ")
                .str.strip_chars()
                .str.to_lowercase()
            )

        self.read_and_prepare_file = read_and_prepare_file
        self.detect_columns = detect_columns
        self.clean_values = clean_values
        self.normalize_key = normalize_key

    def setup_database(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE, 
                width DOUBLE,
                height DOUBLE, 
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                price FLOAT,
                quantity INTEGER,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)

    def create_indexes(self):
        st.info("–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤...")
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_oe ON oe_data(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_parts ON parts_data(artikul_norm, brand_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross ON cross_references(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_artikul ON cross_references(artikul_norm, brand_norm)"
        ]
        for sql in indexes:
            self.conn.execute(sql)
        st.success("–ò–Ω–¥–µ–∫—Å—ã —Å–æ–∑–¥–∞–Ω—ã.")

    def process_and_load_data(self, dataframes: dict):
        # –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ - –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ —Ñ–∞–π–ª–æ–≤
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–ª–∏ –≤—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∞—Ä—Ç–∏–∫—É–ª–∞–º
        if 'price' in dataframes:
            df_price = dataframes['price']
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ü–µ–Ω—É –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É
            for _, row in df_price.iterrows():
                artikul_norm = self.normalize_key(pl.Series([row['artikul']]))[0]
                brand_norm = self.normalize_key(pl.Series([row['brand']]))[0]
                quantity = row.get('quantity', 0)
                price = row.get('price', 0.0)
                self.conn.execute("""
                    UPDATE parts_data SET price = ?, quantity = ? WHERE artikul_norm = ? AND brand_norm = ?
                """, [price, quantity, artikul_norm, brand_norm])
                # –ò–ª–∏ –≤—Å—Ç–∞–≤–ª—è–µ–º, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                self.conn.execute("""
                    INSERT INTO parts_data (artikul_norm, brand_norm, artikul, brand, price, quantity)
                    SELECT ?, ?, ?, ?, ?, ?
                    WHERE NOT EXISTS (
                        SELECT 1 FROM parts_data WHERE artikul_norm=? AND brand_norm=?
                    )
                """, [artikul_norm, brand_norm, row['artikul'], row['brand'], price, quantity, artikul_norm, brand_norm])

        # –ú–æ–∂–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å –æ—Å—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ.

    def update_pricing(self, overall_markup=None, brand_markups=None):
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—â—É—é –Ω–∞—Ü–µ–Ω–∫—É –∏ –ø–æ –±—Ä–µ–Ω–¥–∞–º
        if overall_markup:
            self.overall_markup = overall_markup
        if brand_markups:
            self.brand_markups.update(brand_markups)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Ü–µ–Ω—ã –≤ –±–∞–∑–µ
        cursor = self.conn.execute("SELECT artikul_norm, brand_norm, price FROM parts_data WHERE price IS NOT NULL")
        for artikul_norm, brand_norm, base_price in cursor.fetchall():
            markup = self.brand_markups.get(brand_norm, self.overall_markup)
            new_price = base_price * markup
            self.conn.execute("""
                UPDATE parts_data SET price = ? WHERE artikul_norm = ? AND brand_norm = ?
            """, [new_price, artikul_norm, brand_norm])
        st.success("–¶–µ–Ω—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã —Å —É—á–µ—Ç–æ–º –Ω–∞—Ü–µ–Ω–æ–∫.")

    def load_price_file(self, file_path):
        df = self.read_and_prepare_file(file_path, 'price')
        if df and not df.is_empty():
            self.process_and_load_data({'price': df})
            st.success("–ü—Ä–∞–π—Å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω.")
        else:
            st.warning("–ü—Ä–∞–π—Å —Ñ–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –µ–≥–æ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å.")

    def merge_all_data_parallel(self, file_paths: dict):
        start_time = time.time()
        dataframes = {}
        with ThreadPoolExecutor() as executor:
            futures = {
                executor.submit(self.read_and_prepare_file, path, ftype): ftype
                for ftype, path in file_paths.items()
            }
            for future in as_completed(futures):
                ftype = futures[future]
                try:
                    df = future.result()
                    if not df.is_empty():
                        dataframes[ftype] = df
                        st.success(f"–§–∞–π–ª {ftype} –∑–∞–≥—Ä—É–∂–µ–Ω: {len(df)} —Å—Ç—Ä–æ–∫")
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {ftype}: {e}")

        if not dataframes:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏.")
            return {}
        self.process_and_load_data(dataframes)
        stats = {
            'processing_time': time.time() - start_time,
            'total_records': self.get_total_records()
        }
        self.create_indexes()
        return stats

    def get_total_records(self):
        try:
            res = self.conn.execute("SELECT COUNT(*) FROM parts_data").fetchone()
            return res[0] if res else 0
        except:
            return 0

    def get_statistics(self):
        stats = {}
        try:
            stats['total_parts'] = self.get_total_records()
            res_oe = self.conn.execute("SELECT COUNT(*) FROM oe_data").fetchone()
            stats['total_oe'] = res_oe[0] if res_oe else 0
            res_b = self.conn.execute("SELECT COUNT(DISTINCT brand) FROM parts_data WHERE brand IS NOT NULL").fetchone()
            stats['total_brands'] = res_b[0] if res_b else 0
            # –¢–æ–ø –±—Ä–µ–Ω–¥–æ–≤
            top_b = self.conn.execute("SELECT brand, COUNT(*) FROM parts_data WHERE brand IS NOT NULL GROUP BY brand ORDER BY 2 DESC LIMIT 10").fetchall()
            stats['top_brands'] = pl.DataFrame(top_b, schema=["brand", "count"])
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
            cats = self.conn.execute("SELECT category, COUNT(*) FROM oe_data WHERE category IS NOT NULL GROUP BY category ORDER BY 2 DESC").fetchall()
            stats['categories'] = pl.DataFrame(cats, schema=["category", "count"])
        except:
            pass
        return stats

    def build_export_query(self, selected_columns=None, exclude_positions=None):
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å —É—á–µ—Ç–æ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        # exclude_positions - —Å—Ç—Ä–æ–∫–∞, –≥–¥–µ –ø–æ–∑–∏—Ü–∏–∏ —á–µ—Ä–µ–∑ | –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        base_select = """
        SELECT
            a.artikul_norm, a.brand_norm,
            a.artikul, a.brand,
            a.price, a.quantity,
            p.length, p.width, p.height, p.weight, p.dimensions_str, p.description, p.image_url
        FROM parts_data a
        LEFT JOIN oe_data o ON a.oe_number_norm = o.oe_number_norm
        LEFT JOIN parts_data p ON a.artikul_norm = p.artikul_norm AND a.brand_norm = p.brand_norm
        """
        if selected_columns:
            # –ú–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å
            pass

        where_clauses = []
        if exclude_positions:
            # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ | –∏ –¥–æ–±–∞–≤–ª—è–µ–º —É—Å–ª–æ–≤–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
            positions = [pos.strip() for pos in exclude_positions.split('|')]
            for pos in positions:
                where_clauses.append(f"a.artikul NOT LIKE '%{pos}%'")
        if where_clauses:
            base_select += " WHERE " + " AND ".join(where_clauses)
        return base_select

    def export_to_csv(self, filename, exclude_positions=None):
        query = self.build_export_query(exclude_positions=exclude_positions)
        df = self.conn.execute(query).pl()
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        for col in ['length', 'width', 'height', 'weight', 'price']:
            if col in df.columns:
                df = df.with_columns(
                    pl.when(pl.col(col).is_not_null())
                    .then(pl.col(col).cast(pl.Utf8))
                    .otherwise(pl.lit(""))
                    .alias(col)
                )

        buf = io.StringIO()
        df.write_csv(buf, separator=';')
        csv_text = buf.getvalue()
        with open(filename, 'wb') as f:
            f.write(b'\xef\xbb\xbf')  # BOM
            f.write(csv_text.encode('utf-8'))
        size_mb = os.path.getsize(filename) / (1024 * 1024)
        st.success(f"–≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {filename} ({size_mb:.2f} –ú–ë)")

    def export_to_excel(self, filename, exclude_positions=None):
        query = self.build_export_query(exclude_positions=exclude_positions)
        total_records = self.get_total_records()
        num_files = (total_records + EXCEL_ROW_LIMIT - 1) // EXCEL_ROW_LIMIT
        exported_files = []

        for i in range(num_files):
            offset = i * EXCEL_ROW_LIMIT
            q = query + f" LIMIT {EXCEL_ROW_LIMIT} OFFSET {offset}"
            df = self.conn.execute(q).pl()
            for col in ['length', 'width', 'height', 'weight', 'price']:
                if col in df.columns:
                    df = df.with_columns(
                        pl.when(pl.col(col).is_not_null())
                        .then(pl.col(col).cast(pl.Utf8))
                        .otherwise(pl.lit(""))
                        .alias(col)
                    )
            part_path = Path(filename).with_name(f"{Path(filename).stem}_part_{i+1}.xlsx")
            df.write_excel(str(part_path))
            exported_files.append(part_path)

        # ZIP –µ—Å–ª–∏ –±–æ–ª–µ–µ 1 —Ñ–∞–π–ª–∞
        if len(exported_files) > 1:
            zip_path = Path(filename).with_suffix('.zip')
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for f in exported_files:
                    zipf.write(f, f.name)
                    os.remove(f)
            final_path = zip_path
        else:
            final_path = exported_files[0]
        size_mb = os.path.getsize(final_path) / (1024 * 1024)
        st.success(f"–≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {final_path} ({size_mb:.2f} –ú–ë)")

    def export_to_parquet(self, filename, exclude_positions=None):
        query = self.build_export_query(exclude_positions=exclude_positions)
        df = self.conn.execute(query).pl()
        for col in ['length', 'width', 'height', 'weight', 'price']:
            if col in df.columns:
                df = df.with_columns(
                    pl.when(pl.col(col).is_not_null())
                    .then(pl.col(col).cast(pl.Utf8))
                    .otherwise(pl.lit(""))
                    .alias(col)
                )
        df.write_parquet(filename)
        size_mb = os.path.getsize(filename) / (1024 * 1024)
        st.success(f"–≠–∫—Å–ø–æ—Ä—Ç –≤ Parquet –∑–∞–≤–µ—Ä—à–µ–Ω: {filename} ({size_mb:.2f} –ú–ë)")

    def add_price_data(self, filepath):
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–π—Å–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã
        df = self.read_and_prepare_file(filepath, 'price')
        if df and not df.is_empty():
            for _, row in df.iterrows():
                artikul_norm = self.normalize_key(pl.Series([row['artikul']]))[0]
                brand_norm = self.normalize_key(pl.Series([row['brand']]))[0]
                quantity = row.get('quantity', 0)
                price = row.get('price', 0.0)
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ
                self.conn.execute("""
                    UPDATE parts_data SET price = ?, quantity = ? WHERE artikul_norm = ? AND brand_norm = ?
                """, [price, quantity, artikul_norm, brand_norm])
                # –ò–ª–∏ –≤—Å—Ç–∞–≤–∫–∞
                self.conn.execute("""
                    INSERT INTO parts_data (artikul_norm, brand_norm, artikul, brand, price, quantity)
                    SELECT ?, ?, ?, ?, ?, ?
                    WHERE NOT EXISTS (
                        SELECT 1 FROM parts_data WHERE artikul_norm=? AND brand_norm=?
                    )
                """, [artikul_norm, brand_norm, row['artikul'], row['brand'], price, quantity, artikul_norm, brand_norm])
            st.success("–ü—Ä–∞–π—Å —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω –∏ –æ–±–Ω–æ–≤–ª–µ–Ω.")
        else:
            st.warning("–ü—Ä–∞–π—Å —Ñ–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –µ–≥–æ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å.")

    def set_markup(self, overall=None, brand_dict=None):
        if overall:
            self.overall_markup = overall
        if brand_dict:
            self.brand_markups.update(brand_dict)
        self.update_prices()

    def update_prices(self):
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ü–µ–Ω—ã –ø–æ —Ç–µ–∫—É—â–∏–º –Ω–∞—Ü–µ–Ω–∫–∞–º
        cursor = self.conn.execute("SELECT artikul_norm, brand_norm, price FROM parts_data WHERE price IS NOT NULL")
        for artikul_norm, brand_norm, base_price in cursor.fetchall():
            markup = self.brand_markups.get(brand_norm, self.overall_markup)
            new_price = base_price * markup
            self.conn.execute("""
                UPDATE parts_data SET price = ? WHERE artikul_norm = ? AND brand_norm = ?
            """, [new_price, artikul_norm, brand_norm])
        st.success("–¶–µ–Ω—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã —Å —É—á–µ—Ç–æ–º –Ω–∞—Ü–µ–Ω–æ–∫.")

    def partial_search(self, search_text):
        # –ü–æ–∏—Å–∫ –ø–æ –∞—Ä—Ç–∏–∫—É–ª–∞–º, –±—Ä–µ–Ω–¥–∞–º –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º
        query = """
        SELECT a.artikul, a.brand, a.description, a.price, a.quantity
        FROM parts_data a
        LEFT JOIN oe_data o ON a.oe_number_norm = o.oe_number_norm
        WHERE a.artikul LIKE ? OR a.brand LIKE ? OR o.name LIKE ?
        """
        pattern = f"%{search_text}%"
        df = self.conn.execute(query, [pattern, pattern, pattern]).pl()
        if df.shape[0] == 0:
            st.info("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        else:
            st.dataframe(df.to_pandas())

# ================== –û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ ==================

def main():
    st.title("üöó AutoParts Catalog - –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞")
    catalog = AutoPartsCatalog()

    st.sidebar.title("üß≠ –ú–µ–Ω—é")
    option = st.sidebar.radio("–î–µ–π—Å—Ç–≤–∏—è", ["–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö", "–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–π—Å–∞", "–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ü–µ–Ω", "–≠–∫—Å–ø–æ—Ä—Ç", "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "–ü–æ–∏—Å–∫"])

    if option == "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö":
        st.header("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
        cols = st.columns(2)
        with cols[0]:
            oe_file = st.file_uploader("OE (–ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ)", type=['xlsx'])
            cross_file = st.file_uploader("–ö—Ä–æ—Å—Å—ã", type=['xlsx'])
            barcode_file = st.file_uploader("–®—Ç—Ä–∏—Ö–∫–æ–¥—ã", type=['xlsx'])
        with cols[1]:
            dimensions_file = st.file_uploader("–í–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã", type=['xlsx'])
            images_file = st.file_uploader("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", type=['xlsx'])
            price_file = st.file_uploader("–ü—Ä–∞–π—Å (–Ω–æ–≤—ã–π)", type=['xlsx'])

        if st.button("–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å"):
            file_paths = {}
            for name, file in [('oe', oe_file), ('cross', cross_file), ('barcode', barcode_file),
                               ('dimensions', dimensions_file), ('images', images_file), ('price', price_file)]:
                if file:
                    filename = f"{name}_{int(time.time())}_{file.name}"
                    path = catalog.data_dir / filename
                    with open(path, 'wb') as f:
                        f.write(file.getvalue())
                    file_paths[name] = str(path)

            # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–π—Å–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
            if 'price' in file_paths:
                catalog.add_price_data(file_paths['price'])

            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
            if file_paths:
                catalog.merge_all_data_parallel(file_paths)

    elif option == "–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–π—Å–∞":
        st.header("üîß –û–±–Ω–æ–≤–∏—Ç—å —Ü–µ–Ω—ã")
        overall_markup = st.number_input("–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞ (%)", min_value=0.0, max_value=100.0, value=20.0)
        brand_markups_input = st.text_input("–ù–∞—Ü–µ–Ω–∫–∏ –ø–æ –±—Ä–µ–Ω–¥–∞–º (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é: –±—Ä–µ–Ω–¥=–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç)", "")
        brand_dict = {}
        if brand_markups_input:
            pairs = [p.strip() for p in brand_markups_input.split(',')]
            for pair in pairs:
                if '=' in pair:
                    brand, coeff = pair.split('=')
                    try:
                        brand_dict[brand.strip()] = float(coeff.strip()) / 100.0
                    except:
                        pass
        catalog.set_markup(overall=1 + overall_markup/100.0, brand_dict=brand_dict)

    elif option == "–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ü–µ–Ω":
        st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ü–µ–Ω")
        overall_markup = st.number_input("–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞ (%)", min_value=0.0, max_value=100.0, value=20.0)
        brand_markups_input = st.text_input("–ù–∞—Ü–µ–Ω–∫–∏ –ø–æ –±—Ä–µ–Ω–¥–∞–º (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é: –±—Ä–µ–Ω–¥=–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç)", "")
        brand_dict = {}
        if brand_markups_input:
            pairs = [p.strip() for p in brand_markups_input.split(',')]
            for pair in pairs:
                if '=' in pair:
                    brand, coeff = pair.split('=')
                    try:
                        brand_dict[brand.strip()] = float(coeff.strip()) / 100.0
                    except:
                        pass
        catalog.set_markup(overall=1 + overall_markup/100.0, brand_dict=brand_dict)

    elif option == "–≠–∫—Å–ø–æ—Ä—Ç":
        catalog.show_export_interface()

    elif option == "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞":
        with st.spinner("–°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏..."):
            stats = catalog.get_statistics()
        st.write(f"–í—Å–µ–≥–æ –∞—Ä—Ç–∏–∫—É–ª–æ–≤: {stats.get('total_parts', 0):,}")
        st.write(f"–í—Å–µ–≥–æ OE: {stats.get('total_oe', 0):,}")
        st.write(f"–ë—Ä–µ–Ω–¥–æ–≤: {stats.get('total_brands', 0):,}")
        if not stats['top_brands'].is_empty():
            st.subheader("–¢–æ–ø –±—Ä–µ–Ω–¥–æ–≤")
            st.dataframe(stats['top_brands'].to_pandas())
        if not stats['categories'].is_empty():
            st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
            st.bar_chart(stats['categories'].to_pandas().set_index('category'))

    elif option == "–ü–æ–∏—Å–∫":
        search_text = st.text_input("–í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ, –∞—Ä—Ç–∏–∫—É–ª –∏–ª–∏ –±—Ä–µ–Ω–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞")
        if st.button("–ò—Å–∫–∞—Ç—å"):
            catalog.partial_search(search_text)

if __name__ == "__main__":
    main()
