import polars as pl
import duckdb
import streamlit as st
import os
import time
import io
import zipfile
from pathlib import Path
import warnings

warnings.filterwarnings('ignore')

EXCEL_ROW_LIMIT = 1_000_000

class HighVolumeAutoPartsCatalog:
    def __init__(self):
        self.data_dir = Path("./auto_parts_data")
        self.data_dir.mkdir(exist_ok=True)
        self.db_path = self.data_dir / "catalog.duckdb"
        self.conn = duckdb.connect(str(self.db_path))
        self.setup_database()
        # –ò–∑–Ω–∞—á–∞–ª—å–Ω–∞—è –≥–ª–æ–±–∞–ª—å–Ω–∞—è –Ω–∞—Ü–µ–Ω–∫–∞
        self.global_markup = 0.2  # 20%
        self.create_indexes()
        # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        self.price_cache = {}  # –î–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–Ω, –ø–æ –∂–µ–ª–∞–Ω–∏—é

        st.set_page_config(
            page_title="AutoParts Catalog 10M+", 
            layout="wide",
            page_icon="üöó"
        )

    def setup_database(self):
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE, 
                width DOUBLE,
                height DOUBLE, 
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)
        # –¢–∞–±–ª–∏—Ü–∞ —Ü–µ–Ω
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS prices (
                artikul VARCHAR,
                quantity INTEGER,
                brand VARCHAR,
                price DOUBLE,
                PRIMARY KEY (artikul, brand)
            )
        """)
        # –¢–∞–±–ª–∏—Ü–∞ –Ω–∞—Ü–µ–Ω–æ–∫
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS markups (
                brand VARCHAR PRIMARY KEY,
                markup DOUBLE
            )
        """)
        # –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –≥–ª–æ–±–∞–ª—å–Ω–∞—è –Ω–∞—Ü–µ–Ω–∫–∞
        self.global_markup = 0.2

    def create_indexes(self):
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_oe_data_oe ON oe_data(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_parts_data_keys ON parts_data(artikul_norm, brand_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_oe ON cross_references(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_artikul ON cross_references(artikul_norm, brand_norm)",
        ]
        for idx_sql in indexes:
            self.conn.execute(idx_sql)

    def normalize_key(self, key_series: pl.Series) -> pl.Series:
        return (
            key_series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
            .str.to_lowercase()
        )

    def clean_values(self, value_series: pl.Series) -> pl.Series:
        return (
            value_series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
        )

    def detect_columns(self, actual_columns, expected_columns):
        mapping = {}
        column_variants = {
            'oe_number': ['oe –Ω–æ–º–µ—Ä', 'oe', '–æe', '–Ω–æ–º–µ—Ä', 'code', 'OE'],
            'artikul': ['–∞—Ä—Ç–∏–∫—É–ª', 'article', 'sku'],
            'brand': ['–±—Ä–µ–Ω–¥', 'brand', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', 'manufacturer'],
            'name': ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ', 'name', '–æ–ø–∏—Å–∞–Ω–∏–µ', 'description'],
            'applicability': ['–ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', 'vehicle', 'applicability'],
            'barcode': ['—à—Ç—Ä–∏—Ö-–∫–æ–¥', 'barcode', '—à—Ç—Ä–∏—Ö–∫–æ–¥', 'ean', 'eac13'],
            'multiplicity': ['–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å —à—Ç', '–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å', 'multiplicity'],
            'length': ['–¥–ª–∏–Ω–∞ (—Å–º)', '–¥–ª–∏–Ω–∞', 'length', '–¥–ª–∏–Ω–Ω–∞'],
            'width': ['—à–∏—Ä–∏–Ω–∞ (—Å–º)', '—à–∏—Ä–∏–Ω–∞', 'width'],
            'height': ['–≤—ã—Å–æ—Ç–∞ (—Å–º)', '–≤—ã—Å–æ—Ç–∞', 'height'],
            'weight': ['–≤–µ—Å (–∫–≥)', '–≤–µ—Å, –∫–≥', '–≤–µ—Å', 'weight'],
            'image_url': ['—Å—Å—ã–ª–∫–∞', 'url', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 'image', '–∫–∞—Ä—Ç–∏–Ω–∫–∞'],
            'dimensions_str': ['–≤–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã', '—Ä–∞–∑–º–µ—Ä—ã', 'dimensions', 'size']
        }
        actual_lower = {col.lower(): col for col in actual_columns}
        for expected in expected_columns:
            variants = [v.lower() for v in column_variants.get(expected, [expected])]
            for variant in variants:
                for actual_l, actual_orig in actual_lower.items():
                    if variant in actual_l:
                        mapping[actual_orig] = expected
                        break
                if expected in mapping.values():
                    break
        return mapping

    def read_and_prepare_file(self, file_path, file_type):
        # –ß—Ç–µ–Ω–∏–µ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∞–π–ª–∞
        try:
            df = pl.read_excel(file_path, engine='calamine')
        except Exception:
            return pl.DataFrame()
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
        schemas = {
            'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
            'barcode': ['brand', 'artikul', 'barcode', 'multiplicity'],
            'dimensions': ['artikul', 'brand', 'length', 'width', 'height', 'weight', 'dimensions_str'],
            'images': ['artikul', 'brand', 'image_url'],
            'cross': ['oe_number', 'artikul', 'brand']
        }
        expected_cols = schemas.get(file_type, [])
        col_mapping = self.detect_columns(df.columns, expected_cols)
        if not col_mapping:
            return pl.DataFrame()
        df = df.rename(col_mapping)
        # –û—á–∏—Å—Ç–∫–∞
        if 'artikul' in df.columns:
            df = df.with_columns(artikul=self.clean_values(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand=self.clean_values(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number=self.clean_values(pl.col('oe_number')))
        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–æ –∫–ª—é—á–∞–º
        key_cols = [col for col in ['oe_number', 'artikul', 'brand'] if col in df.columns]
        if key_cols:
            df = df.unique(subset=key_cols, keep='first')
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–π
        if 'artikul' in df.columns:
            df = df.with_columns(artikul_norm=self.normalize_key(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand_norm=self.normalize_key(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number_norm=self.normalize_key(pl.col('oe_number')))
        return df

    def upsert_data(self, table_name, df, pk):
        if df.is_empty():
            return
        df = df.unique(keep='first')
        cols = df.columns
        pk_str = ", ".join(f'"{c}"' for c in pk)
        temp_view = f"temp_{table_name}_{int(time.time())}"
        self.conn.register(temp_view, df.to_arrow())
        update_cols = [col for col in cols if col not in pk]
        if not update_cols:
            on_conflict = "DO NOTHING"
        else:
            set_clause = ", ".join([f'"{col}"=excluded."{col}"' for col in update_cols])
            on_conflict = f"DO UPDATE SET {set_clause}"
        sql = f"""
        INSERT INTO {table_name}
        SELECT * FROM {temp_view}
        ON CONFLICT ({pk_str}) {on_conflict};
        """
        self.conn.execute(sql)
        self.conn.unregister(temp_view)

    def process_and_load_data(self, dataframes):
        # –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å
        st.info("üîÑ –ù–∞—á–∏–Ω–∞—é –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ oe
        if 'oe' in dataframes:
            df = dataframes['oe'].filter(pl.col('oe_number_norm') != "")
            oe_df = df.select(['oe_number_norm', 'oe_number', 'name', 'applicability']).unique(subset=['oe_number_norm'])
            if 'name' in oe_df.columns:
                oe_df = oe_df.with_columns(self.determine_category_vectorized(pl.col('name')))
            else:
                oe_df = oe_df.with_columns(category=pl.lit('–†–∞–∑–Ω–æ–µ'))
            self.upsert_data('oe_data', oe_df, ['oe_number_norm'])
            # Cross
            cross_df = df.filter(pl.col('artikul_norm') != "").select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df, ['oe_number_norm', 'artikul_norm', 'brand_norm'])
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ cross
        if 'cross' in dataframes:
            df = dataframes['cross']
            cross_df = df.filter((pl.col('oe_number_norm') != "") & (pl.col('artikul_norm') != ""))
            self.upsert_data('cross_references', cross_df, ['oe_number_norm', 'artikul_norm', 'brand_norm'])
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ parts
        # –°–æ–±–∏—Ä–∞–µ–º –∞—Ä—Ç–∏–∫—É–ª–∞ –∏ –±—Ä–µ–Ω–¥—ã –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        parts_df = None
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∞—Ä—Ç–∏–∫—É–ª–∞–º –∏ –±—Ä–µ–Ω–¥–∞–º
        # ...
        # –î–∞–ª–µ–µ –∑–¥–µ—Å—å –ª–æ–≥–∏–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è parts_data, –∞–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è –≤–∞—à–µ–º—É –∫–æ–¥—É
        # (–¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏ –ø—Ä–æ–ø—É—â–µ–Ω–æ, –≤—Å—Ç–∞–≤—å—Ç–µ –∫–∞–∫ –µ—Å—Ç—å)
        # –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ: –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞, —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Ä–∞—Å—á–µ—Ç —Ü–µ–Ω —Å –Ω–∞—Ü–µ–Ω–∫–∞–º–∏
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∞—Ä—Ç–∏–∫—É–ª–∞ –∏—â–µ–º —Ü–µ–Ω—É –≤ —Ç–∞–±–ª–∏—Ü–µ prices, –µ—Å–ª–∏ –µ—Å—Ç—å, –ø—Ä–∏–º–µ–Ω—è–µ–º –Ω–∞—Ü–µ–Ω–∫—É
        # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤—å—Ç–µ —Ä–∞—Å—á–µ—Ç —Ü–µ–Ω—ã —Å —É—á–µ—Ç–æ–º –Ω–∞—Ü–µ–Ω–∫–∏ –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π SELECT

        # –í –∫–æ–Ω—Ü–µ –≤—ã–∑–æ–≤ self.upsert_data –¥–ª—è parts_data
        # (—Ä–∞—Å–ø–∏—à–∏—Ç–µ –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏ –≤—ã—à–µ –∏–ª–∏ –≤—Å—Ç–∞–≤—å—Ç–µ –≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥)

        st.success("üíæ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    def load_price_list(self, file_path):
        df = pl.read_excel(file_path, engine='calamine')
        df = df.rename({col: col.lower() for col in df.columns})
        required_cols = ['–∞—Ä—Ç–∏–∫—É–ª', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–±—Ä–µ–Ω–¥', '—Ü–µ–Ω–∞']
        if not all(c in df.columns for c in required_cols):
            st.error("–ü—Ä–∞–π—Å-–ª–∏—Å—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã: –ê—Ä—Ç–∏–∫—É–ª, –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ, –ë—Ä–µ–Ω–¥, –¶–µ–Ω–∞")
            return
        df = df.with_columns(
            artikul=self.clean_values(pl.col('–∞—Ä—Ç–∏–∫—É–ª')),
            brand=self.clean_values(pl.col('–±—Ä–µ–Ω–¥')),
            quantity=pl.col('–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ').cast(pl.Int32),
            price=pl.col('—Ü–µ–Ω–∞').cast(pl.Float64)
        )
        for row in df.to_dicts():
            self.conn.execute("""
                INSERT INTO prices (artikul, quantity, brand, price) VALUES (?, ?, ?, ?)
                ON CONFLICT (artikul, brand) DO UPDATE SET
                quantity=excluded.quantity,
                price=excluded.price
            """, [row['–∞—Ä—Ç–∏–∫—É–ª'], row['–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ'], row['–±—Ä–µ–Ω–¥'], row['—Ü–µ–Ω–∞']])
        st.success("–¶–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω—ã")

    def get_price_for_artikul(self, artikul, brand):
        # –ú–æ–∂–Ω–æ –∫—ç—à–∏—Ä–æ–≤–∞—Ç—å –∏–ª–∏ –¥–µ–ª–∞—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        result = self.conn.execute("SELECT price FROM prices WHERE artikul = ? AND brand = ?", [artikul, brand]).fetchone()
        if result:
            return result[0]
        return None

    def build_export_query(self, selected_columns=None, exclude_names=None, include_markup=True):
        # –§–æ—Ä–º–∏—Ä—É–µ–º SELECT —Å —É—á–µ—Ç–æ–º –∫–æ–ª–æ–Ω–æ–∫, –Ω–∞—Ü–µ–Ω–æ–∫ –∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        # –û–±—ä—è–≤–ª—è–µ–º CTE —Å —Ç–µ–∫—Å—Ç–æ–º (–æ–ø–∏—Å–∞–Ω–∏–µ)
        standard_description = """–°–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: –Ω–æ–≤—ã–π (–≤ —É–ø–∞–∫–æ–≤–∫–µ).
–í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã ‚Äî –Ω–∞–¥–µ–∂–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è. 
–û–±–µ—Å–ø–µ—á—å—Ç–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –¥–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç—å –∏ –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ —Å –ø–æ–º–æ—â—å—é –Ω–∞—à–µ–≥–æ —à–∏—Ä–æ–∫–æ–≥–æ –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–µ–π.

–í –Ω–∞—à–µ–º –∫–∞—Ç–∞–ª–æ–≥–µ –≤—ã –Ω–∞–π–¥–µ—Ç–µ —Ç–æ—Ä–º–æ–∑–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, —Ñ–∏–ª—å—Ç—Ä—ã (–º–∞—Å–ª—è–Ω—ã–µ, –≤–æ–∑–¥—É—à–Ω—ã–µ, —Å–∞–ª–æ–Ω–Ω—ã–µ), —Å–≤–µ—á–∏ –∑–∞–∂–∏–≥–∞–Ω–∏—è, —Ä–∞—Å—Ö–æ–¥–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –∞–≤—Ç–æ—Ö–∏–º–∏—é, —ç–ª–µ–∫—Ç—Ä–∏–∫—É, –∞–≤—Ç–æ–º–∞—Å–ª–∞, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∞ —Ç–∞–∫–∂–µ –¥—Ä—É–≥–∏–µ –∫–æ–º–ø–ª–µ–∫—Ç—É—é—â–∏–µ, –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. 

–ú—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –±—ã—Å—Ç—Ä—É—é –¥–æ—Å—Ç–∞–≤–∫—É, –≤—ã–≥–æ–¥–Ω—ã–µ —Ü–µ–Ω—ã –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –¥–ª—è –ª—é–±–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ ‚Äî –∞–≤—Ç–æ–ª—é–±–∏—Ç–µ–ª—è, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –∏–ª–∏ –∞–≤—Ç–æ—Å–µ—Ä–≤–∏—Å–∞. 

–í—ã–±–∏—Ä–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –ª—É—á—à–µ–µ ‚Äî –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç –≤–µ–¥—É—â–∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π."""
        # –ö–æ–ª–æ–Ω–∫–∏ –∏ –∏—Ö SQL –≤—ã—Ä–∞–∂–µ–Ω–∏—è
        columns_map = [
            ("–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", 'p.artikul AS "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞"'),
            ("–ë—Ä–µ–Ω–¥", 'p.brand AS "–ë—Ä–µ–Ω–¥"'),
            ("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", 'COALESCE(p.name, p2.name) AS "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"'),
            ("–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", 'COALESCE(p.applicability, p2.applicability) AS "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å"'),
            ("–û–ø–∏—Å–∞–Ω–∏–µ", "CONCAT(COALESCE(p.description, ''), dt.text) AS \"–û–ø–∏—Å–∞–Ω–∏–µ\""),
            ("–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", 'COALESCE(p.category, p2.category) AS "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞"'),
            ("–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", 'p.multiplicity AS "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"'),
            ("–î–ª–∏–Ω–Ω–∞", 'COALESCE(p.length, p2.length) AS "–î–ª–∏–Ω–Ω–∞"'),
            ("–®–∏—Ä–∏–Ω–∞", 'COALESCE(p.width, p2.width) AS "–®–∏—Ä–∏–Ω–∞"'),
            ("–í—ã—Å–æ—Ç–∞", 'COALESCE(p.height, p2.height) AS "–í—ã—Å–æ—Ç–∞"'),
            ("–í–µ—Å", 'COALESCE(p.weight, p2.weight) AS "–í–µ—Å"'),
            ("–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "COALESCE(p.dimensions_str, p2.dimensions_str) AS \"–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞\""),
            ("OE –Ω–æ–º–µ—Ä", 'p.oe_list AS "OE –Ω–æ–º–µ—Ä"'),
            ("–∞–Ω–∞–ª–æ–≥–∏", 'p.analog_list AS "–∞–Ω–∞–ª–æ–≥–∏"'),
            ("–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", 'p.image_url AS "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"'),
            ("–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π", 'CASE WHEN p.price IS NOT NULL THEN p.price * (1 + ? + COALESCE(m.markup, 0)) ELSE NULL END AS "–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π"')
        ]
        # –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã, –±–µ—Ä–µ–º –≤—Å–µ
        if selected_columns is None:
            selected_columns = [name for name, _ in columns_map]
        else:
            # —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º
            columns_map = [item for item in columns_map if item[0] in selected_columns]
        select_exprs = [expr for _, expr in columns_map]
        # –§–æ—Ä–º–∏—Ä—É–µ–º WHERE —Å –∏—Å–∫–ª—é—á–µ–Ω–∏—è–º–∏
        where_clauses = []
        if exclude_names:
            conditions = []
            for name in exclude_names:
                conditions.append(f"p.name LIKE '%{name}%'")
            where_clauses.append("(" + " OR ".join(conditions) + ")")
        where_sql = ""
        if where_clauses:
            where_sql = " WHERE " + " AND ".join(where_clauses)
        # –§–æ—Ä–º–∏—Ä—É–µ–º CTE —Å —Ç–µ–∫—Å—Ç–æ–º
        ctes = f"""
        WITH DescriptionTemplate AS (
            SELECT CHR(10) || CHR(10) || $${standard_description}$$ AS text
        ), 
        PartDetails AS (
            SELECT
                cr.artikul_norm,
                cr.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(o.oe_number, '''', ''), '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]', '', 'g'), ', ') AS oe_list,
                ANY_VALUE(o.name) AS representative_name,
                ANY_VALUE(o.applicability) AS representative_applicability,
                ANY_VALUE(o.category) AS representative_category
            FROM cross_references cr
            JOIN oe_data o ON cr.oe_number_norm = o.oe_number_norm
            GROUP BY cr.artikul_norm, cr.brand_norm
        ),
        AllAnalogs AS (
            SELECT
                cr1.artikul_norm,
                cr1.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(p2.artikul, '''', ''), '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]', '', 'g'), ', ') as analog_list
            FROM cross_references cr1
            JOIN cross_references cr2 ON cr1.oe_number_norm = cr2.oe_number_norm
            JOIN parts_data p2 ON cr2.artikul_norm = p2.artikul_norm AND cr2.brand_norm = p2.brand_norm
            WHERE (cr1.artikul_norm != p2.artikul_norm OR cr1.brand_norm != p2.brand_norm)
            GROUP BY cr1.artikul_norm, cr1.brand_norm
        )
        -- (–ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —É—Ä–æ–≤–Ω–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        """
        # –°–æ–±–∏—Ä–∞–µ–º –∏—Ç–æ–≥
        query = f"""
        {ctes}
        SELECT
            {", ".join(select_exprs)}
        FROM parts_data p
        LEFT JOIN cross_references cr ON p.artikul_norm = cr.artikul_norm AND p.brand_norm = cr.brand_norm
        LEFT JOIN oe_data o ON cr.oe_number_norm = o.oe_number_norm
        LEFT JOIN PartDetails pd ON p.artikul_norm = pd.artikul_norm AND p.brand_norm = pd.brand_norm
        LEFT JOIN AllAnalogs aa ON p.artikul_norm = aa.artikul_norm AND p.brand_norm = aa.brand_norm
        LEFT JOIN DescriptionTemplate dt ON 1=1
        {where_sql}
        WHERE 1=1
        """
        return query, self.global_markup

    def show_export_interface(self):
        st.header("üì§ –£–º–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")
        total_records = self.conn.execute("SELECT count(DISTINCT (artikul_norm, brand_norm)) FROM parts_data").fetchone()[0]
        st.info(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ (—Å—Ç—Ä–æ–∫): {total_records:,}")
        if total_records == 0:
            st.warning("–ë–∞–∑–∞ –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞.")
            return
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–ª–æ–Ω–æ–∫
        available_columns = [
            "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", "–ë—Ä–µ–Ω–¥", "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", "–û–ø–∏—Å–∞–Ω–∏–µ",
            "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", "–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞",
            "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "OE –Ω–æ–º–µ—Ä", "–∞–Ω–∞–ª–æ–≥–∏", "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π"
        ]
        columns_order = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫", options=available_columns, default=available_columns)

        # –ò—Å–∫–ª—é—á–µ–Ω–∏—è
        exclusions_input = st.text_area("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è (—á–µ—Ä–µ–∑ |)", height=100)
        exclude_names = [n.strip() for n in exclusions_input.split('|') if n.strip()]

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞—Ü–µ–Ω–∫–∏
        st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞—Ü–µ–Ω–∫–∏")
        self.global_markup = st.slider("–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞ (%)", 0, 100, int(self.global_markup*100))/100
        brand_name = st.text_input("–ë—Ä–µ–Ω–¥ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞—Ü–µ–Ω–∫–∏")
        if brand_name:
            res = self.conn.execute("SELECT markup FROM markups WHERE brand = ?", [brand_name]).fetchone()
            current_markup = res[0] if res else 0
            new_markup = st.slider(f"–ù–∞—Ü–µ–Ω–∫–∞ –¥–ª—è {brand_name} (%)", 0, 100, int(current_markup*100))
            if st.button("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–∞—Ü–µ–Ω–∫—É –¥–ª—è –±—Ä–µ–Ω–¥–∞"):
                self.conn.execute("""
                    INSERT INTO markups (brand, markup) VALUES (?, ?)
                    ON CONFLICT (brand) DO UPDATE SET markup=excluded.markup
                """, [brand_name, new_markup/100])

        # –í—ã–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫
        selected_columns = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—Å–µ)", options=available_columns, default=columns_order)

        # –§–æ—Ä–º–∞—Ç
        export_format = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞:", ["CSV", "Excel (.xlsx)", "Parquet"], index=0)

        if export_format == "CSV":
            if st.button("üöÄ –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV"):
                output_path = self.data_dir / "auto_parts_export.csv"
                with st.spinner("–≠–∫—Å–ø–æ—Ä—Ç –≤ CSV..."):
                    query, markup_value = self.build_export_query(selected_columns, exclude_names)
                    df = self.conn.execute(query, [self.global_markup]).pl()
                    # –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ —Å—Ç—Ä–æ–∫–∏
                    for colname in ["–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞", "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"]:
                        if colname in df.columns:
                            df = df.with_columns(
                                pl.when(pl.col(colname).is_not_null())
                                .then(pl.col(colname).cast(pl.Utf8))
                                .otherwise("")
                                .alias(colname)
                            )
                    buf = io.StringIO()
                    df.write_csv(buf, separator=';')
                    with open(output_path, 'wb') as f:
                        f.write(b'\xef\xbb\xbf')
                        f.write(buf.getvalue().encode('utf-8'))
                st.success(f"–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
                st.download_button("–°–∫–∞—á–∞—Ç—å CSV", open(output_path, "rb"), "auto_parts_export.csv")
        elif export_format == "Excel (.xlsx)":
            if st.button("üìä –≠–∫—Å–ø–æ—Ä—Ç –≤ Excel"):
                output_path = self.data_dir / "auto_parts_export.xlsx"
                # –ü–æ—Å–∫–æ–ª—å–∫—É Excel –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ, –¥–µ–ª–∞–µ–º –ø–æ —á–∞—Å—Ç—è–º
                total_count = self.conn.execute("SELECT COUNT(DISTINCT artikul_norm, brand_norm) FROM parts_data").fetchone()[0]
                num_files = (total_count // EXCEL_ROW_LIMIT) + 1
                all_files = []
                for i in range(num_files):
                    offset = i * EXCEL_ROW_LIMIT
                    query, markup_value = self.build_export_query(selected_columns, exclude_names)
                    df = self.conn.execute(f"{query} LIMIT {EXCEL_ROW_LIMIT} OFFSET {offset}", [self.global_markup]).pl()
                    for colname in ["–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞", "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"]:
                        if colname in df.columns:
                            df = df.with_columns(
                                pl.when(pl.col(colname).is_not_null())
                                .then(pl.col(colname).cast(pl.Utf8))
                                .otherwise("")
                                .alias(colname)
                            )
                    file_path = self.data_dir / f"part_{i+1}.xlsx"
                    df.write_excel(str(file_path))
                    all_files.append(file_path)
                # ZIP –µ—Å–ª–∏ –±–æ–ª—å—à–µ 1 —Ñ–∞–π–ª–∞
                if len(all_files) > 1:
                    zip_path = self.data_dir / "export_parts.zip"
                    with zipfile.ZipFile(zip_path, 'w') as zf:
                        for file in all_files:
                            zf.write(file, arcname=file.name)
                            os.remove(file)
                    st.download_button("–°–∫–∞—á–∞—Ç—å ZIP", open(zip_path, "rb"), "export_parts.zip")
                else:
                    st.download_button("–°–∫–∞—á–∞—Ç—å Excel", open(all_files[0], "rb"), "auto_parts_export.xlsx")
        elif export_format == "Parquet":
            if st.button("‚ö°Ô∏è –≠–∫—Å–ø–æ—Ä—Ç –≤ Parquet"):
                output_path = self.data_dir / "auto_parts_export.parquet"
                query, _ = self.build_export_query(selected_columns, exclude_names)
                df = self.conn.execute(query, [self.global_markup]).pl()
                df.write_parquet(str(output_path))
                st.download_button("–°–∫–∞—á–∞—Ç—å Parquet", open(output_path, "rb"), "auto_parts_export.parquet")
        
    def get_statistics(self):
        stats = {}
        try:
            stats['total_parts'] = self.conn.execute("SELECT COUNT(*) FROM parts_data").fetchone()[0]
            stats['total_oe'] = self.conn.execute("SELECT COUNT(*) FROM oe_data").fetchone()[0]
            stats['total_brands'] = self.conn.execute("SELECT COUNT(DISTINCT brand) FROM parts_data").fetchone()[0]
            # –¢–æ–ø –±—Ä–µ–Ω–¥–æ–≤
            br_res = self.conn.execute("SELECT brand, COUNT(*) FROM parts_data GROUP BY brand ORDER BY COUNT(*) DESC LIMIT 10").fetchall()
            stats['top_brands'] = pl.DataFrame(br_res, schema=["brand", "count"])
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
            cat_res = self.conn.execute("SELECT category, COUNT(*) FROM oe_data GROUP BY category ORDER BY COUNT(*) DESC").fetchall()
            stats['categories'] = pl.DataFrame(cat_res, schema=["category", "count"])
        except Exception:
            pass
        return stats

    def merge_all_data_parallel(self, file_paths):
        # –†–µ–∞–ª–∏–∑—É–π—Ç–µ –∫–∞–∫ —É –≤–∞—Å
        # –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ ‚Äî –≤—ã–∑–æ–≤ self.process_and_load_data(...)
        pass

# –í –æ—Å–Ω–æ–≤–Ω–æ–º –≤—ã–∑—ã–≤–∞–π—Ç–µ
def main():
    catalog = HighVolumeAutoPartsCatalog()

    st.title("üöó AutoParts Catalog - –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è 10+ –º–ª–Ω –∑–∞–ø–∏—Å–µ–π")
    st.markdown("...")  # –í–∞—à–∞ –æ–ø–∏—Å–∞–Ω–∏–µ

    menu_option = st.sidebar.radio("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:", ["–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö", "–≠–∫—Å–ø–æ—Ä—Ç", "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏"])

    if menu_option == "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö":
        # –í–∞—à–∞ –ª–æ–≥–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏, –≤—ã–∑–æ–≤ catalog.load_price_list() –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        pass
    elif menu_option == "–≠–∫—Å–ø–æ—Ä—Ç":
        catalog.show_export_interface()
    elif menu_option == "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞":
        stats = catalog.get_statistics()
        # –í–∞—à–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        pass
    elif menu_option == "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏":
        # –û–ø–µ—Ä–∞—Ü–∏–∏ —É–¥–∞–ª–µ–Ω–∏—è
        pass

if __name__ == "__main__":
    main()
