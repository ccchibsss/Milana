import polars as pl
import duckdb
import streamlit as st
import os
import time
import logging
import io
import zipfile
from pathlib import Path
from typing import Dict, List
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
EXCEL_ROW_LIMIT = 1_000_000

class HighVolumeAutoPartsCatalog:
    
    def __init__(self):
        self.data_dir = Path("./auto_parts_data")
        self.data_dir.mkdir(exist_ok=True)
        self.db_path = self.data_dir / "catalog.duckdb"
        self.conn = duckdb.connect(database=str(self.db_path))
        self.setup_database()
        
        st.set_page_config(
            page_title="AutoParts Catalog 10M+", 
            layout="wide",
            page_icon="ðŸš—"
        )
    
    def setup_database(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE, 
                width DOUBLE,
                height DOUBLE, 
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)
        
    def create_indexes(self):
        st.info("Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð² Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð¸ÑÐºÐ°...")
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_oe_data_oe ON oe_data(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_parts_data_keys ON parts_data(artikul_norm, brand_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_oe ON cross_references(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_artikul ON cross_references(artikul_norm, brand_norm)",
        ]
        for index_sql in indexes:
            self.conn.execute(index_sql)
        st.success("Ð˜Ð½Ð´ÐµÐºÑÑ‹ ÑÐ¾Ð·Ð´Ð°Ð½Ñ‹.")

    @staticmethod
    def normalize_key(key_series: pl.Series) -> pl.Series:
        return (
            key_series
            .fill_null("")
            .cast(pl.Utf8)
            # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° ÑƒÐ´Ð°Ð»ÑÐµÐ¼ Ð°Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ñ„ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ (Ð¾Ð½ Ð½Ðµ Ð½ÑƒÐ¶ÐµÐ½ Ð²Ð¾Ð¾Ð±Ñ‰Ðµ)
            .str.replace_all("'", "")
            # Ð—Ð°Ñ‚ÐµÐ¼ ÑƒÐ´Ð°Ð»ÑÐµÐ¼ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð¼ÑƒÑÐ¾Ñ€Ð½Ñ‹Ðµ ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ‹, Ð¾ÑÑ‚Ð°Ð²Ð»ÑÑ: Ð±ÑƒÐºÐ²Ñ‹, Ñ†Ð¸Ñ„Ñ€Ñ‹, `, -, Ð¿Ñ€Ð¾Ð±ÐµÐ»
            .str.replace_all(r"[^0-9A-Za-zA-za-ÑÐÑ‘`\-\s]", "")
            # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹ (Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹ -> Ð¾Ð´Ð¸Ð½)
            .str.replace_all(r"\s+", " ")
            # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹ Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ Ð¸ ÐºÐ¾Ð½Ñ†Ðµ
            .str.strip_chars()
            .str.to_lowercase()
        )

    @staticmethod
    def clean_values(value_series: pl.Series) -> pl.Series:
        """ÐžÑ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð¾Ñ‚ Ð°Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ñ„Ð¾Ð² Ð¸ Ð¼ÑƒÑÐ¾Ñ€Ð° Ð½Ð° Ð²Ñ…Ð¾Ð´Ðµ"""
        return (
            value_series
            .fill_null("")
            .cast(pl.Utf8)
            # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð°Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ñ„ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ
            .str.replace_all("'", "")
            # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð¼ÑƒÑÐ¾Ñ€Ð½Ñ‹Ðµ ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ‹, Ð¾ÑÑ‚Ð°Ð²Ð»ÑÑ: Ð±ÑƒÐºÐ²Ñ‹, Ñ†Ð¸Ñ„Ñ€Ñ‹, `, -, Ð¿Ñ€Ð¾Ð±ÐµÐ»
            .str.replace_all(r"[^0-9A-Za-zA-za-ÑÐÑ‘`\-\s]", "")
            # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹ (Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹ -> Ð¾Ð´Ð¸Ð½)
            .str.replace_all(r"\s+", " ")
            # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹ Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ Ð¸ ÐºÐ¾Ð½Ñ†Ðµ
            .str.strip_chars()
        )

    @staticmethod
    def determine_category_vectorized(name_series: pl.Series) -> pl.Series:
        categories_map = {
            'Ð¤Ð¸Ð»ÑŒÑ‚Ñ€': 'Ñ„Ð¸Ð»ÑŒÑ‚Ñ€|filter', 
            'Ð¢Ð¾Ñ€Ð¼Ð¾Ð·Ð½Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð°': 'Ñ‚Ð¾Ñ€Ð¼Ð¾Ð·|brake|ÐºÐ¾Ð»Ð¾Ð´Ðº|Ð´Ð¸ÑÐº|ÑÑƒÐ¿Ð¿Ð¾Ñ€Ñ‚',
            'ÐŸÐ¾Ð´Ð²ÐµÑÐºÐ°': 'Ð°Ð¼Ð¾Ñ€Ñ‚Ð¸Ð·Ð°Ñ‚Ð¾Ñ€|ÑÑ‚Ð¾Ð¹Ðº|spring|Ð¿Ð¾Ð´Ð²ÐµÑÐº|Ð Ñ‹Ñ‡Ð°Ð³|Ð Ñ‹Ñ‡Ð°Ð³Ð¸|Ð¨Ð°Ñ€Ð¾Ð²Ð°Ñ Ð¾Ð¿Ð¾Ñ€Ð°|ÐžÐ¿Ð¾Ñ€Ð° ÑˆÐ°Ñ€Ð¾Ð²Ð°Ñ|Ð¡Ð°Ð¹Ð»ÐµÐ½Ñ‚Ð±Ð»Ð¾Ðº|Ð¡Ñ‚ÑƒÐ¿Ð¸Ñ†|Ð¿Ð¾Ð´ÑˆÐ¸Ð¿Ð½Ð¸Ðº ÑÑ‚ÑƒÐ¿Ð¸Ñ†Ñ‹|Ð¿Ð¾Ð´ÑˆÐ¸Ð¿Ð½Ð¸ÐºÐ¸ ÑÑ‚ÑƒÐ¿Ð¸Ñ†Ñ‹', 
            'Ð”Ð²Ð¸Ð³Ð°Ñ‚ÐµÐ»ÑŒ': 'Ð´Ð²Ð¸Ð³Ð°Ñ‚ÐµÐ»ÑŒ|engine|ÑÐ²ÐµÑ‡|Ð¿Ð¾Ñ€ÑˆÐµÐ½ÑŒ|ÐºÐ»Ð°Ð¿Ð°Ð½',
            'Ð¢Ñ€Ð°Ð½ÑÐ¼Ð¸ÑÑÐ¸Ñ': 'Ñ‚Ñ€Ð°Ð½ÑÐ¼Ð¸ÑÑÐ¸Ñ|ÑÑ†ÐµÐ¿Ð»ÐµÐ½|ÐºÐ¾Ñ€Ð¾Ð±Ðº|transmission', 
            'Ð­Ð»ÐµÐºÑ‚Ñ€Ð¸ÐºÐ°': 'Ð°ÐºÐºÑƒÐ¼ÑƒÐ»ÑÑ‚Ð¾Ñ€|Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¾Ñ€|ÑÑ‚Ð°Ñ€Ñ‚ÐµÑ€|Ð¿Ñ€Ð¾Ð²Ð¾Ð´|Ð»Ð°Ð¼Ð¿',
            'Ð ÑƒÐ»ÐµÐ²Ð¾Ðµ': 'Ñ€ÑƒÐ»ÐµÐ²Ð¾Ð¹|Ñ‚ÑÐ³Ð°|Ð½Ð°ÐºÐ¾Ð½ÐµÑ‡Ð½Ð¸Ðº|steering', 
            'Ð’Ñ‹Ñ…Ð»Ð¾Ð¿Ð½Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð°': 'Ð³Ð»ÑƒÑˆÐ¸Ñ‚ÐµÐ»ÑŒ|Ð³Ð»ÑƒÑˆÐ¸Ñ‚ÐµÐ»|ÐºÐ°Ñ‚Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€|Ð²Ñ‹Ñ…Ð»Ð¾Ð¿|exhaust|',
            'ÐžÑ…Ð»Ð°Ð¶Ð´ÐµÐ½Ð¸Ðµ': 'Ñ€Ð°Ð´Ð¸Ð°Ñ‚Ð¾Ñ€|Ð²ÐµÐ½Ñ‚Ð¸Ð»ÑÑ‚Ð¾Ñ€|Ñ‚ÐµÑ€Ð¼Ð¾ÑÑ‚Ð°Ñ‚|cooling', 
            'Ð¢Ð¾Ð¿Ð»Ð¸Ð²Ð¾': 'Ñ‚Ð¾Ð¿Ð»Ð¸Ð²Ð½Ñ‹Ð¹|Ð±ÐµÐ½Ð·Ð¾Ð½Ð°ÑÐ¾Ñ|Ñ„Ð¾Ñ€ÑÑƒÐ½Ðº|fuel',
            
            
        }
        name_lower = name_series.str.to_lowercase()
        categorization_expr = pl.when(pl.lit(False)).then(pl.lit(None))
        for category, pattern in categories_map.items():
            categorization_expr = categorization_expr.when(name_lower.str.contains(pattern)).then(pl.lit(category))
        return categorization_expr.otherwise(pl.lit('Ð Ð°Ð·Ð½Ð¾Ðµ')).alias('category')

    def detect_columns(self, actual_columns: List[str], expected_columns: List[str]) -> Dict[str, str]:
        mapping = {}
        column_variants = {
            'oe_number': ['oe Ð½Ð¾Ð¼ÐµÑ€', 'oe', 'Ð¾e', 'Ð½Ð¾Ð¼ÐµÑ€', 'code', 'OE'], 'artikul': ['Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»', 'article', 'sku'],
            'brand': ['Ð±Ñ€ÐµÐ½Ð´', 'brand', 'Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒ', 'manufacturer'], 'name': ['Ð½Ð°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ', 'Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ', 'name', 'Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ', 'description'],
            'applicability': ['Ð¿Ñ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ð¾ÑÑ‚ÑŒ', 'Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»ÑŒ', 'vehicle', 'applicability'], 'barcode': ['ÑˆÑ‚Ñ€Ð¸Ñ…-ÐºÐ¾Ð´', 'barcode', 'ÑˆÑ‚Ñ€Ð¸Ñ…ÐºÐ¾Ð´', 'ean', 'eac13'],
            'multiplicity': ['ÐºÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ ÑˆÑ‚', 'ÐºÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ', 'multiplicity'], 'length': ['Ð´Ð»Ð¸Ð½Ð° (ÑÐ¼)', 'Ð´Ð»Ð¸Ð½Ð°', 'length', 'Ð´Ð»Ð¸Ð½Ð½Ð°'],
            'width': ['ÑˆÐ¸Ñ€Ð¸Ð½Ð° (ÑÐ¼)', 'ÑˆÐ¸Ñ€Ð¸Ð½Ð°', 'width'], 'height': ['Ð²Ñ‹ÑÐ¾Ñ‚Ð° (ÑÐ¼)', 'Ð²Ñ‹ÑÐ¾Ñ‚Ð°', 'height'],
            'weight': ['Ð²ÐµÑ (ÐºÐ³)', 'Ð²ÐµÑ, ÐºÐ³', 'Ð²ÐµÑ', 'weight'], 'image_url': ['ÑÑÑ‹Ð»ÐºÐ°', 'url', 'Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ', 'image', 'ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ°'],
            'dimensions_str': ['Ð²ÐµÑÐ¾Ð³Ð°Ð±Ð°Ñ€Ð¸Ñ‚Ñ‹', 'Ñ€Ð°Ð·Ð¼ÐµÑ€Ñ‹', 'dimensions', 'size']
        }
        actual_lower = {col.lower(): col for col in actual_columns}
        for expected in expected_columns:
            variants = [v.lower() for v in column_variants.get(expected, [expected])]
            for variant in variants:
                for actual_l, actual_orig in actual_lower.items():
                    if variant in actual_l:
                        mapping[actual_orig] = expected
                        break
                if expected in mapping.values():
                    break
        return mapping

    def read_and_prepare_file(self, file_path: str, file_type: str) -> pl.DataFrame:
        logger.info(f"ÐÐ°Ñ‡Ð¸Ð½Ð°ÑŽ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ Ñ„Ð°Ð¹Ð»Ð°: {file_type} ({file_path})")
        try:
            df = pl.read_excel(file_path, engine='calamine')
        except Exception as e:
            logger.error(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ñ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ Ñ„Ð°Ð¹Ð» {file_path}: {e}")
            return pl.DataFrame()

        schemas = {
            'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
            'barcode': ['brand', 'artikul', 'barcode', 'multiplicity'],
            'dimensions': ['artikul', 'brand', 'length', 'width', 'height', 'weight', 'dimensions_str'],
            'images': ['artikul', 'brand', 'image_url'],
            'cross': ['oe_number', 'artikul', 'brand']
        }
        expected_cols = schemas.get(file_type, [])
        column_mapping = self.detect_columns(df.columns, expected_cols)
        df = df.rename(column_mapping)
        
        # ÐžÑ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð¾Ñ‚ Ð°Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ñ„Ð¾Ð² Ð¸ Ð¼ÑƒÑÐ¾Ñ€Ð° Ð½Ð° Ð²Ñ…Ð¾Ð´Ðµ
        if 'artikul' in df.columns:
            df = df.with_columns(artikul=self.clean_values(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand=self.clean_values(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number=self.clean_values(pl.col('oe_number')))
        
        key_cols = [col for col in ['oe_number', 'artikul', 'brand'] if col in df.columns]
        if key_cols:
            df = df.unique(subset=key_cols, keep='first')

        # Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð²ÐµÑ€ÑÐ¸Ð¸ Ð´Ð»Ñ ÐºÐ»ÑŽÑ‡ÐµÐ¹ (Ð½Ð¸Ð¶Ð½Ð¸Ð¹ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€)
        if 'artikul' in df.columns:
            df = df.with_columns(artikul_norm=self.normalize_key(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand_norm=self.normalize_key(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number_norm=self.normalize_key(pl.col('oe_number')))
            
        return df

    def upsert_data(self, table_name: str, df: pl.DataFrame, pk: List[str]):
        if df.is_empty():
            return
        
        df = df.unique(keep='first')
        
        cols = df.columns
        pk_str = ", ".join(f'"{c}"' for c in pk)
        
        temp_view_name = f"temp_{table_name}_{int(time.time())}"
        self.conn.register(temp_view_name, df.to_arrow())
        
        update_cols = [col for col in cols if col not in pk]
        
        if not update_cols:
            on_conflict_action = "DO NOTHING"
        else:
            update_clause = ", ".join([f'"{col}" = excluded."{col}"' for col in update_cols])
            on_conflict_action = f"DO UPDATE SET {update_clause}"

        sql = f"""
        INSERT INTO {table_name}
        SELECT * FROM {temp_view_name}
        ON CONFLICT ({pk_str}) {on_conflict_action};
        """
        
        try:
            self.conn.execute(sql)
            logger.info(f"Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾/Ð²ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¾ {len(df)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð² Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ {table_name}.")
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ UPSERT Ð² {table_name}: {e}")
            st.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð² Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ {table_name}. Ð”ÐµÑ‚Ð°Ð»Ð¸ Ð² Ð»Ð¾Ð³Ðµ.")
        finally:
            self.conn.unregister(temp_view_name)


    def process_and_load_data(self, dataframes: Dict[str, pl.DataFrame]):
        st.info("ðŸ”„ ÐÐ°Ñ‡Ð°Ð»Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¸ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Ð±Ð°Ð·Ðµ...")
        
        steps = [s for s in ['oe', 'cross', 'parts'] if s in dataframes or s == 'parts']
        num_steps = len(steps)
        progress_bar = st.progress(0, text="ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ðº Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸ÑŽ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
        step_counter = 0

        if 'oe' in dataframes:
            step_counter += 1
            progress_bar.progress(step_counter / (num_steps + 1), text=f"({step_counter}/{num_steps}) ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° OE Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
            df = dataframes['oe'].filter(pl.col('oe_number_norm') != "")
            
            oe_df = df.select(['oe_number_norm', 'oe_number', 'name', 'applicability']).unique(subset=['oe_number_norm'], keep='first')
            if 'name' in oe_df.columns:
                oe_df = oe_df.with_columns(self.determine_category_vectorized(pl.col('name')))
            else:
                oe_df = oe_df.with_columns(category=pl.lit('Ð Ð°Ð·Ð½Ð¾Ðµ'))
            self.upsert_data('oe_data', oe_df, ['oe_number_norm'])
            
            cross_df_from_oe = df.filter(pl.col('artikul_norm') != "").select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df_from_oe, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        if 'cross' in dataframes:
            step_counter += 1
            progress_bar.progress(step_counter / (num_steps + 1), text=f"({step_counter}/{num_steps}) ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÐºÑ€Ð¾ÑÑÐ¾Ð²...")
            df = dataframes['cross'].filter((pl.col('oe_number_norm') != "") & (pl.col('artikul_norm') != ""))
            cross_df_from_cross = df.select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df_from_cross, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        step_counter += 1
        progress_bar.progress(step_counter / (num_steps + 1), text=f"({step_counter}/{num_steps}) Ð¡Ð±Ð¾Ñ€ÐºÐ° Ð¸ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ð¾ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð°Ð¼...")
        parts_df = None
        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð´Ð»Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ…
        # ÐŸÐ¾Ñ€ÑÐ´Ð¾Ðº Ð²Ð°Ð¶ÐµÐ½: ÑÐ½Ð°Ñ‡Ð°Ð»Ð° Ð±Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ, Ð¿Ð¾Ñ‚Ð¾Ð¼ ÑÐ¿ÐµÑ†Ð¸Ñ„Ð¸Ñ‡Ð½Ñ‹Ðµ (dimensions Ð¸Ð¼ÐµÐµÑ‚ Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚)
        file_priority = ['oe', 'barcode', 'images', 'dimensions']
        key_files = {ftype: df for ftype, df in dataframes.items() if ftype in file_priority}
        
        if key_files:
            # Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð²ÑÐµ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ñ‹ Ð¸Ð· Ð²ÑÐµÑ… Ñ„Ð°Ð¹Ð»Ð¾Ð²
            all_parts = pl.concat([
                df.select(['artikul', 'artikul_norm', 'brand', 'brand_norm']) 
                for df in key_files.values() if 'artikul_norm' in df.columns and 'brand_norm' in df.columns
            ]).filter(pl.col('artikul_norm') != "").unique(subset=['artikul_norm', 'brand_norm'], keep='first')

            parts_df = all_parts

            # ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ñ„Ð°Ð¹Ð»Ñ‹ Ð² Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð½Ð¾Ð¼ Ð¿Ð¾Ñ€ÑÐ´ÐºÐµ Ð´Ð»Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ…
            for ftype in file_priority:
                if ftype not in key_files: continue
                df = key_files[ftype]
                if df.is_empty() or 'artikul_norm' not in df.columns: continue
                
                join_cols = [col for col in df.columns if col not in ['artikul', 'artikul_norm', 'brand', 'brand_norm']]
                if not join_cols: continue
                
                # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÑƒÐ¶Ðµ ÐµÑÑ‚ÑŒ Ð² parts_df, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Ð´ÑƒÐ±Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
                existing_cols = set(parts_df.columns)
                join_cols = [col for col in join_cols if col not in existing_cols]
                if not join_cols: continue
                
                df_subset = df.select(['artikul_norm', 'brand_norm'] + join_cols).unique(subset=['artikul_norm', 'brand_norm'], keep='first')
                # coalesce=True Ð¿ÐµÑ€ÐµÐ·Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÑ‚ Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº
                # Ð¡ÑƒÑ„Ñ„Ð¸ÐºÑÑ‹ Ð½Ðµ ÑÐ¾Ð·Ð´Ð°ÑŽÑ‚ÑÑ, Ñ‚Ð°Ðº ÐºÐ°Ðº Ð¼Ñ‹ ÑƒÐ¶Ðµ Ð¾Ñ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ð»Ð¸ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸
                parts_df = parts_df.join(df_subset, on=['artikul_norm', 'brand_norm'], how='left', coalesce=True)

        if parts_df is not None and not parts_df.is_empty():
            # Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° multiplicity
            if 'multiplicity' not in parts_df.columns:
                parts_df = parts_df.with_columns(multiplicity=pl.lit(1).cast(pl.Int32))
            else:
                parts_df = parts_df.with_columns(
                    pl.col('multiplicity').fill_null(1).cast(pl.Int32)
                )
            
            # Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° dimensions_str - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð±Ð¾Ð»ÐµÐµ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´
            # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° ÑƒÐ±ÐµÐ¶Ð´Ð°ÐµÐ¼ÑÑ, Ñ‡Ñ‚Ð¾ Ð²ÑÐµ Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ ÐµÑÑ‚ÑŒ
            for col in ['length', 'width', 'height']:
                if col not in parts_df.columns:
                    parts_df = parts_df.with_columns(pl.lit(None).cast(pl.Float64).alias(col))
            
            # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ dimensions_str Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾
            if 'dimensions_str' not in parts_df.columns:
                parts_df = parts_df.with_columns(dimensions_str=pl.lit(None).cast(pl.Utf8))
            
            # Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ dimensions_str
            # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ Ð´Ð»Ñ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾Ð¹ ÐºÐ¾Ð½ÐºÐ°Ñ‚ÐµÐ½Ð°Ñ†Ð¸Ð¸
            parts_df = parts_df.with_columns([
                pl.col('length').cast(pl.Utf8).fill_null('').alias('_length_str'),
                pl.col('width').cast(pl.Utf8).fill_null('').alias('_width_str'),
                pl.col('height').cast(pl.Utf8).fill_null('').alias('_height_str'),
            ])
            
            # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ dimensions_str Ð¸Ð· Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº
            parts_df = parts_df.with_columns(
                dimensions_str=pl.when(
                    (pl.col('dimensions_str').is_not_null()) & 
                    (pl.col('dimensions_str').cast(pl.Utf8) != '')
                ).then(
                    pl.col('dimensions_str').cast(pl.Utf8)
                ).otherwise(
                    pl.concat_str([
                        pl.col('_length_str'), pl.lit('x'), 
                        pl.col('_width_str'), pl.lit('x'), 
                        pl.col('_height_str')
                    ], separator='')
                )
            )
            
            # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸
            parts_df = parts_df.drop(['_length_str', '_width_str', '_height_str'])
            
            # Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° description
            if 'artikul' not in parts_df.columns:
                parts_df = parts_df.with_columns(artikul=pl.lit(''))
            if 'brand' not in parts_df.columns:
                parts_df = parts_df.with_columns(brand=pl.lit(''))
            
            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ Ð´Ð»Ñ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾Ð¹ ÐºÐ¾Ð½ÐºÐ°Ñ‚ÐµÐ½Ð°Ñ†Ð¸Ð¸
            parts_df = parts_df.with_columns([
                pl.col('artikul').cast(pl.Utf8).fill_null('').alias('_artikul_str'),
                pl.col('brand').cast(pl.Utf8).fill_null('').alias('_brand_str'),
                pl.col('multiplicity').cast(pl.Utf8).alias('_multiplicity_str'),
            ])
            
            # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ description Ð¸Ð· Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº
            parts_df = parts_df.with_columns(
                description=pl.concat_str([
                    pl.lit('ÐÑ€Ñ‚Ð¸ÐºÑƒÐ»: '), pl.col('_artikul_str'),
                    pl.lit(', Ð‘Ñ€ÐµÐ½Ð´: '), pl.col('_brand_str'),
                    pl.lit(', ÐšÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ: '), pl.col('_multiplicity_str'), pl.lit(' ÑˆÑ‚.')
                ], separator='')
            )
            
            # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸
            parts_df = parts_df.drop(['_artikul_str', '_brand_str', '_multiplicity_str'])
            final_columns = [
                'artikul_norm', 'brand_norm', 'artikul', 'brand', 'multiplicity', 'barcode', 
                'length', 'width', 'height', 'weight', 'image_url', 'dimensions_str', 'description'
            ]
            select_exprs = [pl.col(c) if c in parts_df.columns else pl.lit(None).alias(c) for c in final_columns]
            parts_df = parts_df.select(select_exprs)
            
            self.upsert_data('parts_data', parts_df, ['artikul_norm', 'brand_norm'])
        
        progress_bar.progress(1.0, text="ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾!")
        time.sleep(1)
        progress_bar.empty()
        st.success("ðŸ’¾ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Ð±Ð°Ð·Ñƒ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°.")

    def merge_all_data_parallel(self, file_paths: Dict[str, str]) -> Dict[str, any]:
        start_time = time.time()
        stats = {}
        
        st.info("ðŸš€ ÐÐ°Ñ‡Ð°Ð»Ð¾ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Ð¸ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ¸ Ñ„Ð°Ð¹Ð»Ð¾Ð²...")
        n_files = len(file_paths)
        file_progress_bar = st.progress(0, text="ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ...")
        
        dataframes = {}
        processed_files = 0
        with ThreadPoolExecutor() as executor:
            future_to_file = {executor.submit(self.read_and_prepare_file, path, ftype): ftype for ftype, path in file_paths.items()}
            for future in as_completed(future_to_file):
                ftype = future_to_file[future]
                try:
                    df = future.result()
                    if not df.is_empty():
                        dataframes[ftype] = df
                        st.success(f"âœ… Ð¤Ð°Ð¹Ð» '{ftype}' Ð¿Ñ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ð½: {len(df):,} ÑÑ‚Ñ€Ð¾Ðº.")
                    else:
                        st.warning(f"âš ï¸ Ð¤Ð°Ð¹Ð» '{ftype}' Ð¿ÑƒÑÑ‚ Ð¸Ð»Ð¸ Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ.")
                except Exception as e:
                    logger.exception(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ñ„Ð°Ð¹Ð»Ð° {ftype}")
                    st.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð² {ftype}: {e}")
                finally:
                    processed_files += 1
                    file_progress_bar.progress(processed_files / n_files, text=f"ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ„Ð°Ð¹Ð»Ð°: {ftype} ({processed_files}/{n_files})")
        
        file_progress_bar.empty()

        if not dataframes:
            st.error("âŒ ÐÐ¸ Ð¾Ð´Ð¸Ð½ Ñ„Ð°Ð¹Ð» Ð½Ðµ Ð±Ñ‹Ð» Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½. ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð°.")
            return {}

        self.process_and_load_data(dataframes)
        
        processing_time = time.time() - start_time
        total_records = self.get_total_records()
        
        stats['processing_time'] = processing_time
        stats['total_records'] = total_records
        
        st.success(f"ðŸŽ‰ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð° Ð·Ð° {processing_time:.2f} ÑÐµÐºÑƒÐ½Ð´")
        st.success(f"ðŸ“Š Ð’ÑÐµÐ³Ð¾ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð¾Ð² Ð² Ð±Ð°Ð·Ðµ: {total_records:,}")
        
        self.create_indexes()
        return stats
    
    def get_total_records(self) -> int:
        try:
            result = self.conn.execute("SELECT COUNT(*) FROM parts_data").fetchone()
            return result[0] if result else 0
        except (duckdb.Error, TypeError):
            return 0

    def get_export_query(self) -> str:
        return r"""
        WITH PartDetails AS (
            SELECT
                cr.artikul_norm,
                cr.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(o.oe_number, '''', ''), '[^0-9A-Za-zÐ-Ð¯Ð°-ÑÐÑ‘`\-\s]', '', 'g'), ', ') AS oe_list,
                ANY_VALUE(o.name) AS representative_name,
                ANY_VALUE(o.applicability) AS representative_applicability,
                ANY_VALUE(o.category) AS representative_category
            FROM cross_references cr
            JOIN oe_data o ON cr.oe_number_norm = o.oe_number_norm
            GROUP BY cr.artikul_norm, cr.brand_norm
        ),
        AllAnalogs AS (
            SELECT
                cr1.artikul_norm,
                cr1.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(p2.artikul, '''', ''), '[^0-9A-Za-zÐ-Ð¯Ð°-ÑÐÑ‘`\-\s]', '', 'g'), ', ') as analog_list
            FROM cross_references cr1
            JOIN cross_references cr2 ON cr1.oe_number_norm = cr2.oe_number_norm
            JOIN parts_data p2 ON cr2.artikul_norm = p2.artikul_norm AND cr2.brand_norm = p2.brand_norm
            WHERE cr1.artikul_norm != p2.artikul_norm OR cr1.brand_norm != p2.brand_norm
            GROUP BY cr1.artikul_norm, cr1.brand_norm
        )
        SELECT
            p.artikul AS "ÐÑ€Ñ‚Ð¸ÐºÑƒÐ» Ð±Ñ€ÐµÐ½Ð´Ð°",
            p.brand AS "Ð‘Ñ€ÐµÐ½Ð´",
            pd.representative_name AS "ÐÐ°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ",
            pd.representative_applicability AS "ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ð¾ÑÑ‚ÑŒ",
            p.description AS "ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ",
            pd.representative_category AS "ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ Ñ‚Ð¾Ð²Ð°Ñ€Ð°",
            p.multiplicity AS "ÐšÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ",
            p.length AS "Ð”Ð»Ð¸Ð½Ð½Ð°",
            p.width AS "Ð¨Ð¸Ñ€Ð¸Ð½Ð°",
            p.height AS "Ð’Ñ‹ÑÐ¾Ñ‚Ð°",
            p.weight AS "Ð’ÐµÑ",
            p.dimensions_str AS "Ð”Ð»Ð¸Ð½Ð½Ð°/Ð¨Ð¸Ñ€Ð¸Ð½Ð°/Ð’Ñ‹ÑÐ¾Ñ‚Ð°",
            pd.oe_list AS "OE Ð½Ð¾Ð¼ÐµÑ€",
            aa.analog_list AS "Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸",
            p.image_url AS "Ð¡ÑÑ‹Ð»ÐºÐ° Ð½Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ"
        FROM parts_data p
        LEFT JOIN PartDetails pd ON p.artikul_norm = pd.artikul_norm AND p.brand_norm = pd.brand_norm
        LEFT JOIN AllAnalogs aa ON p.artikul_norm = aa.artikul_norm AND p.brand_norm = aa.brand_norm
        WHERE pd.oe_list IS NOT NULL
        ORDER BY p.brand, p.artikul
        """

    from typing import List

    def build_export_query(self, selected_columns: List[str] | None) -> str:
        # Ð¡Ñ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ. ÐžÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ ÐµÐ³Ð¾ ÐºÐ°Ðº ÐµÑÑ‚ÑŒ, Ñ Ð¿ÐµÑ€ÐµÐ½Ð¾ÑÐ°Ð¼Ð¸ ÑÑ‚Ñ€Ð¾Ðº.
        standard_description = """Ð¡Ð¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ñ‚Ð¾Ð²Ð°Ñ€Ð°: Ð½Ð¾Ð²Ñ‹Ð¹ (Ð² ÑƒÐ¿Ð°ÐºÐ¾Ð²ÐºÐµ).
    Ð’Ñ‹ÑÐ¾ÐºÐ¾ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð°Ð²Ñ‚Ð¾Ð·Ð°Ð¿Ñ‡Ð°ÑÑ‚Ð¸ Ð¸ Ð°Ð²Ñ‚Ð¾Ñ‚Ð¾Ð²Ð°Ñ€Ñ‹ â€” Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾Ðµ Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð²Ð°ÑˆÐµÐ³Ð¾ Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»Ñ. 
    ÐžÐ±ÐµÑÐ¿ÐµÑ‡ÑŒÑ‚Ðµ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ, Ð´Ð¾Ð»Ð³Ð¾Ð²ÐµÑ‡Ð½Ð¾ÑÑ‚ÑŒ Ð¸ Ð²Ñ‹ÑÐ¾ÐºÑƒÑŽ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð²Ð°ÑˆÐµÐ³Ð¾ Ð°Ð²Ñ‚Ð¾ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð½Ð°ÑˆÐµÐ³Ð¾ ÑˆÐ¸Ñ€Ð¾ÐºÐ¾Ð³Ð¾ Ð°ÑÑÐ¾Ñ€Ñ‚Ð¸Ð¼ÐµÐ½Ñ‚Ð° Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¸ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ñ‹Ñ… Ð°Ð²Ñ‚Ð¾Ð·Ð°Ð¿Ñ‡Ð°ÑÑ‚ÐµÐ¹.

    Ð’ Ð½Ð°ÑˆÐµÐ¼ ÐºÐ°Ñ‚Ð°Ð»Ð¾Ð³Ðµ Ð²Ñ‹ Ð½Ð°Ð¹Ð´ÐµÑ‚Ðµ Ñ‚Ð¾Ñ€Ð¼Ð¾Ð·Ð½Ñ‹Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹, Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹ (Ð¼Ð°ÑÐ»ÑÐ½Ñ‹Ðµ, Ð²Ð¾Ð·Ð´ÑƒÑˆÐ½Ñ‹Ðµ, ÑÐ°Ð»Ð¾Ð½Ð½Ñ‹Ðµ), ÑÐ²ÐµÑ‡Ð¸ Ð·Ð°Ð¶Ð¸Ð³Ð°Ð½Ð¸Ñ, Ñ€Ð°ÑÑ…Ð¾Ð´Ð½Ñ‹Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ñ‹, Ð°Ð²Ñ‚Ð¾Ñ…Ð¸Ð¼Ð¸ÑŽ, ÑÐ»ÐµÐºÑ‚Ñ€Ð¸ÐºÑƒ, Ð°Ð²Ñ‚Ð¾Ð¼Ð°ÑÐ»Ð°, Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚, Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ð´Ñ€ÑƒÐ³Ð¸Ðµ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑ‚ÑƒÑŽÑ‰Ð¸Ðµ, Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð°Ð¼ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð¸ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸. 

    ÐœÑ‹ Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð±Ñ‹ÑÑ‚Ñ€ÑƒÑŽ Ð´Ð¾ÑÑ‚Ð°Ð²ÐºÑƒ, Ð²Ñ‹Ð³Ð¾Ð´Ð½Ñ‹Ðµ Ñ†ÐµÐ½Ñ‹ Ð¸ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½ÑƒÑŽ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸ÑŽ Ð´Ð»Ñ Ð»ÑŽÐ±Ð¾Ð³Ð¾ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° â€” Ð°Ð²Ñ‚Ð¾Ð»ÑŽÐ±Ð¸Ñ‚ÐµÐ»Ñ, ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸ÑÑ‚Ð° Ð¸Ð»Ð¸ Ð°Ð²Ñ‚Ð¾ÑÐµÑ€Ð²Ð¸ÑÐ°. 

    Ð’Ñ‹Ð±Ð¸Ñ€Ð°Ð¹Ñ‚Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð»ÑƒÑ‡ÑˆÐµÐµ â€” Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚ÑŒ Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¾Ñ‚ Ð²ÐµÐ´ÑƒÑ‰Ð¸Ñ… Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÐµÐ¹."""
        
        # Ð¡Ð¾Ð¿Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶Ð°ÐµÐ¼Ð¾Ð³Ð¾ Ð¸Ð¼ÐµÐ½Ð¸ Ñ Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð¸ÐµÐ¼ SQL
        columns_map = [
            ("ÐÑ€Ñ‚Ð¸ÐºÑƒÐ» Ð±Ñ€ÐµÐ½Ð´Ð°", 'r.artikul AS "ÐÑ€Ñ‚Ð¸ÐºÑƒÐ» Ð±Ñ€ÐµÐ½Ð´Ð°"'),
            ("Ð‘Ñ€ÐµÐ½Ð´", 'r.brand AS "Ð‘Ñ€ÐµÐ½Ð´"'),
            ("ÐÐ°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ", 'COALESCE(r.representative_name, r.analog_representative_name) AS "ÐÐ°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ"'),
            ("ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ð¾ÑÑ‚ÑŒ", 'COALESCE(r.representative_applicability, r.analog_representative_applicability) AS "ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ð¾ÑÑ‚ÑŒ"'),
            # Ð˜Ð—ÐœÐ•ÐÐ•ÐÐ˜Ð•: Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð¼Ñ‹ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ ÐºÐ¾Ð½ÐºÐ°Ñ‚ÐµÐ½Ð¸Ñ€ÑƒÐµÐ¼ Ñ Ð¿Ð¾Ð»ÐµÐ¼ Ð¸Ð· Ð½Ð°ÑˆÐµÐ³Ð¾ Ð½Ð¾Ð²Ð¾Ð³Ð¾ CTE
            ("ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ", "CONCAT(COALESCE(r.description, ''), dt.text) AS \"ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ\""),
            ("ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ Ñ‚Ð¾Ð²Ð°Ñ€Ð°", 'COALESCE(r.representative_category, r.analog_representative_category) AS "ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ Ñ‚Ð¾Ð²Ð°Ñ€Ð°"'),
            ("ÐšÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ", 'r.multiplicity AS "ÐšÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ"'),
            ("Ð”Ð»Ð¸Ð½Ð½Ð°", 'COALESCE(r.length, r.analog_length) AS "Ð”Ð»Ð¸Ð½Ð½Ð°"'),
            ("Ð¨Ð¸Ñ€Ð¸Ð½Ð°", 'COALESCE(r.width, r.analog_width) AS "Ð¨Ð¸Ñ€Ð¸Ð½Ð°"'),
            ("Ð’Ñ‹ÑÐ¾Ñ‚Ð°", 'COALESCE(r.height, r.analog_height) AS "Ð’Ñ‹ÑÐ¾Ñ‚Ð°"'),
            ("Ð’ÐµÑ", 'COALESCE(r.weight, r.analog_weight) AS "Ð’ÐµÑ"'),
            ("Ð”Ð»Ð¸Ð½Ð½Ð°/Ð¨Ð¸Ñ€Ð¸Ð½Ð°/Ð’Ñ‹ÑÐ¾Ñ‚Ð°", "COALESCE(CASE WHEN r.dimensions_str IS NULL OR r.dimensions_str = '' OR UPPER(TRIM(r.dimensions_str)) = 'XX' THEN NULL ELSE r.dimensions_str END, r.analog_dimensions_str) AS \"Ð”Ð»Ð¸Ð½Ð½Ð°/Ð¨Ð¸Ñ€Ð¸Ð½Ð°/Ð’Ñ‹ÑÐ¾Ñ‚Ð°\""),
            ("OE Ð½Ð¾Ð¼ÐµÑ€", 'r.oe_list AS "OE Ð½Ð¾Ð¼ÐµÑ€"'),
            ("Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸", 'r.analog_list AS "Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸"'),
            ("Ð¡ÑÑ‹Ð»ÐºÐ° Ð½Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ", 'r.image_url AS "Ð¡ÑÑ‹Ð»ÐºÐ° Ð½Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ"')
        ]

        if not selected_columns:
            selected_exprs = [expr for _, expr in columns_map]
        else:
            selected_exprs = [expr for name, expr in columns_map if name in selected_columns]
            if not selected_exprs:
                selected_exprs = [expr for _, expr in columns_map]

        # Ð“Ð›ÐÐ’ÐÐžÐ• Ð˜Ð—ÐœÐ•ÐÐ•ÐÐ˜Ð•: ÐœÑ‹ ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ CTE Ñ Ð½Ð°ÑˆÐ¸Ð¼ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ $$ Ð´Ð»Ñ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸.
        # Ð­Ñ‚Ð¾ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð¸Ð·Ð¾Ð»Ð¸Ñ€ÑƒÐµÑ‚ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð¾Ñ‚ Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð»Ð¾Ð³Ð¸ÐºÐ¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°.
        ctes = f"""
        WITH DescriptionTemplate AS (
            SELECT CHR(10) || CHR(10) || $${standard_description}$$ AS text
        ),
        PartDetails AS (
            SELECT
                cr.artikul_norm,
                cr.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(o.oe_number, '''', ''), '[^0-9A-Za-zÐ-Ð¯Ð°-ÑÐÑ‘`\\-\\s]', '', 'g'), ', ') AS oe_list,
                ANY_VALUE(o.name) AS representative_name,
                ANY_VALUE(o.applicability) AS representative_applicability,
                ANY_VALUE(o.category) AS representative_category
            FROM cross_references cr
            JOIN oe_data o ON cr.oe_number_norm = o.oe_number_norm
            GROUP BY cr.artikul_norm, cr.brand_norm
        ),
        AllAnalogs AS (
            SELECT
                cr1.artikul_norm,
                cr1.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(p2.artikul, '''', ''), '[^0-9A-Za-zÐ-Ð¯Ð°-ÑÐÑ‘`\\-\\s]', '', 'g'), ', ') as analog_list
            FROM cross_references cr1
            JOIN cross_references cr2 ON cr1.oe_number_norm = cr2.oe_number_norm
            JOIN parts_data p2 ON cr2.artikul_norm = p2.artikul_norm AND cr2.brand_norm = p2.brand_norm
            WHERE (cr1.artikul_norm != p2.artikul_norm OR cr1.brand_norm != p2.brand_norm)
            GROUP BY cr1.artikul_norm, cr1.brand_norm
        ),
        InitialOENumbers AS (
            SELECT DISTINCT
                p.artikul_norm,
                p.brand_norm,
                cr.oe_number_norm
            FROM parts_data p
            LEFT JOIN cross_references cr ON p.artikul_norm = cr.artikul_norm AND p.brand_norm = cr.brand_norm
            WHERE cr.oe_number_norm IS NOT NULL
        ),
        Level1Analogs AS (
            SELECT DISTINCT
                i.artikul_norm AS source_artikul_norm,
                i.brand_norm AS source_brand_norm,
                cr2.artikul_norm AS related_artikul_norm,
                cr2.brand_norm AS related_brand_norm
            FROM InitialOENumbers i
            JOIN cross_references cr2 ON i.oe_number_norm = cr2.oe_number_norm
            WHERE NOT (i.artikul_norm = cr2.artikul_norm AND i.brand_norm = cr2.brand_norm)
        ),
        Level1OENumbers AS (
            SELECT DISTINCT
                l1.source_artikul_norm,
                l1.source_brand_norm,
                cr3.oe_number_norm
            FROM Level1Analogs l1
            JOIN cross_references cr3 ON l1.related_artikul_norm = cr3.artikul_norm 
                                        AND l1.related_brand_norm = cr3.brand_norm
            WHERE NOT EXISTS (
                SELECT 1 FROM InitialOENumbers i 
                WHERE i.artikul_norm = l1.source_artikul_norm 
                AND i.brand_norm = l1.source_brand_norm 
                AND i.oe_number_norm = cr3.oe_number_norm
            )
        ),
        Level2Analogs AS (
            SELECT DISTINCT
                loe.source_artikul_norm,
                loe.source_brand_norm,
                cr4.artikul_norm AS related_artikul_norm,
                cr4.brand_norm AS related_brand_norm
            FROM Level1OENumbers loe
            JOIN cross_references cr4 ON loe.oe_number_norm = cr4.oe_number_norm
            WHERE NOT (loe.source_artikul_norm = cr4.artikul_norm AND loe.source_brand_norm = cr4.brand_norm)
        ),
        AllRelatedParts AS (
            SELECT DISTINCT source_artikul_norm, source_brand_norm, related_artikul_norm, related_brand_norm
            FROM Level1Analogs
            UNION
            SELECT DISTINCT source_artikul_norm, source_brand_norm, related_artikul_norm, related_brand_norm
            FROM Level2Analogs
        ),
        AggregatedAnalogData AS (
            SELECT
                arp.source_artikul_norm AS artikul_norm,
                arp.source_brand_norm AS brand_norm,
                MAX(CASE WHEN p2.length IS NOT NULL THEN p2.length ELSE NULL END) AS length,
                MAX(CASE WHEN p2.width IS NOT NULL THEN p2.width ELSE NULL END) AS width,
                MAX(CASE WHEN p2.height IS NOT NULL THEN p2.height ELSE NULL END) AS height,
                MAX(CASE WHEN p2.weight IS NOT NULL THEN p2.weight ELSE NULL END) AS weight,
                ANY_VALUE(CASE WHEN p2.dimensions_str IS NOT NULL 
                               AND p2.dimensions_str != '' 
                               AND UPPER(TRIM(p2.dimensions_str)) != 'XX' 
                          THEN p2.dimensions_str ELSE NULL END) AS dimensions_str,
                ANY_VALUE(CASE WHEN pd2.representative_name IS NOT NULL AND pd2.representative_name != '' THEN pd2.representative_name ELSE NULL END) AS representative_name,
                ANY_VALUE(CASE WHEN pd2.representative_applicability IS NOT NULL AND pd2.representative_applicability != '' THEN pd2.representative_applicability ELSE NULL END) AS representative_applicability,
                ANY_VALUE(CASE WHEN pd2.representative_category IS NOT NULL AND pd2.representative_category != '' THEN pd2.representative_category ELSE NULL END) AS representative_category
            FROM AllRelatedParts arp
            JOIN parts_data p2 ON arp.related_artikul_norm = p2.artikul_norm AND arp.related_brand_norm = p2.brand_norm
            LEFT JOIN PartDetails pd2 ON p2.artikul_norm = pd2.artikul_norm AND p2.brand_norm = pd2.brand_norm
            GROUP BY arp.source_artikul_norm, arp.source_brand_norm
        ),
        RankedData AS (
            SELECT
                p.artikul,
                p.brand,
                p.description,
                p.multiplicity,
                p.length,
                p.width,
                p.height,
                p.weight,
                p.dimensions_str,
                p.image_url,
                pd.representative_name,
                pd.representative_applicability,
                pd.representative_category,
                pd.oe_list,
                aa.analog_list,
                p_analog.length AS analog_length,
                p_analog.width AS analog_width,
                p_analog.height AS analog_height,
                p_analog.weight AS analog_weight,
                p_analog.dimensions_str AS analog_dimensions_str,
                p_analog.representative_name AS analog_representative_name,
                p_analog.representative_applicability AS analog_representative_applicability,
                p_analog.representative_category AS analog_representative_category,
                ROW_NUMBER() OVER(PARTITION BY p.artikul_norm, p.brand_norm ORDER BY pd.representative_name DESC NULLS LAST, pd.oe_list DESC NULLS LAST) as rn
            FROM parts_data p
            LEFT JOIN PartDetails pd ON p.artikul_norm = pd.artikul_norm AND p.brand_norm = pd.brand_norm
            LEFT JOIN AllAnalogs aa ON p.artikul_norm = aa.artikul_norm AND p.brand_norm = aa.brand_norm
            LEFT JOIN AggregatedAnalogData p_analog ON p.artikul_norm = p_analog.artikul_norm AND p.brand_norm = p_analog.brand_norm
        )
        """

        select_clause = ",\n            ".join(selected_exprs)

        # Ð˜Ð—ÐœÐ•ÐÐ•ÐÐ˜Ð•: Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ CROSS JOIN Ðº Ð½Ð°ÑˆÐµÐ¼Ñƒ CTE Ñ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼
        query = ctes + r"""
        SELECT
            """ + select_clause + r"""
        FROM RankedData r
        CROSS JOIN DescriptionTemplate dt
        WHERE r.rn = 1
        ORDER BY r.brand, r.artikul
        """

        return query

    def export_to_csv_optimized(self, output_path: str, selected_columns: List[str] | None = None) -> bool:
        total_records = self.conn.execute("SELECT count(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data) AS t").fetchone()[0]
        if total_records == 0:
            st.warning("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°")
            return False
        
        st.info(f"ðŸ“¤ Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ {total_records:,} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð² CSV...")
        try:
            query = self.build_export_query(selected_columns)
            df = self.conn.execute(query).pl()

            # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ðµ ÑÑ‚Ð¾Ð»Ð±Ñ†Ñ‹ Ð² ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð´Ð»Ñ ÐºÐ¾Ð½ÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð½Ð¾ÑÑ‚Ð¸
            dimension_cols = ["Ð”Ð»Ð¸Ð½Ð½Ð°", "Ð¨Ð¸Ñ€Ð¸Ð½Ð°", "Ð’Ñ‹ÑÐ¾Ñ‚Ð°", "Ð’ÐµÑ", "Ð”Ð»Ð¸Ð½Ð½Ð°/Ð¨Ð¸Ñ€Ð¸Ð½Ð°/Ð’Ñ‹ÑÐ¾Ñ‚Ð°", "ÐšÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ"]
            for col_name in dimension_cols:
                if col_name in df.columns:
                    # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð² ÑÑ‚Ñ€Ð¾ÐºÑƒ, Ð·Ð°Ð¼ÐµÐ½ÑÑ null Ð½Ð° Ð¿ÑƒÑÑ‚ÑƒÑŽ ÑÑ‚Ñ€Ð¾ÐºÑƒ
                    df = df.with_columns(
                        pl.when(pl.col(col_name).is_not_null())
                        .then(pl.col(col_name).cast(pl.Utf8))
                        .otherwise(pl.lit(""))
                        .alias(col_name)
                    )

            buf = io.StringIO()
            df.write_csv(buf, separator=';')
            csv_text = buf.getvalue()
            
            with open(output_path, 'wb') as f:
                f.write(b'\xef\xbb\xbf')
                f.write(csv_text.encode('utf-8'))

            file_size = os.path.getsize(output_path) / (1024 * 1024)
            st.success(f"âœ… Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ð² CSV: {output_path} ({file_size:.1f} ÐœÐ‘)")
            return True
        except Exception as e:
            logger.exception("ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð° Ð² CSV")
            st.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð° Ð² CSV: {e}")
            return False
    
    def export_to_excel(self, output_path: Path, selected_columns: List[str] | None = None) -> tuple[bool, Path | None]:
        total_records = self.conn.execute("SELECT count(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data) AS t").fetchone()[0]
        if total_records == 0:
            st.warning("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°")
            return False, None

        st.info(f"ðŸ“¤ Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ {total_records:,} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð² Excel...")
        try:
            num_files = (total_records + EXCEL_ROW_LIMIT - 1) // EXCEL_ROW_LIMIT
            base_query = self.build_export_query(selected_columns)
            exported_files = []
            
            progress_bar = st.progress(0, text=f"ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ðº ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ñƒ {num_files} Ñ„Ð°Ð¹Ð»Ð°(Ð¾Ð²)...")

            for i in range(num_files):
                progress_bar.progress((i + 1) / num_files, text=f"Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ Ñ‡Ð°ÑÑ‚Ð¸ {i+1} Ð¸Ð· {num_files}...")
                offset = i * EXCEL_ROW_LIMIT
                query = f"{base_query} LIMIT {EXCEL_ROW_LIMIT} OFFSET {offset}"
                df = self.conn.execute(query).pl()
                
                # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ðµ ÑÑ‚Ð¾Ð»Ð±Ñ†Ñ‹ Ð² ÑÑ‚Ñ€Ð¾ÐºÐ¸, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Excel Ð½Ðµ Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð» Ð¸Ñ… ÐºÐ°Ðº Ð´Ð°Ñ‚Ñ‹
                dimension_cols = ["Ð”Ð»Ð¸Ð½Ð½Ð°", "Ð¨Ð¸Ñ€Ð¸Ð½Ð°", "Ð’Ñ‹ÑÐ¾Ñ‚Ð°", "Ð’ÐµÑ", "Ð”Ð»Ð¸Ð½Ð½Ð°/Ð¨Ð¸Ñ€Ð¸Ð½Ð°/Ð’Ñ‹ÑÐ¾Ñ‚Ð°", "ÐšÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ"]
                for col_name in dimension_cols:
                    if col_name in df.columns:
                        # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð² ÑÑ‚Ñ€Ð¾ÐºÑƒ, Ð·Ð°Ð¼ÐµÐ½ÑÑ null Ð½Ð° Ð¿ÑƒÑÑ‚ÑƒÑŽ ÑÑ‚Ñ€Ð¾ÐºÑƒ
                        df = df.with_columns(
                            pl.when(pl.col(col_name).is_not_null())
                            .then(pl.col(col_name).cast(pl.Utf8))
                            .otherwise(pl.lit(""))
                            .alias(col_name)
                        )

                file_part_path = output_path.with_name(f"{output_path.stem}_part_{i+1}.xlsx")
                df.write_excel(str(file_part_path))
                exported_files.append(file_part_path)
            
            progress_bar.empty()

            if num_files > 1:
                st.info("ÐÑ€Ñ…Ð¸Ð²Ð°Ñ†Ð¸Ñ Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð² ZIP...")
                zip_path = output_path.with_suffix('.zip')
                with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for file in exported_files:
                        zipf.write(file, file.name)
                        os.remove(file)
                final_path = zip_path
            else:
                final_path = exported_files[0]
                if final_path.name != output_path.name:
                    os.rename(final_path, output_path)
                    final_path = output_path

            file_size = os.path.getsize(final_path) / (1024 * 1024)
            st.success(f"âœ… Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹: {final_path.name} ({file_size:.1f} ÐœÐ‘)")
            return True, final_path

        except Exception as e:
            logger.exception("ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð° Ð² Excel")
            st.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð° Ð² Excel: {e}")
            return False, None
            
    def export_to_parquet(self, output_path: str, selected_columns: List[str] | None = None) -> bool:
        total_records = self.conn.execute("SELECT count(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data) AS t").fetchone()[0]
        if total_records == 0:
            st.warning("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°")
            return False
        
        st.info(f"ðŸ“¤ Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ {total_records:,} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð² Parquet...")
        try:
            query = self.build_export_query(selected_columns)
            df = self.conn.execute(query).pl()
            
            df.write_parquet(output_path)

            file_size = os.path.getsize(output_path) / (1024 * 1024)
            st.success(f"âœ… Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ð² Parquet: {output_path} ({file_size:.1f} ÐœÐ‘)")
            return True
        except Exception as e:
            logger.exception("ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð° Ð² Parquet")
            st.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð° Ð² Parquet: {e}")
            return False

    def show_export_interface(self):
        st.header("ðŸ“¤ Ð£Ð¼Ð½Ñ‹Ð¹ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ…")
        total_records = self.conn.execute("SELECT count(DISTINCT (artikul_norm, brand_norm)) FROM parts_data").fetchone()[0]
        st.info(f"Ð’ÑÐµÐ³Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð´Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð° (ÑÑ‚Ñ€Ð¾Ðº): {total_records:,}")
        
        if total_records == 0:
            st.warning("Ð‘Ð°Ð·Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿ÑƒÑÑ‚Ð° Ð¸Ð»Ð¸ Ð½ÐµÑ‚ ÑÐ²ÑÐ·ÐµÐ¹ Ð´Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°. Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ.")
            return
        # Allow user to choose which columns to include in the export
        available_columns = [
            "ÐÑ€Ñ‚Ð¸ÐºÑƒÐ» Ð±Ñ€ÐµÐ½Ð´Ð°", "Ð‘Ñ€ÐµÐ½Ð´", "ÐÐ°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ", "ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ð¾ÑÑ‚ÑŒ", "ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ",
            "ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ Ñ‚Ð¾Ð²Ð°Ñ€Ð°", "ÐšÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ", "Ð”Ð»Ð¸Ð½Ð½Ð°", "Ð¨Ð¸Ñ€Ð¸Ð½Ð°", "Ð’Ñ‹ÑÐ¾Ñ‚Ð°",
            "Ð’ÐµÑ", "Ð”Ð»Ð¸Ð½Ð½Ð°/Ð¨Ð¸Ñ€Ð¸Ð½Ð°/Ð’Ñ‹ÑÐ¾Ñ‚Ð°", "OE Ð½Ð¾Ð¼ÐµÑ€", "Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸", "Ð¡ÑÑ‹Ð»ÐºÐ° Ð½Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ"
        ]
        selected_columns = st.multiselect("Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ ÑÑ‚Ð¾Ð»Ð±Ñ†Ñ‹ Ð´Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð° (Ð¿ÑƒÑÑ‚Ð¾ = Ð²ÑÐµ)", options=available_columns, default=available_columns)

        export_format = st.radio("Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°:", ["CSV", "Excel (.xlsx)", "Parquet (Ð´Ð»Ñ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ¾Ð²)"], index=0)

        if export_format == "CSV":
            if st.button("ðŸš€ Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ Ð² CSV", type="primary"):
                output_path = self.data_dir / "auto_parts_report.csv"
                with st.spinner("Ð˜Ð´ÐµÑ‚ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚ Ð² CSV..."):
                    success = self.export_to_csv_optimized(str(output_path), selected_columns if selected_columns else None)
                if success:
                    with open(output_path, "rb") as f:
                        st.download_button("ðŸ“¥ Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ CSV Ñ„Ð°Ð¹Ð»", f, "auto_parts_report.csv", "text/csv")

        elif export_format == "Excel (.xlsx)":
            st.info("â„¹ï¸ Ð•ÑÐ»Ð¸ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð±Ð¾Ð»ÑŒÑˆÐµ 1 Ð¼Ð»Ð½, Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð±ÑƒÐ´ÐµÑ‚ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½ Ð½Ð° Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð¸ ÑƒÐ¿Ð°ÐºÐ¾Ð²Ð°Ð½ Ð² ZIP-Ð°Ñ€Ñ…Ð¸Ð².")
            if st.button("ðŸ“Š Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ Ð² Excel", type="primary"):
                output_path = self.data_dir / "auto_parts_report.xlsx"
                with st.spinner("Ð˜Ð´ÐµÑ‚ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚ Ð² Excel..."):
                    success, final_path = self.export_to_excel(output_path, selected_columns if selected_columns else None)
                if success and final_path and final_path.exists():
                    with open(final_path, "rb") as f:
                        mime = "application/zip" if final_path.suffix == ".zip" else "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        st.download_button(f"ðŸ“¥ Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ {final_path.name}", f, final_path.name, mime)
        
        elif export_format == "Parquet (Ð´Ð»Ñ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ¾Ð²)":
            if st.button("âš¡ï¸ Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ Ð² Parquet", type="primary"):
                output_path = self.data_dir / "auto_parts_report.parquet"
                with st.spinner("Ð˜Ð´ÐµÑ‚ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚ Ð² Parquet..."):
                    success = self.export_to_parquet(str(output_path), selected_columns if selected_columns else None)
                if success:
                    with open(output_path, "rb") as f:
                        st.download_button("ðŸ“¥ Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ Parquet Ñ„Ð°Ð¹Ð»", f, "auto_parts_report.parquet", "application/octet-stream")
    
    def delete_by_brand(self, brand_norm: str) -> int:
        """Delete all records for a given normalized brand. Returns count of deleted records."""
        try:
            # Get count before deletion using parameterized query
            count_result = self.conn.execute("SELECT COUNT(*) FROM parts_data WHERE brand_norm = ?", [brand_norm]).fetchone()
            deleted_count = count_result[0] if count_result else 0
            
            if deleted_count == 0:
                logger.info(f"No records found for brand: {brand_norm}")
                return 0
            
            # Delete from parts_data using parameterized query
            self.conn.execute("DELETE FROM parts_data WHERE brand_norm = ?", [brand_norm])
            
            # Delete associated cross_references that no longer have matching parts_data
            self.conn.execute("DELETE FROM cross_references WHERE (artikul_norm, brand_norm) NOT IN (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)")
            
            logger.info(f"Deleted {deleted_count} records for brand: {brand_norm}")
            return deleted_count
        except Exception as e:
            logger.error(f"Error deleting by brand {brand_norm}: {e}")
            raise
    
    def delete_by_artikul(self, artikul_norm: str) -> int:
        """Delete all records for a given normalized artikul. Returns count of deleted records."""
        try:
            # Get count before deletion using parameterized query
            count_result = self.conn.execute("SELECT COUNT(*) FROM parts_data WHERE artikul_norm = ?", [artikul_norm]).fetchone()
            deleted_count = count_result[0] if count_result else 0
            
            if deleted_count == 0:
                logger.info(f"No records found for artikul: {artikul_norm}")
                return 0
            
            # Delete from parts_data using parameterized query
            self.conn.execute("DELETE FROM parts_data WHERE artikul_norm = ?", [artikul_norm])
            
            # Delete associated cross_references that no longer have matching parts_data
            self.conn.execute("DELETE FROM cross_references WHERE (artikul_norm, brand_norm) NOT IN (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)")
            
            logger.info(f"Deleted {deleted_count} records for artikul: {artikul_norm}")
            return deleted_count
        except Exception as e:
            logger.error(f"Error deleting by artikul {artikul_norm}: {e}")
            raise
    
    def get_statistics(self) -> Dict:
        stats = {}
        try:
            stats['total_parts'] = self.get_total_records()
            if stats['total_parts'] == 0:
                return {
                    'total_parts': 0, 'total_oe': 0, 'total_brands': 0,
                    'top_brands': pl.DataFrame(), 'categories': pl.DataFrame()
                }

            total_oe_res = self.conn.execute("SELECT COUNT(*) FROM oe_data").fetchone()
            stats['total_oe'] = total_oe_res[0] if total_oe_res else 0

            total_brands_res = self.conn.execute("SELECT COUNT(DISTINCT brand) FROM parts_data WHERE brand IS NOT NULL").fetchone()
            stats['total_brands'] = total_brands_res[0] if total_brands_res else 0
            
            brand_stats = self.conn.execute("SELECT brand, COUNT(*) as count FROM parts_data WHERE brand IS NOT NULL GROUP BY brand ORDER BY count DESC LIMIT 10").pl()
            stats['top_brands'] = brand_stats
            
            category_stats = self.conn.execute("SELECT category, COUNT(*) as count FROM oe_data WHERE category IS NOT NULL GROUP BY category ORDER BY count DESC").pl()
            stats['categories'] = category_stats
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ±Ð¾Ñ€Ðµ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸: {e}")
            return {
                'total_parts': 0, 'total_oe': 0, 'total_brands': 0,
                'top_brands': pl.DataFrame(), 'categories': pl.DataFrame()
            }
        return stats

def main():
    st.title("ðŸš— AutoParts Catalog - ÐŸÑ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð»Ñ 10+ Ð¼Ð»Ð½ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
    st.markdown("""
    ### ðŸ’ª ÐœÐ¾Ñ‰Ð½Ð°Ñ Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð° Ð´Ð»Ñ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð¼Ð¸ Ð¾Ð±ÑŠÐµÐ¼Ð°Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð°Ð²Ñ‚Ð¾Ð·Ð°Ð¿Ñ‡Ð°ÑÑ‚ÐµÐ¹
    - **Ð˜Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ**: Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐ¹Ñ‚Ðµ Ð½Ð¾Ð²Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹ Ð´Ð»Ñ Ð´Ð¾Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð¸ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ ÐºÐ°Ñ‚Ð°Ð»Ð¾Ð³Ð°.
    - **ÐÐ°Ð´ÐµÐ¶Ð½Ð¾Ðµ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ**: Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· 5-Ñ‚Ð¸ Ñ‚Ð¸Ð¿Ð¾Ð² Ñ„Ð°Ð¹Ð»Ð¾Ð² ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾ ÑÐ»Ð¸Ð²Ð°ÑŽÑ‚ÑÑ Ð² ÐµÐ´Ð¸Ð½ÑƒÑŽ Ð±Ð°Ð·Ñƒ.
    - **ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ**: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ DuckDB Ð´Ð»Ñ Ð¼Ð³Ð½Ð¾Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°.
    - **Ð£Ð¼Ð½Ñ‹Ð¹ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚**: Ð‘Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹ Ð¸ Ð½Ð°Ð´ÐµÐ¶Ð½Ñ‹Ð¹ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚ Ð² CSV, Excel Ð¸Ð»Ð¸ Parquet Ñ Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸ÐµÐ¹ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ñ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð².
    """)
    
    catalog = HighVolumeAutoPartsCatalog()
    
    st.sidebar.title("ðŸ§­ ÐÐ°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ñ")
    menu_option = st.sidebar.radio("Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ:", ["Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…", "Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚", "Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°", "Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸"])
    
    if menu_option == "Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…":
        st.header("ðŸ“¥ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…")
        st.info("""
        **ÐŸÐ¾Ñ€ÑÐ´Ð¾Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹:**
        1. Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ð¾Ð´Ð¸Ð½ Ð¸Ð»Ð¸ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ñ„Ð°Ð¹Ð»Ð¾Ð² Excel (`.xlsx`, `.xls`). ÐÐµ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°Ñ‚ÑŒ Ð²ÑÐµ ÑÑ€Ð°Ð·Ñƒ.
        2. ÐÐ°Ð¶Ð¼Ð¸Ñ‚Ðµ ÐºÐ½Ð¾Ð¿ÐºÑƒ "ÐÐ°Ñ‡Ð°Ñ‚ÑŒ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ".
        3. Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð¿Ñ€Ð¾Ñ‡Ð¸Ñ‚Ð°ÐµÑ‚, Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½Ð¸Ñ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸ Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚/Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰ÑƒÑŽ Ð±Ð°Ð·Ñƒ.
        
        **ðŸ’¡ Ð”Ð¾Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…:**
        - Ð’Ñ‹ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°Ñ‚ÑŒ Ñ„Ð°Ð¹Ð»Ñ‹ **Ð¿Ð¾ Ð¾Ð´Ð½Ð¾Ð¼Ñƒ** Ð¸Ð»Ð¸ **Ð¿Ð°Ñ‡ÐºÐ°Ð¼Ð¸** (Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾).
        - Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼ UPSERT: Ð½Ð¾Ð²Ñ‹Ðµ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÑŽÑ‚ÑÑ, ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÑŽÑ‚ÑÑ.
        - ÐŸÑ€Ð¸ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ð¾Ð¹ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐµ Ñ„Ð°Ð¹Ð»Ð° Ñ Ñ‚ÐµÐ¼Ð¸ Ð¶Ðµ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð°Ð¼Ð¸/Ð±Ñ€ÐµÐ½Ð´Ð°Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð±ÑƒÐ´ÑƒÑ‚ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹, Ð° Ð½Ðµ Ð¿Ñ€Ð¾Ð´ÑƒÐ±Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹.
        - ÐœÐ¾Ð¶Ð½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°Ñ‚ÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ðµ Ñ‚Ð¸Ð¿Ñ‹ Ñ„Ð°Ð¹Ð»Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ñƒ Ð²Ð°Ñ ÐµÑÑ‚ÑŒ - Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÑŽÑ‚ÑÑ.
        
        **Ð¢Ð¸Ð¿Ñ‹ Ð¤Ð°Ð¹Ð»Ð¾Ð²:**
        - **ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ**: OE Ð½Ð¾Ð¼ÐµÑ€Ð°, Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ñ‹, Ð±Ñ€ÐµÐ½Ð´, Ð½Ð°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ.
        - **ÐšÑ€Ð¾ÑÑÑ‹ (OE -> ÐÑ€Ñ‚Ð¸ÐºÑƒÐ»)**: Ð¡Ð²ÑÐ·ÑŒ OE Ð½Ð¾Ð¼ÐµÑ€Ð¾Ð² Ñ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð°Ð¼Ð¸ Ð¸ Ð±Ñ€ÐµÐ½Ð´Ð°Ð¼Ð¸.
        - **Ð¨Ñ‚Ñ€Ð¸Ñ…-ÐºÐ¾Ð´Ñ‹**: Ð¡Ð²ÑÐ·ÑŒ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð¾Ð² ÑÐ¾ ÑˆÑ‚Ñ€Ð¸Ñ…-ÐºÐ¾Ð´Ð°Ð¼Ð¸ Ð¸ ÐºÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒÑŽ.
        - **Ð’ÐµÑÐ¾Ð³Ð°Ð±Ð°Ñ€Ð¸Ñ‚Ñ‹**: Ð Ð°Ð·Ð¼ÐµÑ€Ñ‹ Ð¸ Ð²ÐµÑ Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð².
        - **Ð˜Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ**: Ð¡ÑÑ‹Ð»ÐºÐ¸ Ð½Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ.
        """)
        
        col1, col2 = st.columns(2)
        
        with col1:
            oe_file = st.file_uploader("1. ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ (OE)", type=['xlsx', 'xls'])
            cross_file = st.file_uploader("2. ÐšÑ€Ð¾ÑÑÑ‹ (OE -> ÐÑ€Ñ‚Ð¸ÐºÑƒÐ»)", type=['xlsx', 'xls'])
            barcode_file = st.file_uploader("3. Ð¨Ñ‚Ñ€Ð¸Ñ…-ÐºÐ¾Ð´Ñ‹ Ð¸ ÐºÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ", type=['xlsx', 'xls'])
        with col2:
            dimensions_file = st.file_uploader("4. Ð’ÐµÑÐ¾Ð³Ð°Ð±Ð°Ñ€Ð¸Ñ‚Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ", type=['xlsx', 'xls'])
            images_file = st.file_uploader("5. Ð¡ÑÑ‹Ð»ÐºÐ¸ Ð½Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ", type=['xlsx', 'xls'])

        file_map = {
            'oe': oe_file, 'cross': cross_file, 'barcode': barcode_file,
            'dimensions': dimensions_file, 'images': images_file
        }
        
        if st.button("ðŸš€ ÐÐ°Ñ‡Ð°Ñ‚ÑŒ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ Ð´Ð°Ð½Ð½Ñ‹Ñ…", type="primary"):
            paths_to_process = {}
            any_file_uploaded = False
            for ftype, uploaded_file in file_map.items():
                if uploaded_file:
                    any_file_uploaded = True
                    path = catalog.data_dir / f"{ftype}_data_{int(time.time())}_{uploaded_file.name}"
                    with open(path, "wb") as f: f.write(uploaded_file.getvalue())
                    paths_to_process[ftype] = str(path)
            
            if any_file_uploaded:
                stats = catalog.merge_all_data_parallel(paths_to_process)
                if stats:
                    st.subheader("ðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸")
                    col1, col2, col3 = st.columns(3)
                    col1.metric("ÐžÐ±Ñ‰ÐµÐµ Ð²Ñ€ÐµÐ¼Ñ", f"{stats.get('processing_time', 0):.2f} ÑÐµÐº")
                    col2.metric("Ð’ÑÐµÐ³Ð¾ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð¾Ð² Ð² Ð±Ð°Ð·Ðµ", f"{stats.get('total_records', 0):,}")
                    col3.metric("ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ Ñ„Ð°Ð¹Ð»Ð¾Ð²", f"{len(paths_to_process)}")
            else:
                st.warning("âš ï¸ ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ñ…Ð¾Ñ‚Ñ Ð±Ñ‹ Ð¾Ð´Ð¸Ð½ Ñ„Ð°Ð¹Ð» Ð´Ð»Ñ Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸.")

    elif menu_option == "Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚":
        catalog.show_export_interface()
    
    elif menu_option == "Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°":
        st.header("ðŸ“ˆ Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¿Ð¾ ÐºÐ°Ñ‚Ð°Ð»Ð¾Ð³Ñƒ")
        with st.spinner("Ð¡Ð±Ð¾Ñ€ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸..."):
            stats = catalog.get_statistics()
        
        if stats.get('total_parts', 0) > 0:
            col1, col2, col3 = st.columns(3)
            col1.metric("Ð£Ð½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð¾Ð²", f"{stats.get('total_parts', 0):,}")
            col2.metric("Ð£Ð½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… OE", f"{stats.get('total_oe', 0):,}")
            col3.metric("Ð£Ð½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð±Ñ€ÐµÐ½Ð´Ð¾Ð²", f"{stats.get('total_brands', 0):,}")
            
            st.subheader("ðŸ† Ð¢Ð¾Ð¿-10 Ð±Ñ€ÐµÐ½Ð´Ð¾Ð² Ð¿Ð¾ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ñƒ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð¾Ð²")
            if 'top_brands' in stats and not stats['top_brands'].is_empty():
                st.dataframe(stats['top_brands'].to_pandas(), width='stretch')
            else:
                st.write("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ð¾ Ð±Ñ€ÐµÐ½Ð´Ð°Ð¼.")

            st.subheader("ðŸ“Š Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¿Ð¾ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸ÑÐ¼")
            if 'categories' in stats and not stats['categories'].is_empty():
                st.bar_chart(stats['categories'].to_pandas().set_index('category'))
            else:
                st.write("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ð¾ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸ÑÐ¼.")
        else:
            st.info("Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚. ÐŸÐµÑ€ÐµÐ¹Ð´Ð¸Ñ‚Ðµ Ð² Ñ€Ð°Ð·Ð´ÐµÐ» 'Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…', Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ð°Ñ‡Ð°Ñ‚ÑŒ.")
    
    elif menu_option == "Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸":
        st.header("ðŸ—‘ï¸ Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ð² Ð±Ð°Ð·Ðµ")
        st.warning("âš ï¸ Ð‘ÑƒÐ´ÑŒÑ‚Ðµ Ð¾ÑÑ‚Ð¾Ñ€Ð¾Ð¶Ð½Ñ‹! ÐžÐ¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ Ð½ÐµÐ¾Ð±Ñ€Ð°Ñ‚Ð¸Ð¼Ñ‹.")
        
        management_option = st.radio("Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸ÑŽ:", ["Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð¿Ð¾ Ð±Ñ€ÐµÐ½Ð´Ñƒ", "Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð¿Ð¾ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ñƒ"])
        
        if management_option == "Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð¿Ð¾ Ð±Ñ€ÐµÐ½Ð´Ñƒ":
            st.subheader("ðŸ­ Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð²ÑÐµ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ñ‹ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð±Ñ€ÐµÐ½Ð´Ð°")
            
            # Get list of available brands
            brands_result = catalog.conn.execute("SELECT DISTINCT brand FROM parts_data WHERE brand IS NOT NULL ORDER BY brand").pl()
            available_brands = brands_result['brand'].to_list() if not brands_result.is_empty() else []
            
            if available_brands:
                selected_brand = st.selectbox("Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð±Ñ€ÐµÐ½Ð´ Ð´Ð»Ñ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ:", available_brands)
                
                col1, col2 = st.columns(2)
                
                with col1:
                    brand_norm_result = catalog.conn.execute("SELECT brand_norm FROM parts_data WHERE brand = ? LIMIT 1", [selected_brand]).fetchone()
                    if brand_norm_result:
                        brand_norm = brand_norm_result[0]
                    else:
                        # Fallback: normalize the brand name if not found in DB
                        brand_series = pl.Series([selected_brand])
                        normalized_series = catalog.normalize_key(brand_series)
                        brand_norm = normalized_series[0] if len(normalized_series) > 0 else ""
                    
                    # Count records to delete using parameterized query
                    count_result = catalog.conn.execute("SELECT COUNT(*) FROM parts_data WHERE brand_norm = ?", [brand_norm]).fetchone()
                    count_to_delete = count_result[0] if count_result else 0
                    
                    st.info(f"Ðš ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸ÑŽ: **{count_to_delete}** Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð¸Ð· Ð±Ñ€ÐµÐ½Ð´Ð° '{selected_brand}'")
                
                with col2:
                    confirm_delete_brand = st.checkbox("Ð¯ Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´Ð°ÑŽ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð²ÑÐµÑ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ ÑÑ‚Ð¾Ð³Ð¾ Ð±Ñ€ÐµÐ½Ð´Ð°", key=f"confirm_brand_{selected_brand}")
                    if st.button("âŒ Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð²ÑÐµ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð±Ñ€ÐµÐ½Ð´Ð°", type="secondary", disabled=not confirm_delete_brand):
                        try:
                            deleted = catalog.delete_by_brand(brand_norm)
                            st.success(f"âœ… Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¾ {deleted} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð´Ð»Ñ Ð±Ñ€ÐµÐ½Ð´Ð° '{selected_brand}'")
                            st.rerun()  # ÐŸÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ Ð´Ð»Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ ÑÐ¿Ð¸ÑÐºÐ° Ð±Ñ€ÐµÐ½Ð´Ð¾Ð²
                        except Exception as e:
                            st.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ð¸: {e}")
                    if not confirm_delete_brand:
                        st.caption("âš ï¸ ÐžÑ‚Ð¼ÐµÑ‚ÑŒÑ‚Ðµ Ñ‡ÐµÐºÐ±Ð¾ÐºÑ Ð´Ð»Ñ Ð°ÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸Ð¸ ÐºÐ½Ð¾Ð¿ÐºÐ¸ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ")
            else:
                st.warning("ÐÐµÑ‚ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð±Ñ€ÐµÐ½Ð´Ð¾Ð² Ð´Ð»Ñ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ.")
        
        elif management_option == "Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð¿Ð¾ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ñƒ":
            st.subheader("ðŸ“¦ Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð²ÑÐµ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð°")
            st.info("ðŸ’¡ Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ» (Ð¿Ð¾Ð¸ÑÐº Ð±ÐµÐ· ÑƒÑ‡ÐµÑ‚Ð° Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð° Ð¸ ÑÐ¿ÐµÑ†ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²)")
            
            # Manual input for artikul
            input_artikul = st.text_input("Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ» Ð´Ð»Ñ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ:")
            
            if input_artikul:
                col1, col2 = st.columns(2)
                
                with col1:
                    # Normalize input using the same method as the system
                    if input_artikul:
                        # Use the normalize_key method to ensure consistent normalization
                        input_series = pl.Series([input_artikul])
                        normalized_series = catalog.normalize_key(input_series)
                        artikul_norm = normalized_series[0] if len(normalized_series) > 0 else ""
                    else:
                        artikul_norm = ""
                    
                    # Count records to delete using parameterized query
                    count_result = catalog.conn.execute("SELECT COUNT(*) FROM parts_data WHERE artikul_norm = ?", [artikul_norm]).fetchone()
                    count_to_delete = count_result[0] if count_result else 0
                    
                    if count_to_delete > 0:
                        st.info(f"Ðš ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸ÑŽ: **{count_to_delete}** Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð° '{input_artikul}'")
                    else:
                        st.warning(f"ÐÑ€Ñ‚Ð¸ÐºÑƒÐ» '{input_artikul}' Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ Ð² Ð±Ð°Ð·Ðµ")
                
                with col2:
                    if count_to_delete > 0:
                        confirm_delete_artikul = st.checkbox("Ð¯ Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´Ð°ÑŽ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð²ÑÐµÑ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ ÑÑ‚Ð¾Ð³Ð¾ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð°", key=f"confirm_artikul_{artikul_norm}")
                        if st.button("âŒ Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð²ÑÐµ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð°", type="secondary", disabled=not confirm_delete_artikul):
                            try:
                                deleted = catalog.delete_by_artikul(artikul_norm)
                                st.success(f"âœ… Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¾ {deleted} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð´Ð»Ñ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð° '{input_artikul}'")
                                st.rerun()  # ÐŸÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ Ð´Ð»Ñ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸ Ð¿Ð¾Ð»Ñ Ð²Ð²Ð¾Ð´Ð°
                            except Exception as e:
                                st.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ð¸: {e}")
                        if not confirm_delete_artikul:
                            st.caption("âš ï¸ ÐžÑ‚Ð¼ÐµÑ‚ÑŒÑ‚Ðµ Ñ‡ÐµÐºÐ±Ð¾ÐºÑ Ð´Ð»Ñ Ð°ÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸Ð¸ ÐºÐ½Ð¾Ð¿ÐºÐ¸ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ")

if __name__ == "__main__":
    main()
