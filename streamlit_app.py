import streamlit as st
import duckdb
import polars as pl
import io
import os
import time
import zipfile
import json
from pathlib import Path

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
DATA_DIR = Path("./auto_parts_data")
DATA_DIR.mkdir(exist_ok=True)
DB_PATH = DATA_DIR / "catalog.duckdb"
EXCEL_ROW_LIMIT = 1_000_000

class AutoPartsCatalog:
    def __init__(self):
        self.conn = duckdb.connect(str(DB_PATH))
        self._setup_database()
        self._create_indexes()
        self._init_settings()

    def _setup_database(self):
        # –¢–∞–±–ª–∏—Ü—ã
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE,
                width DOUBLE,
                height DOUBLE,
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                price_with_markup DOUBLE,
                category VARCHAR,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS recommended_prices (
                artikul_norm VARCHAR PRIMARY KEY,
                price DOUBLE
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS price_list (
                artikul VARCHAR,
                brand VARCHAR,
                quantity INTEGER,
                price DOUBLE,
                PRIMARY KEY (artikul, brand)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS markup_settings (
                id INTEGER PRIMARY KEY,
                total_markup DOUBLE,
                brand_markup JSON
            )
        """)
        if not self.conn.execute("SELECT 1 FROM markup_settings").fetchone():
            self.conn.execute("INSERT INTO markup_settings (id, total_markup, brand_markup) VALUES (1, 0, '{}')")

    def _create_indexes(self):
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_oe ON oe_data(oe_number_norm)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_parts ON parts_data(artikul_norm, brand_norm)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_cross ON cross_references(oe_number_norm)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_cross_art ON cross_references(artikul_norm, brand_norm)")

    def _init_settings(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –Ω–∞—Ü–µ–Ω–∫–∏
        pass

    @staticmethod
    def normalize_key(series):
        return (
            series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
            .str.to_lowercase()
        )

    @staticmethod
    def clean_values(series):
        return (
            series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
        )

    def detect_columns(self, actual_cols, expected_cols):
        mapping = {}
        col_variants = {
            'oe_number': ['oe –Ω–æ–º–µ—Ä', 'oe', '–æe', '–Ω–æ–º–µ—Ä', 'code', 'OE'],
            'artikul': ['–∞—Ä—Ç–∏–∫—É–ª', 'article', 'sku'],
            'brand': ['–±—Ä–µ–Ω–¥', 'brand', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', 'manufacturer'],
            'name': ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ', 'name', '–æ–ø–∏—Å–∞–Ω–∏–µ', 'description'],
            'applicability': ['–ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', 'vehicle', 'applicability'],
            'barcode': ['—à—Ç—Ä–∏—Ö-–∫–æ–¥', 'barcode', '—à—Ç—Ä–∏—Ö–∫–æ–¥', 'ean', 'eac13'],
            'multiplicity': ['–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å —à—Ç', '–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å', 'multiplicity'],
            'length': ['–¥–ª–∏–Ω–∞ (—Å–º)', '–¥–ª–∏–Ω–∞', 'length', '–¥–ª–∏–Ω–Ω–∞'],
            'width': ['—à–∏—Ä–∏–Ω–∞ (—Å–º)', '—à–∏—Ä–∏–Ω–∞', 'width'],
            'height': ['–≤—ã—Å–æ—Ç–∞ (—Å–º)', '–≤—ã—Å–æ—Ç–∞', 'height'],
            'weight': ['–≤–µ—Å (–∫–≥)', '–≤–µ—Å, –∫–≥', '–≤–µ—Å', 'weight'],
            'image_url': ['—Å—Å—ã–ª–∫–∞', 'url', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 'image', '–∫–∞—Ä—Ç–∏–Ω–∫–∞'],
            'dimensions_str': ['–≤–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã', '—Ä–∞–∑–º–µ—Ä—ã', 'dimensions', 'size']
        }
        actual_lower = {c.lower(): c for c in actual_cols}
        for exp_col in expected_cols:
            variants = [v.lower() for v in col_variants.get(exp_col, [exp_col])]
            for var in variants:
                for act_lower, act_orig in actual_lower.items():
                    if var in act_lower:
                        mapping[act_orig] = exp_col
                        break
                if exp_col in mapping.values():
                    break
        return mapping

    def read_and_prepare_file(self, path, file_type):
        try:
            df = pl.read_excel(path, engine='calamine')
            if df.is_empty():
                return pl.DataFrame()
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {path}: {e}")
            return pl.DataFrame()
        schemas = {
            'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
            'barcode': ['brand', 'artikul', 'barcode', 'multiplicity'],
            'dimensions': ['artikul', 'brand', 'length', 'width', 'height', 'weight', 'dimensions_str'],
            'images': ['artikul', 'brand', 'image_url'],
            'cross': ['oe_number', 'artikul', 'brand']
        }
        expected_cols = schemas.get(file_type, [])
        col_mapping = self.detect_columns(df.columns, expected_cols)
        if not col_mapping:
            return pl.DataFrame()
        df = df.rename(col_mapping)
        if 'artikul' in df.columns:
            df = df.with_columns(artikul=self.clean_values(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand=self.clean_values(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number=self.clean_values(pl.col('oe_number')))
        key_cols = [c for c in ['oe_number', 'artikul', 'brand'] if c in df.columns]
        if key_cols:
            df = df.unique(subset=key_cols, keep='first')
        if 'artikul' in df.columns:
            df = df.with_columns(artikul_norm=self.normalize_key(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand_norm=self.normalize_key(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number_norm=self.normalize_key(pl.col('oe_number')))
        return df

    def upsert_data(self, table_name, df, pk):
        if df.is_empty():
            return
        df_unique = df.unique(keep='first')
        timestamp = int(time.time())
        self.conn.register(f"temp_{table_name}_{timestamp}", df_unique.to_arrow())
        pk_str = ", ".join([f'"{col}"' for col in pk])
        update_cols = [col for col in df_unique.columns if col not in pk]
        if update_cols:
            update_clause = ", ".join([f'"{col}"=excluded."{col}"' for col in update_cols])
        else:
            update_clause = "DO NOTHING"
        sql = f"""
            INSERT INTO {table_name}
            SELECT * FROM "temp_{table_name}_{timestamp}"
            ON CONFLICT ({pk_str}) {update_clause}
        """
        try:
            self.conn.execute(sql)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ {table_name}: {e}")
        finally:
            self.conn.unregister(f"temp_{table_name}_{timestamp}")

    def process_and_load(self, dataframes):
        st.info("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        total_steps = 2
        progress = st.progress(0)
        step = 0

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ OE
        if 'oe' in dataframes:
            step +=1
            progress.progress(step/total_steps, text=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ OE ({step}/{total_steps})")
            df_oe = dataframes['oe'].filter(pl.col('oe_number_norm') != "")
            oe_df = df_oe.select(['oe_number_norm', 'oe_number', 'name', 'applicability']).unique(subset=['oe_number_norm'])
            if 'name' in oe_df.columns:
                oe_df = oe_df.with_columns(self._category_by_name(pl.col('name')))
            else:
                oe_df = oe_df.with_columns(category=pl.lit('–†–∞–∑–Ω–æ–µ'))
            self.upsert_data('oe_data', oe_df, ['oe_number_norm'])
            cross_df = df_oe.filter(pl.col('artikul_norm') != "").select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ cross
        if 'cross' in dataframes:
            step +=1
            progress.progress(step/total_steps, text=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫—Ä–æ—Å—Å–æ–≤ ({step}/{total_steps})")
            df_cross = dataframes['cross'].filter(
                (pl.col('oe_number_norm') != "") & (pl.col('artikul_norm') != "")
            )
            self.upsert_data('cross_references', df_cross, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ parts
        step +=1
        progress.progress(step/total_steps, text=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—Ä—Ç–∏–∫—É–ª–∞ ({step}/{total_steps})")
        # –ú–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É
        progress.progress(1)
        time.sleep(0.5)
        st.success("üóÉÔ∏è –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

    def _category_by_name(self, name_col):
        categories_map = {
            '–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä': '–ê–≤—Ç–æ—ç–ª–µ–∫—Ç—Ä–∏–∫–∞',
            '—Ñ–∏–ª—å—Ç—Ä': '–§–∏–ª—å—Ç—Ä—ã',
            '–º–∞—Å–ª–æ': '–ú–∞—Å–ª–∞',
            '—Ç–æ—Ä–º–æ–∑': '–¢–æ—Ä–º–æ–∑–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã',
            '—Å–≤–µ—á–∞': '–ê–≤—Ç–æ—ç–ª–µ–∫—Ç—Ä–∏–∫–∞'
        }
        def get_category(name):
            n = name.lower()
            for k, v in categories_map.items():
                if k in n:
                    return v
            return '–†–∞–∑–Ω–æ–µ'
        return name_col.apply(get_category)

    def merge_all_data(self, paths: dict):
        start_time = time.time()
        import concurrent.futures
        futures = {}
        with concurrent.futures.ThreadPoolExecutor() as executor:
            for key, path in paths.items():
                futures[executor.submit(self.read_and_prepare_file, path, key)] = key
            dataframes = {}
            for future in concurrent.futures.as_completed(futures):
                t = futures[future]
                df = future.result()
                if not df.is_empty():
                    dataframes[t] = df
        if dataframes:
            self.process_and_load(dataframes)
        st.success(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫.")

    def get_statistics(self):
        total_parts = self.conn.execute("SELECT COUNT(*) FROM parts_data").fetchone()[0]
        total_oe = self.conn.execute("SELECT COUNT(*) FROM oe_data").fetchone()[0]
        total_brands = self.conn.execute("SELECT COUNT(DISTINCT brand) FROM parts_data WHERE brand IS NOT NULL").fetchone()[0]
        top_brands = self.conn.execute("SELECT brand, COUNT(*) as cnt FROM parts_data WHERE brand IS NOT NULL GROUP BY brand ORDER BY cnt DESC LIMIT 10").fetchdf()
        categories = self.conn.execute("SELECT category, COUNT(*) as cnt FROM oe_data WHERE category IS NOT NULL GROUP BY category ORDER BY cnt DESC").fetchdf()
        return {
            'total_parts': total_parts,
            'total_oe': total_oe,
            'total_brands': total_brands,
            'top_brands': top_brands,
            'categories': categories
        }

    def load_recommended_prices(self, file_bytes):
        df = pl.read_excel(io.BytesIO(file_bytes))
        if '–∞—Ä—Ç–∏–∫—É–ª' not in df.columns or '—Ü–µ–Ω–∞' not in df.columns:
            st.error("–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å '–∞—Ä—Ç–∏–∫—É–ª' –∏ '—Ü–µ–Ω–∞'")
            return
        df = df.select([
            pl.col('–∞—Ä—Ç–∏–∫—É–ª').alias('artikul'),
            pl.col('—Ü–µ–Ω–∞').cast(pl.Float64)
        ])
        for row in df.iter_rows():
            artikul, price = row
            norm_series = self.normalize_key(pl.Series([artikul]))
            artikul_norm = norm_series[0]
            self.conn.execute("""
                INSERT INTO recommended_prices (artikul_norm, price)
                VALUES (?, ?)
                ON CONFLICT (artikul_norm) DO UPDATE SET price=excluded.price
            """, [artikul_norm, price])
        st.success("–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ —Ü–µ–Ω—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã!")

    def load_price_list(self, file_bytes):
        df = pl.read_excel(io.BytesIO(file_bytes))
        required_cols = ['–∞—Ä—Ç–∏–∫—É–ª', '–±—Ä–µ–Ω–¥', '–∫–æ–ª-–≤–æ', '—Ü–µ–Ω–∞']
        if not all(c in df.columns for c in required_cols):
            st.error("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–æ–∫ '–∞—Ä—Ç–∏–∫—É–ª', '–±—Ä–µ–Ω–¥', '–∫–æ–ª-–≤–æ', '—Ü–µ–Ω–∞'")
            return
        df = df.select([
            pl.col('–∞—Ä—Ç–∏–∫—É–ª'),
            pl.col('–±—Ä–µ–Ω–¥'),
            pl.col('–∫–æ–ª-–≤–æ').cast(pl.Int32),
            pl.col('—Ü–µ–Ω–∞').cast(pl.Float64)
        ])
        for row in df.iter_rows():
            artikul, brand, qty, price = row
            norm_artikul = self.normalize_key(pl.Series([artikul]))[0]
            norm_brand = self.normalize_key(pl.Series([brand]))[0]
            self.conn.execute("""
                INSERT INTO price_list (artikul, brand, quantity, price)
                VALUES (?, ?, ?, ?)
                ON CONFLICT (artikul, brand) DO UPDATE SET quantity=excluded.quantity, price=excluded.price
            """, [artikul, brand, qty, price])
        st.success("–ü—Ä–∞–π—Å-–ª–∏—Å—Ç –∑–∞–≥—Ä—É–∂–µ–Ω!")

    def set_markups(self, total_markup, brand_markup):
        self.conn.execute("""
            UPDATE markup_settings SET total_markup=?, brand_markup=?
            WHERE id=1
        """, [total_markup, json.dumps(brand_markup)])
        st.success("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞—Ü–µ–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")

    def get_markups(self):
        row = self.conn.execute("SELECT total_markup, brand_markup FROM markup_settings WHERE id=1").fetchone()
        if row:
            total_markup, brand_markup_json = row
            brand_markup = json.loads(brand_markup_json) if brand_markup_json else {}
            return total_markup, brand_markup
        return 0, {}

    def get_marked_brands(self):
        _, brand_markup = self.get_markups()
        return json.loads(brand_markup) if brand_markup else {}

    def build_export_query(self, selected_columns=None):
        desc_text = """–°–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: –Ω–æ–≤—ã–π (–≤ —É–ø–∞–∫–æ–≤–∫–µ).
–í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã ‚Äî –Ω–∞–¥–µ–∂–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è. 
–û–±–µ—Å–ø–µ—á—å—Ç–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –¥–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç—å –∏ –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ —Å –ø–æ–º–æ—â—å—é –Ω–∞—à–µ–≥–æ —à–∏—Ä–æ–∫–æ–≥–æ –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–µ–π.

–í –Ω–∞—à–µ–º –∫–∞—Ç–∞–ª–æ–≥–µ –≤—ã –Ω–∞–π–¥–µ—Ç–µ —Ç–æ—Ä–º–æ–∑–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, —Ñ–∏–ª—å—Ç—Ä—ã (–º–∞—Å–ª—è–Ω—ã–µ, –≤–æ–∑–¥—É—à–Ω—ã–µ, —Å–∞–ª–æ–Ω–Ω—ã–µ), —Å–≤–µ—á–∏ –∑–∞–∂–∏–≥–∞–Ω–∏—è, —Ä–∞—Å—Ö–æ–¥–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –∞–≤—Ç–æ—Ö–∏–º–∏—é, —ç–ª–µ–∫—Ç—Ä–∏–∫—É, –∞–≤—Ç–æ–º–∞—Å–ª–∞, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∞ —Ç–∞–∫–∂–µ –¥—Ä—É–≥–∏–µ –∫–æ–º–ø–ª–µ–∫—Ç—É—é—â–∏–µ, –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. 

–ú—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –±—ã—Å—Ç—Ä—É—é –¥–æ—Å—Ç–∞–≤–∫—É, –≤—ã–≥–æ–¥–Ω—ã–µ —Ü–µ–Ω—ã –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –¥–ª—è –ª—é–±–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ ‚Äî –∞–≤—Ç–æ–ª—é–±–∏—Ç–µ–ª—è, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –∏–ª–∏ –∞–≤—Ç–æ—Å–µ—Ä–≤–∏—Å–∞. 

–í—ã–±–∏—Ä–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –ª—É—á—à–µ–µ ‚Äî –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç –≤–µ–¥—É—â–∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π."""
        ctes = f"""
        WITH DescriptionTemplate AS (
            SELECT CHR(10) || CHR(10) || $${desc_text}$$ AS text
        ),
        PartDetails AS (
            SELECT
                cr.artikul_norm,
                cr.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(o.oe_number, '''', ''), '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]', '', 'g'), ', ') AS oe_list,
                ANY_VALUE(o.name) AS representative_name,
                ANY_VALUE(o.applicability) AS representative_applicability,
                ANY_VALUE(o.category) AS representative_category
            FROM cross_references cr
            JOIN oe_data o ON cr.oe_number_norm = o.oe_number_norm
            GROUP BY cr.artikul_norm, cr.brand_norm
        ),
        RankedData AS (
            -- –ú–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        )
        """
        # –ö–æ–ª–æ–Ω–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if not selected_columns:
            select_exprs = [
                'p.artikul AS "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞"',
                'p.brand AS "–ë—Ä–µ–Ω–¥"',
                'COALESCE(p.representative_name, p.analog_representative_name) AS "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"',
                'COALESCE(p.representative_applicability, p.analog_representative_applicability) AS "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å"',
                'CONCAT(COALESCE(p.description, \'\'), dt.text) AS "–û–ø–∏—Å–∞–Ω–∏–µ"',
                'COALESCE(p.representative_category, p.analog_representative_category) AS "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞"',
                'p.multiplicity AS "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"',
                'COALESCE(p.length, p.analog_length) AS "–î–ª–∏–Ω–Ω–∞"',
                'COALESCE(p.width, p.analog_width) AS "–®–∏—Ä–∏–Ω–∞"',
                'COALESCE(p.height, p.analog_height) AS "–í—ã—Å–æ—Ç–∞"',
                'COALESCE(p.weight, p.analog_weight) AS "–í–µ—Å"',
                'COALESCE(CASE WHEN p.dimensions_str IS NULL OR p.dimensions_str = \'\' OR UPPER(TRIM(p.dimensions_str)) = \'XX\' THEN NULL ELSE p.dimensions_str END, p.analog_dimensions_str) AS "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞"',
                'p.oe_list AS "OE –Ω–æ–º–µ—Ä"',
                'p.analog_list AS "–∞–Ω–∞–ª–æ–≥–∏"',
                'p.image_url AS "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"',
                'p.price_with_markup AS "–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π"'
            ]
        else:
            select_exprs = []
            for col_name in selected_columns:
                if col_name == "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞":
                    select_exprs.append('p.artikul AS "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞"')
                elif col_name == "–ë—Ä–µ–Ω–¥":
                    select_exprs.append('p.brand AS "–ë—Ä–µ–Ω–¥"')
                elif col_name == "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ":
                    select_exprs.append('COALESCE(p.representative_name, p.analog_representative_name) AS "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"')
                elif col_name == "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å":
                    select_exprs.append('COALESCE(p.representative_applicability, p.analog_representative_applicability) AS "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å"')
                elif col_name == "–û–ø–∏—Å–∞–Ω–∏–µ":
                    select_exprs.append('CONCAT(COALESCE(p.description, \'\'), dt.text) AS "–û–ø–∏—Å–∞–Ω–∏–µ"')
                elif col_name == "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞":
                    select_exprs.append('COALESCE(p.representative_category, p.analog_representative_category) AS "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞"')
                elif col_name == "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å":
                    select_exprs.append('p.multiplicity AS "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"')
                elif col_name == "–î–ª–∏–Ω–Ω–∞":
                    select_exprs.append('COALESCE(p.length, p.analog_length) AS "–î–ª–∏–Ω–Ω–∞"')
                elif col_name == "–®–∏—Ä–∏–Ω–∞":
                    select_exprs.append('COALESCE(p.width, p.analog_width) AS "–®–∏—Ä–∏–Ω–∞"')
                elif col_name == "–í—ã—Å–æ—Ç–∞":
                    select_exprs.append('COALESCE(p.height, p.analog_height) AS "–í—ã—Å–æ—Ç–∞"')
                elif col_name == "–í–µ—Å":
                    select_exprs.append('COALESCE(p.weight, p.analog_weight) AS "–í–µ—Å"')
                elif col_name == "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞":
                    select_exprs.append('COALESCE(CASE WHEN p.dimensions_str IS NULL OR p.dimensions_str = \'\' OR UPPER(TRIM(p.dimensions_str)) = \'XX\' THEN NULL ELSE p.dimensions_str END, p.analog_dimensions_str) AS "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞"')
                elif col_name == "OE –Ω–æ–º–µ—Ä":
                    select_exprs.append('p.oe_list AS "OE –Ω–æ–º–µ—Ä"')
                elif col_name == "–∞–Ω–∞–ª–æ–≥–∏":
                    select_exprs.append('p.analog_list AS "–∞–Ω–∞–ª–æ–≥–∏"')
                elif col_name == "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ":
                    select_exprs.append('p.image_url AS "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"')
                elif col_name == "–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π":
                    select_exprs.append('p.price_with_markup AS "–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π"')
        select_clause = ", ".join(select_exprs)
        query = f"""
        {ctes}
        SELECT {select_clause}
        FROM RankedData p
        CROSS JOIN DescriptionTemplate dt
        WHERE p.rn=1
        ORDER BY p.brand, p.artikul
        """
        return query

    def export_csv(self, output_path, selected_columns=None, exclude_names=None):
        total = self.conn.execute("""
            SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)
        """).fetchone()[0]
        if total == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False
        query = self.build_export_query(selected_columns)
        df = self.conn.execute(query).pl()

        if exclude_names:
            pattern = '|'.join(exclude_names)
            df = df.filter(~pl.col("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ").str.contains(pattern, case=False))
        if '–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π' in df.columns:
            df = df.with_columns(
                pl.col('–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π').apply(lambda p: self.apply_markup(p)).alias('–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π')
            )
        buf = io.StringIO()
        df.write_csv(buf, separator=';')
        with open(output_path, 'wb') as f:
            f.write(b'\xef\xbb\xbf')
            f.write(buf.getvalue().encode('utf-8'))
        st.success(f"–≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {output_path}")
        return True

    def export_excel(self, output_path, selected_columns=None, exclude_names=None):
        total = self.conn.execute("""
            SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)
        """).fetchone()[0]
        if total == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False, None
        num_files = (total + EXCEL_ROW_LIMIT - 1) // EXCEL_ROW_LIMIT
        progress = st.progress(0)
        files = []
        base_query = self.build_export_query(selected_columns)
        for i in range(num_files):
            offset = i * EXCEL_ROW_LIMIT
            query = f"{base_query} LIMIT {EXCEL_ROW_LIMIT} OFFSET {offset}"
            df = self.conn.execute(query).pl()

            if exclude_names:
                pattern = '|'.join(exclude_names)
                df = df.filter(~pl.col("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ").str.contains(pattern, case=False))
            if '–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π' in df.columns:
                df = df.with_columns(
                    pl.col('–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π').apply(lambda p: self.apply_markup(p)).alias('–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π')
                )
            fname = output_path.with_name(f"{output_path.stem}_part_{i+1}.xlsx")
            df.write_excel(str(fname))
            files.append(fname)
            progress.progress((i+1)/num_files)
        if len(files) > 1:
            zip_path = output_path.with_suffix('.zip')
            with zipfile.ZipFile(zip_path, 'w') as zf:
                for f in files:
                    zf.write(f, arcname=f.name)
                    os.remove(f)
            final_path = zip_path
        else:
            final_path = files[0]
            if final_path != output_path:
                os.rename(final_path, output_path)
                final_path = output_path
        st.success(f"–≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {final_path}")
        return True, final_path

    def export_parquet(self, output_path, selected_columns=None, exclude_names=None):
        total = self.conn.execute("""
            SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)
        """).fetchone()[0]
        if total == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False
        query = self.build_export_query(selected_columns)
        df = self.conn.execute(query).pl()

        if exclude_names:
            pattern = '|'.join(exclude_names)
            df = df.filter(~pl.col("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ").str.contains(pattern, case=False))
        if '–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π' in df.columns:
            df = df.with_columns(
                pl.col('–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π').apply(lambda p: self.apply_markup(p)).alias('–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π')
            )
        df.write_parquet(output_path)
        st.success(f"–≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {output_path}")
        return True

    def apply_markup(self, price):
        total, brand = self.get_markups()
        markup = total
        return price * (1 + markup / 100)

    def get_markups(self):
        row = self.conn.execute("SELECT total_markup, brand_markup FROM markup_settings WHERE id=1").fetchone()
        if row:
            total_markup, brand_markup_json = row
            brand_markup = json.loads(brand_markup_json) if brand_markup_json else {}
            return total_markup, brand_markup
        return 0, {}

    def show_export_interface(self):
        st.header("üì§ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")
        total = self.conn.execute("""
            SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)
        """).fetchone()[0]
        if total == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return
        options = [
            "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", "–ë—Ä–µ–Ω–¥", "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", "–û–ø–∏—Å–∞–Ω–∏–µ",
            "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", "–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞",
            "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "OE –Ω–æ–º–µ—Ä", "–∞–Ω–∞–ª–æ–≥–∏", "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π"
        ]
        selected_columns = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –∏ –ø–æ—Ä—è–¥–æ–∫", options=options, default=options)
        exclude_input = st.text_input("–ò—Å–∫–ª—é—á–∏—Ç—å –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —á–µ—Ä–µ–∑ | (—Ç–æ—á–Ω–æ–µ –∏–ª–∏ —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)")
        exclude_names = [n.strip() for n in exclude_input.split('|')] if exclude_input else []

        format_opt = st.radio("–§–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞", ["CSV", "Excel (.xlsx)", "Parquet"], index=0)

        st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞—Ü–µ–Ω–∫–∏")
        total_markup = st.slider("–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞ (%)", 0, 100, 0)
        brand_markups = {}
        if st.checkbox("–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –Ω–∞—Ü–µ–Ω–∫–∏ –ø–æ –±—Ä–µ–Ω–¥–∞–º"):
            brands = self.conn.execute("SELECT DISTINCT brand, brand_norm FROM parts_data WHERE brand IS NOT NULL").fetchall()
            for b, bn in brands:
                mark = st.slider(f"–ù–∞—Ü–µ–Ω–∫–∞ –¥–ª—è –±—Ä–µ–Ω–¥–∞ '{b}'", 0, 100, 0)
                brand_markups[bn] = mark
        if st.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"):
            self.set_markups(total_markup, brand_markups)

        if st.button("üöÄ –ù–∞—á–∞—Ç—å —ç–∫—Å–ø–æ—Ä—Ç"):
            output_path = self.data_dir / "auto_parts_export"
            if format_opt == "CSV":
                out_file = output_path.with_suffix('.csv')
                self.export_csv(str(out_file), selected_columns, exclude_names)
                with open(out_file, "rb") as f:
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å CSV", f, "auto_parts_report.csv", "text/csv")
            elif format_opt == "Excel (.xlsx)":
                out_file = output_path.with_suffix('.xlsx')
                self.export_excel(out_file, selected_columns, exclude_names)
                with open(out_file, "rb") as f:
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å XLSX", f, out_file.name, "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
            elif format_opt == "Parquet":
                out_file = str(output_path.with_suffix('.parquet'))
                self.export_parquet(out_file, selected_columns, exclude_names)
                with open(out_file, "rb") as f:
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å Parquet", f, "auto_parts_report.parquet", "application/octet-stream")

# –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
def main():
    st.set_page_config(page_title="AutoParts Catalog", layout="wide")
    st.title("üöó AutoParts Catalog ‚Äî –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ —ç–∫—Å–ø–æ—Ä—Ç")
    catalog = AutoPartsCatalog()

    menu = st.sidebar.radio("–ú–µ–Ω—é", ["–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö", "–≠–∫—Å–ø–æ—Ä—Ç", "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Ü–µ–Ω", "–ü—Ä–∞–π—Å-–ª–∏—Å—Ç"])

    if menu == "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö":
        st.subheader("–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤")
        col1, col2 = st.columns(2)
        with col1:
            file_oe = st.file_uploader("–û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (OE)", type=['xlsx', 'xls'])
            file_cross = st.file_uploader("–ö—Ä–æ—Å—Å—ã (OE ‚Üí –ê—Ä—Ç–∏–∫—É–ª)", type=['xlsx', 'xls'])
            file_barcode = st.file_uploader("–®—Ç—Ä–∏—Ö-–∫–æ–¥—ã", type=['xlsx', 'xls'])
        with col2:
            file_dim = st.file_uploader("–í–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã", type=['xlsx', 'xls'])
            file_img = st.file_uploader("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", type=['xlsx', 'xls'])
        files_map = {
            'oe': file_oe,
            'cross': file_cross,
            'barcode': file_barcode,
            'dimensions': file_dim,
            'images': file_img
        }
        if st.button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª—ã"):
            paths = {}
            for key, uploaded in files_map.items():
                if uploaded:
                    filename = f"{key}_{int(time.time())}_{uploaded.name}"
                    path = DATA_DIR / filename
                    with open(path, "wb") as f:
                        f.write(uploaded.read())
                    paths[key] = str(path)
            if paths:
                catalog.merge_all_data(paths)
            else:
                st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
    elif menu == "–≠–∫—Å–ø–æ—Ä—Ç":
        catalog.show_export_interface()
    elif menu == "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞":
        stats = catalog.get_statistics()
        st.metric("–ê—Ä—Ç–∏–∫—É–ª–æ–≤", stats['total_parts'])
        st.metric("OE", stats['total_oe'])
        st.metric("–ë—Ä–µ–Ω–¥–æ–≤", stats['total_brands'])
        st.subheader("–¢–æ–ø –±—Ä–µ–Ω–¥–æ–≤")
        st.dataframe(stats['top_brands'])
        st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
        st.dataframe(stats['categories'])
        st.bar_chart(stats['categories'].set_index('category')['cnt'])
    elif menu == "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Ü–µ–Ω":
        st.subheader("–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —Ü–µ–Ω–∞–º")
        uploaded = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å —Ü–µ–Ω–∞–º–∏", type=['xlsx', 'xls'])
        if uploaded:
            catalog.load_recommended_prices(uploaded.read())
    elif menu == "–ü—Ä–∞–π—Å-–ª–∏—Å—Ç":
        st.subheader("–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–∞")
        uploaded = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç", type=['xlsx', 'xls'])
        if uploaded:
            catalog.load_price_list(uploaded.read())


if __name__ == "__main__":
    main()
