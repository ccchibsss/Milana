import polars as pl
import duckdb
import streamlit as st
import os
import time
import io
import zipfile
from pathlib import Path
from typing import Dict, List
import warnings

warnings.filterwarnings('ignore')

EXCEL_ROW_LIMIT = 1_000_000

class AutoPartsCatalog:
    def __init__(self):
        self.data_dir = Path("./auto_parts_data")
        self.data_dir.mkdir(exist_ok=True)
        self.db_path = self.data_dir / "catalog.duckdb"
        self.conn = duckdb.connect(str(self.db_path))
        self.setup_database()
        st.set_page_config(
            page_title="AutoParts Catalog 10M+",
            layout="wide",
            page_icon="üöó"
        )

        # –ù–∞—Ü–µ–Ω–∫–∏
        self.global_markup_percent = 0.0
        self.brand_markups: Dict[str, float] = {}
        self.load_recommended_prices()

    def setup_database(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE, 
                width DOUBLE,
                height DOUBLE, 
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                recommended_price DOUBLE,
                price_with_markup DOUBLE,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS prices (
                artikul VARCHAR PRIMARY KEY,
                recommended_price DOUBLE,
                brand VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS markup_settings (
                id INTEGER PRIMARY KEY,
                global_markup DOUBLE
            )
        """)
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        self.conn.execute("INSERT OR IGNORE INTO markup_settings (id, global_markup) VALUES (1, 0.0)")
        self.load_recommended_prices()

    def load_recommended_prices(self):
        filepath = self.data_dir / "recommended_prices.xlsx"
        if filepath.exists():
            df = pl.read_excel(str(filepath))
            for row in df.rows():
                artikul, price, brand = row
                self.conn.execute("""
                    INSERT OR REPLACE INTO prices (artikul, recommended_price, brand)
                    VALUES (?, ?, ?)
                """, [artikul, price, brand])
            st.success("–ó–∞–≥—Ä—É–∂–µ–Ω—ã —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ —Ü–µ–Ω—ã –∏–∑ —Ñ–∞–π–ª–∞.")
        else:
            pass

    def save_recommended_prices(self, df: pl.DataFrame):
        for row in df.rows():
            artikul, price, brand = row
            self.conn.execute("""
                INSERT OR REPLACE INTO prices (artikul, recommended_price, brand)
                VALUES (?, ?, ?)
            """, [artikul, price, brand])
        st.success("–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ —Ü–µ–Ω—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")

    def set_global_markup(self, percent: float):
        self.global_markup_percent = percent
        self.conn.execute("UPDATE markup_settings SET global_markup = ?", [percent])

    def get_global_markup(self):
        res = self.conn.execute("SELECT global_markup FROM markup_settings WHERE id=1").fetchone()
        return res[0] if res else 0.0

    def set_brand_markup(self, brand: str, percent: float):
        self.brand_markups[brand] = percent

    def get_brand_markup(self, brand: str):
        return self.brand_markups.get(brand, 0.0)

    def create_indexes(self):
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_oe ON oe_data(oe_number_norm)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_parts ON parts_data(artikul_norm, brand_norm)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_cross ON cross_references(oe_number_norm)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_prices ON prices(artikul)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_markup ON markup_settings(id)")

    @staticmethod
    def normalize_key(series: pl.Series) -> pl.Series:
        return (
            series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-zA-za-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
            .str.to_lowercase()
        )

    @staticmethod
    def clean_values(series: pl.Series) -> pl.Series:
        return (
            series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-zA-za-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
        )

    def detect_columns(self, actual_cols: List[str], expected_cols: List[str]) -> Dict[str, str]:
        mapping = {}
        col_variants = {
            'oe_number': ['oe –Ω–æ–º–µ—Ä', 'oe', '–æe', '–Ω–æ–º–µ—Ä', 'code', 'OE'],
            'artikul': ['–∞—Ä—Ç–∏–∫—É–ª', 'article', 'sku'],
            'brand': ['–±—Ä–µ–Ω–¥', 'brand', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', 'manufacturer'],
            'name': ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ', 'name', '–æ–ø–∏—Å–∞–Ω–∏–µ', 'description'],
            'applicability': ['–ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', 'vehicle', 'applicability'],
            'barcode': ['—à—Ç—Ä–∏—Ö-–∫–æ–¥', 'barcode', '—à—Ç—Ä–∏—Ö–∫–æ–¥', 'ean', 'eac13'],
            'multiplicity': ['–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å —à—Ç', '–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å', 'multiplicity'],
            'length': ['–¥–ª–∏–Ω–∞ (—Å–º)', '–¥–ª–∏–Ω–∞', 'length', '–¥–ª–∏–Ω–Ω–∞'],
            'width': ['—à–∏—Ä–∏–Ω–∞ (—Å–º)', '—à–∏—Ä–∏–Ω–∞', 'width'],
            'height': ['–≤—ã—Å–æ—Ç–∞ (—Å–º)', '–≤—ã—Å–æ—Ç–∞', 'height'],
            'weight': ['–≤–µ—Å (–∫–≥)', '–≤–µ—Å, –∫–≥', '–≤–µ—Å', 'weight'],
            'image_url': ['—Å—Å—ã–ª–∫–∞', 'url', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 'image', '–∫–∞—Ä—Ç–∏–Ω–∫–∞'],
            'dimensions_str': ['–≤–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã', '—Ä–∞–∑–º–µ—Ä—ã', 'dimensions', 'size']
        }
        actual_lower = {col.lower(): col for col in actual_cols}
        for key, variants in col_variants.items():
            for variant in variants:
                for actual_l, original_name in actual_lower.items():
                    if variant in actual_l:
                        mapping[original_name] = key
                        break
        return mapping

    def read_and_prepare_file(self, file_path: str, file_type: str):
        try:
            if not os.path.exists(file_path):
                return pl.DataFrame()
            df = pl.read_excel(file_path, engine='calamine')
            if df.is_empty():
                return pl.DataFrame()
            expected = {
                'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
                'barcode': ['barcode', 'artikul', 'brand', 'multiplicity'],
                'dimensions': ['artikul', 'brand', 'length', 'width', 'height', 'weight', 'dimensions_str'],
                'images': ['artikul', 'brand', 'image_url'],
                'cross': ['oe_number', 'artikul', 'brand']
            }
            expected_cols = expected.get(file_type, [])
            col_map = self.detect_columns(df.columns, expected_cols)
            if not col_map:
                return pl.DataFrame()
            df = df.rename(col_map)
            if 'artikul' in df.columns:
                df = df.with_columns(artikul=self.clean_values(pl.col('artikul')))
            if 'brand' in df.columns:
                df = df.with_columns(brand=self.clean_values(pl.col('brand')))
            if 'oe_number' in df.columns:
                df = df.with_columns(oe_number=self.clean_values(pl.col('oe_number')))
            key_cols = [c for c in ['oe_number', 'artikul', 'brand'] if c in df.columns]
            if key_cols:
                df = df.unique(subset=key_cols, keep='first')
            if 'artikul' in df.columns:
                df = df.with_columns(artikul_norm=self.normalize_key(pl.col('artikul')))
            if 'brand' in df.columns:
                df = df.with_columns(brand_norm=self.normalize_key(pl.col('brand')))
            if 'oe_number' in df.columns:
                df = df.with_columns(oe_number_norm=self.normalize_key(pl.col('oe_number')))
            return df
        except:
            return pl.DataFrame()

    def upsert(self, table: str, df: pl.DataFrame, pk: List[str]):
        if df.is_empty():
            return
        df = df.unique(keep='first')
        cols = df.columns
        pk_str = ", ".join([f'"{c}"' for c in pk])
        temp_name = f"temp_{table}_{int(time.time())}"
        self.conn.register(temp_name, df.to_arrow())
        set_clause = ", ".join([f'"{col}"=excluded."{col}"' for col in cols if col not in pk])
        sql = f"""
        INSERT INTO {table} ({', '.join(['"'+c+'"' for c in cols])})
        SELECT * FROM {temp_name}
        ON CONFLICT ({pk_str}) DO UPDATE SET {set_clause};
        """
        self.conn.execute(sql)
        self.conn.unregister(temp_name)

    def process_and_load(self, dfs: Dict[str, pl.DataFrame]):
        st.info("üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        # OE
        if 'oe' in dfs:
            df_oe = dfs['oe'].filter(pl.col('oe_number_norm') != "")
            oe_df = df_oe.select(['oe_number_norm', 'oe_number', 'name', 'applicability']).unique(subset=['oe_number_norm'])
            if 'name' in oe_df.columns:
                oe_df = oe_df.with_columns(self.determine_category_vectorized(pl.col('name')))
            else:
                oe_df = oe_df.with_columns(category=pl.lit('–†–∞–∑–Ω–æ–µ'))
            self.upsert('oe_data', oe_df, ['oe_number_norm'])
            cross_df = df_oe.select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert('cross_references', cross_df, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        # cross
        if 'cross' in dfs:
            df_cross = dfs['cross'].filter((pl.col('oe_number_norm') != "") & (pl.col('artikul_norm') != ""))
            self.upsert('cross_references', df_cross, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        # parts
        all_parts = None
        for f in ['oe', 'barcode', 'images', 'dimensions']:
            df = dfs.get(f)
            if df is None or df.is_empty():
                continue
            if 'artikul_norm' in df.columns:
                temp = df.select(['artikul', 'artikul_norm', 'brand', 'brand_norm'])
                if all_parts is None:
                    all_parts = temp
                else:
                    all_parts = all_parts.join(temp, on=['artikul_norm', 'brand_norm'], how='left', coalesce=True)
        if all_parts is None:
            all_parts = pl.DataFrame()

        # –∑–∞–≥—Ä—É–∑–∫–∞ —Ü–µ–Ω
        prices_df = self.conn.execute("SELECT artikul, recommended_price, brand FROM prices").pl()
        if not all_parts.is_empty():
            all_parts = all_parts.join(prices_df, on='artikul', how='left')
            # —Ä–∞–∑–º–µ—Ä—ã
            for c in ['length','width','height']:
                if c not in all_parts.columns:
                    all_parts = all_parts.with_columns(pl.lit(None).cast(pl.Float64).alias(c))
            if 'dimensions_str' not in all_parts.columns:
                all_parts = all_parts.with_columns(dimensions_str=pl.lit(''))
            # —Å—Ç—Ä–æ–∫–∏ —Ä–∞–∑–º–µ—Ä–æ–≤
            all_parts = all_parts.with_columns([
                pl.col('length').cast(pl.Utf8).fill_null('').alias('_length_str'),
                pl.col('width').cast(pl.Utf8).fill_null('').alias('_width_str'),
                pl.col('height').cast(pl.Utf8).fill_null('').alias('_height_str'),
            ])
            all_parts = all_parts.with_columns(
                pl.when(pl.col('dimensions_str') != '').then(pl.col('dimensions_str'))
                .otherwise(pl.concat_str([pl.col('_length_str'), pl.lit('x'), pl.col('_width_str'), pl.lit('x'), pl.col('_height_str')], separator=''))
                .alias('dimensions_str')
            ).drop(['_length_str', '_width_str', '_height_str'])
            # –æ–ø–∏—Å–∞–Ω–∏–µ
            if 'artikul' not in all_parts.columns:
                all_parts = all_parts.with_columns(artikul=pl.lit(''))
            if 'brand' not in all_parts.columns:
                all_parts = all_parts.with_columns(brand=pl.lit(''))
            all_parts = all_parts.with_columns([
                pl.col('artikul').cast(pl.Utf8).fill_null('').alias('_artikul'),
                pl.col('brand').cast(pl.Utf8).fill_null('').alias('_brand'),
                pl.col('recommended_price').fill_null(0).cast(pl.Float64).alias('_rec_price'),
                pl.col('multiplicity').fill_null(1).cast(pl.Int32).alias('multiplicity'),
            ])
            all_parts = all_parts.with_columns(
                pl.concat_str([
                    '–ê—Ä—Ç–∏–∫—É–ª: ', pl.col('_artikul'),
                    ', –ë—Ä–µ–Ω–¥: ', pl.col('_brand'),
                    ', –ö—Ä–∞—Ç–Ω–æ—Å—Ç—å: ', pl.col('multiplicity').cast(pl.Utf8),
                    ' —à—Ç.'
                ], separator='').alias('description')
            )
            # —Ü–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π
            def compute_price(row):
                base_price = row['recommended_price']
                if base_price is None or base_price==0:
                    return None
                markup_percent = self.get_global_markup()
                brand_markup = self.get_brand_markup(row['brand']) if row['brand'] in self.get_brand_markups() else 0.0
                total_markup = markup_percent + brand_markup
                return base_price * (1 + total_markup/100)

            all_parts = all_parts.with_columns(
                pl.struct([pl.col('brand'), pl.col('recommended_price')])
                .apply(compute_price)
                .alias('price_with_markup')
            )

            # —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            all_parts = all_parts.select([
                'artikul_norm', 'brand_norm', 'artikul', 'brand', 'multiplicity', 'barcode',
                'length','width','height','weight','image_url','dimensions_str','description','price_with_markup'
            ])

            self.upsert('parts_data', all_parts, ['artikul_norm', 'brand_norm'])

        self.create_indexes()
        st.success("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    def get_total_parts(self):
        return self.conn.execute("SELECT COUNT(*) FROM parts_data").fetchone()[0]

    def get_statistics(self):
        total_parts = self.get_total_parts()
        total_oe = self.conn.execute("SELECT COUNT(*) FROM oe_data").fetchone()[0]
        total_brands = self.conn.execute("SELECT COUNT(DISTINCT brand) FROM parts_data").fetchone()[0]
        top_brands = self.conn.execute("SELECT brand, COUNT(*) as cnt FROM parts_data GROUP BY brand ORDER BY cnt DESC LIMIT 10").fetchdf()
        categories = self.conn.execute("SELECT category, COUNT(*) as cnt FROM oe_data WHERE category IS NOT NULL GROUP BY category ORDER BY cnt DESC").fetchdf()
        return {
            'total_parts': total_parts,
            'total_oe': total_oe,
            'total_brands': total_brands,
            'top_brands': top_brands,
            'categories': categories
        }

    def build_export_query(self, selected_cols: List[str]=None, exclude_terms: List[str]=None):
        # –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å
        exclude_where = ""
        if exclude_terms:
            clauses = []
            for term in exclude_terms:
                clauses.append(f"r.\"–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ\" NOT LIKE '%{term}%'")
            if clauses:
                exclude_where = "WHERE " + " AND ".join(clauses)

        columns_map = [
            ("–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", 'p.artikul AS "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞"'),
            ("–ë—Ä–µ–Ω–¥", 'p.brand AS "–ë—Ä–µ–Ω–¥"'),
            ("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", 'COALESCE(p.description, "") AS "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"'),
            ("–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", 'COALESCE(pd.representative_applicability, "") AS "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å"'),
            ("–û–ø–∏—Å–∞–Ω–∏–µ", 'CONCAT(COALESCE(p.description, ""), dt.text) AS "–û–ø–∏—Å–∞–Ω–∏–µ"'),
            ("–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", 'COALESCE(pd.representative_category, "") AS "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞"'),
            ("–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", 'p.multiplicity AS "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"'),
            ("–î–ª–∏–Ω–Ω–∞", 'p.length AS "–î–ª–∏–Ω–Ω–∞"'),
            ("–®–∏—Ä–∏–Ω–∞", 'p.width AS "–®–∏—Ä–∏–Ω–∞"'),
            ("–í—ã—Å–æ—Ç–∞", 'p.height AS "–í—ã—Å–æ—Ç–∞"'),
            ("–í–µ—Å", 'p.weight AS "–í–µ—Å"'),
            ("–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", 'p.dimensions_str AS "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞"'),
            ("OE –Ω–æ–º–µ—Ä", 'pd.oe_list AS "OE –Ω–æ–º–µ—Ä"'),
            ("–∞–Ω–∞–ª–æ–≥–∏", 'aa.analog_list AS "–∞–Ω–∞–ª–æ–≥–∏"'),
            ("–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", 'p.image_url AS "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"'),
            ("–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π", 'p.price_with_markup AS "–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π"')
        ]

        if not selected_cols:
            selected_exprs = [expr for _, expr in columns_map]
        else:
            selected_exprs = []
            for name, expr in columns_map:
                if name in selected_cols:
                    selected_exprs.append(expr)
            if not selected_exprs:
                selected_exprs = [expr for _, expr in columns_map]

        # –û–±—â–∏–π —Ç–µ–∫—Å—Ç
        query = f"""
        WITH DescriptionText AS (
            SELECT CHR(10) || CHR(10) || $${"""–°–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: –Ω–æ–≤—ã–π (–≤ —É–ø–∞–∫–æ–≤–∫–µ).
–í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã ‚Äî –Ω–∞–¥–µ–∂–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è. 
–û–±–µ—Å–ø–µ—á—å—Ç–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –¥–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç—å –∏ –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ —Å –ø–æ–º–æ—â—å—é –Ω–∞—à–µ–≥–æ —à–∏—Ä–æ–∫–æ–≥–æ –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–µ–π.

–í –Ω–∞—à–µ–º –∫–∞—Ç–∞–ª–æ–≥–µ –≤—ã –Ω–∞–π–¥–µ—Ç–µ —Ç–æ—Ä–º–æ–∑–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, —Ñ–∏–ª—å—Ç—Ä—ã (–º–∞—Å–ª—è–Ω—ã–µ, –≤–æ–∑–¥—É—à–Ω—ã–µ, —Å–∞–ª–æ–Ω–Ω—ã–µ), —Å–≤–µ—á–∏ –∑–∞–∂–∏–≥–∞–Ω–∏—è, —Ä–∞—Å—Ö–æ–¥–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –∞–≤—Ç–æ—Ö–∏–º–∏—é, —ç–ª–µ–∫—Ç—Ä–∏–∫—É, –∞–≤—Ç–æ–º–∞—Å–ª–∞, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∞ —Ç–∞–∫–∂–µ –¥—Ä—É–≥–∏–µ –∫–æ–º–ø–ª–µ–∫—Ç—É—é—â–∏–µ, –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. 

–ú—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –±—ã—Å—Ç—Ä—É—é –¥–æ—Å—Ç–∞–≤–∫—É, –≤—ã–≥–æ–¥–Ω—ã–µ —Ü–µ–Ω—ã –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –¥–ª—è –ª—é–±–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ ‚Äî –∞–≤—Ç–æ–ª—é–±–∏—Ç–µ–ª—è, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –∏–ª–∏ –∞–≤—Ç–æ—Å–µ—Ä–≤–∏—Å–∞. 

–í—ã–±–∏—Ä–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –ª—É—á—à–µ–µ ‚Äî –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç –≤–µ–¥—É—â–∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π."""}$$ AS text
        ),
        pd AS (
            SELECT
                cr.artikul_norm,
                cr.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(o.oe_number, '''', ''), '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]', '', 'g'), ', ') AS oe_list,
                ANY_VALUE(o.name) AS representative_name,
                ANY_VALUE(o.applicability) AS representative_applicability,
                ANY_VALUE(o.category) AS representative_category
            FROM cross_references cr
            JOIN oe_data o ON cr.oe_number_norm = o.oe_number_norm
            GROUP BY cr.artikul_norm, cr.brand_norm
        ),
        aa AS (
            SELECT
                cr1.artikul_norm,
                cr1.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(p2.artikul, '''', ''), '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]', '', 'g'), ', ') as analog_list
            FROM cross_references cr1
            JOIN cross_references cr2 ON cr1.oe_number_norm = cr2.oe_number_norm
            JOIN parts_data p2 ON cr2.artikul_norm = p2.artikul_norm AND cr2.brand_norm = p2.brand_norm
            WHERE cr1.artikul_norm != p2.artikul_norm OR cr1.brand_norm != p2.brand_norm
            GROUP BY cr1.artikul_norm, cr1.brand_norm
        )
        SELECT
            {', '.join(selected_exprs)}
        FROM ranked r
        LEFT JOIN DescriptionText dt ON 1=1
        WHERE r.rn=1
        {('AND ' + exclude_where) if exclude_where else ''}
        ORDER BY r.brand, r.artikul
        """
        return query

    def export_csv(self, output_path: str, selected_cols: List[str]=None, exclude_terms: List[str]=None):
        total = self.conn.execute("SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)").fetchone()[0]
        if total == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False
        query = self.build_export_query(selected_cols, exclude_terms)
        df = self.conn.execute(query).pl()
        # —Å—Ç—Ä–æ–∫–∏ —Ä–∞–∑–º–µ—Ä–æ–≤
        for c in ["–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞", "–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π"]:
            if c in df.columns:
                df = df.with_columns(
                    pl.when(pl.col(c).is_not_null())
                    .then(pl.col(c).cast(pl.Utf8))
                    .otherwise("")
                    .alias(c)
                )
        buf = io.StringIO()
        df.write_csv(buf, separator=';')
        with open(output_path, 'wb') as f:
            f.write(b'\xef\xbb\xbf')
            f.write(buf.getvalue().encode('utf-8'))
        size_mb = os.path.getsize(output_path) / 1024 / 1024
        st.success(f"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {total:,} –∑–∞–ø–∏—Å–µ–π. –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {size_mb:.2f} –ú–ë")
        return True

    def export_excel(self, output_path: Path, selected_cols: List[str]=None, exclude_terms: List[str]=None):
        total = self.conn.execute("SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)").fetchone()[0]
        if total == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False, None
        num_files = (total + EXCEL_ROW_LIMIT - 1) // EXCEL_ROW_LIMIT
        for i in range(num_files):
            offset = i * EXCEL_ROW_LIMIT
            query = self.build_export_query(selected_cols, exclude_terms) + f" LIMIT {EXCEL_ROW_LIMIT} OFFSET {offset}"
            df = self.conn.execute(query).pl()
            for c in ["–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞", "–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π"]:
                if c in df.columns:
                    df = df.with_columns(
                        pl.when(pl.col(c).is_not_null())
                        .then(pl.col(c).cast(pl.Utf8))
                        .otherwise("")
                        .alias(c)
                    )
            filename = output_path.with_name(f"{output_path.stem}_part_{i+1}.xlsx")
            df.write_excel(str(filename))
        if num_files > 1:
            zip_path = output_path.with_suffix('.zip')
            with zipfile.ZipFile(zip_path, 'w') as zipf:
                for i in range(num_files):
                    filename = output_path.with_name(f"{output_path.stem}_part_{i+1}.xlsx")
                    zipf.write(str(filename), filename.name)
                    os.remove(str(filename))
            return True, zip_path
        else:
            return True, output_path

    def export_parquet(self, output_path: str, selected_cols: List[str]=None, exclude_terms: List[str]=None):
        total = self.conn.execute("SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)").fetchone()[0]
        if total == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False
        query = self.build_export_query(selected_cols, exclude_terms)
        df = self.conn.execute(query).pl()
        df.write_parquet(output_path)
        size_mb = os.path.getsize(output_path) / 1024 / 1024
        st.success(f"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {total} –∑–∞–ø–∏—Å–µ–π. –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {size_mb:.2f} –ú–ë")
        return True

    def show_export_ui(self):
        st.header("üì§ –£–º–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")
        total = self.conn.execute("SELECT COUNT(DISTINCT artikul_norm, brand_norm) FROM parts_data").fetchone()[0]
        if total == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞.")
            return

        # –ö–æ–ª–æ–Ω–∫–∏ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é drag-and-drop
        default_cols = [
            "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", "–ë—Ä–µ–Ω–¥", "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", "–û–ø–∏—Å–∞–Ω–∏–µ",
            "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", "–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞",
            "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "OE –Ω–æ–º–µ—Ä", "–∞–Ω–∞–ª–æ–≥–∏", "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π"
        ]
        selected_cols = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ (–ø—É—Å—Ç–æ ‚Äî –≤—Å–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é):", options=default_cols, default=default_cols)

        # —Ñ–∏–ª—å—Ç—Ä –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        exclude_input = st.text_input("–ò—Å–∫–ª—é—á–∏—Ç—å –ø–æ–∑–∏—Ü–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—è–º (—á–µ—Ä–µ–∑ |):")
        exclude_terms = [t.strip() for t in exclude_input.split('|')] if exclude_input else []

        format_choice = st.radio("–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤:", ["CSV", "Excel (.xlsx)", "Parquet"], index=0)
        if st.button("–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å"):
            if format_choice == "CSV":
                path = self.data_dir / "auto_parts_export.csv"
                self.export_csv(str(path), selected_cols, exclude_terms)
                with open(str(path), "rb") as f:
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å CSV", f, "auto_parts_export.csv", "text/csv")
            elif format_choice == "Excel (.xlsx)":
                path = self.data_dir / "auto_parts_export.xlsx"
                success, final_path = self.export_excel(path, selected_cols, exclude_terms)
                if success and final_path:
                    with open(str(final_path), "rb") as f:
                        mime = "application/zip" if final_path.suffix == ".zip" else "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        st.download_button("üì• –°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª", f, final_path.name, mime)
            else:
                path = self.data_dir / "auto_parts_export.parquet"
                self.export_parquet(str(path), selected_cols, exclude_terms)
                with open(str(path), "rb") as f:
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å Parquet", f, "auto_parts_export.parquet", "application/octet-stream")

    def show_settings_ui(self):
        st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        st.subheader("–ù–∞—Ü–µ–Ω–∫–∏")
        # –ì–ª–æ–±–∞–ª—å–Ω–∞—è
        new_markup = st.slider("–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞ %:", min_value=0.0, max_value=100.0, value=self.get_global_markup(), step=0.5)
        if st.button("–ü—Ä–∏–º–µ–Ω–∏—Ç—å –Ω–∞—Ü–µ–Ω–∫—É"):
            self.set_global_markup(new_markup)
            st.success(f"–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ {new_markup}%")
        # –ü–æ –±—Ä–µ–Ω–¥–∞–º
        st.subheader("–ù–∞—Ü–µ–Ω–∫–∏ –ø–æ –±—Ä–µ–Ω–¥–∞–º")
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—É—â–∏—Ö
        brands = self.conn.execute("SELECT DISTINCT brand FROM parts_data").fetchdf()
        for b in brands['brand']:
            current = self.get_brand_markup(b)
            new_b_markup = st.slider(f"–ù–∞—Ü–µ–Ω–∫–∞ –¥–ª—è –±—Ä–µ–Ω–¥–∞ '{b}':", min_value=0.0, max_value=100.0, value=current, step=0.5)
            if st.button(f"–ü—Ä–∏–º–µ–Ω–∏—Ç—å –¥–ª—è {b}"):
                self.set_brand_markup(b, new_b_markup)
                st.success(f"–ù–∞—Ü–µ–Ω–∫–∞ –¥–ª—è {b} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ {new_b_markup}%")
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ü–µ–Ω
        st.subheader("–ó–∞–≥—Ä—É–∑–∫–∞ —Ü–µ–Ω –∏–∑ —Ñ–∞–π–ª–∞")
        uploaded_files = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ Excel —Ñ–∞–π–ª —Å —Ü–µ–Ω–∞–º–∏", type=['xlsx','xls'])
        if uploaded_files:
            df_prices = pl.read_excel(uploaded_files)
            self.save_recommended_prices(df_prices)
            st.success("–¶–µ–Ω—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã.")

    def delete_brand(self, brand_norm: str):
        count = self.conn.execute("DELETE FROM parts_data WHERE brand_norm=?", [brand_norm]).fetchone()[0]
        self.conn.execute("DELETE FROM cross_references WHERE (artikul_norm, brand_norm) NOT IN (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)")
        return count

    def delete_artikul(self, artikul_norm: str):
        count = self.conn.execute("DELETE FROM parts_data WHERE artikul_norm=?", [artikul_norm]).fetchone()[0]
        self.conn.execute("DELETE FROM cross_references WHERE (artikul_norm, brand_norm) NOT IN (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)")
        return count

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def main():
    st.title("üöó AutoParts Catalog - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
    st.markdown("""
    ### üí™ –ú–æ—â–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç—è–º–∏
    - –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    - –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫
    - –≠–∫—Å–ø–æ—Ä—Ç –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    - –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ü–µ–Ω –∏ –Ω–∞—Ü–µ–Ω–æ–∫
    """)
    catalog = AutoPartsCatalog()

    menu = st.sidebar.radio("–ù–∞–≤–∏–≥–∞—Ü–∏—è", ["–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö", "–ù–∞—Å—Ç—Ä–æ–π–∫–∏", "–≠–∫—Å–ø–æ—Ä—Ç", "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ"])

    if menu == "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö":
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤
        st.header("üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤")
        total_parts = catalog.get_total_parts()
        if total_parts == 0:
            st.warning("–ë–∞–∑–∞ –ø—É—Å—Ç–∞. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤—Å–µ —Ñ–∞–π–ª—ã –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.")
        else:
            st.info("–ë–∞–∑–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã–µ. –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è—Ç—å —Ñ–∞–π–ª—ã.")
        col1, col2 = st.columns(2)
        with col1:
            file_oe = st.file_uploader("–û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (OE)", type=['xlsx','xls'])
            file_cross = st.file_uploader("–ö—Ä–æ—Å—Å—ã", type=['xlsx','xls'])
            file_prices = st.file_uploader("–¶–µ–Ω—ã (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ)", type=['xlsx','xls'])
        with col2:
            file_dim = st.file_uploader("–í–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã", type=['xlsx','xls'])
            file_img = st.file_uploader("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", type=['xlsx','xls'])
        files_map = {
            'oe': file_oe,
            'cross': file_cross,
            'dimensions': file_dim,
            'images': file_img,
            'prices': file_prices
        }
        if st.button("–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª—ã"):
            dfs = {}
            for key, file in files_map.items():
                if file:
                    path = catalog.data_dir / f"{key}_{int(time.time())}.xlsx"
                    with open(path, 'wb') as f:
                        f.write(file.read())
                    df = catalog.read_and_prepare_file(str(path), key)
                    dfs[key] = df
            catalog.process_and_load(dfs)
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω
            if files_map['prices']:
                df_prices = pl.read_excel(str(path))
                catalog.save_recommended_prices(df_prices)

    elif menu == "–ù–∞—Å—Ç—Ä–æ–π–∫–∏":
        catalog.show_settings_ui()

    elif menu == "–≠–∫—Å–ø–æ—Ä—Ç":
        catalog.show_export_ui()

    elif menu == "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞":
        stats = catalog.get_statistics()
        st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        st.metric("–ê—Ä—Ç–∏–∫—É–ª–æ–≤", stats['total_parts'])
        st.metric("OE", stats['total_oe'])
        st.metric("–ë—Ä–µ–Ω–¥–æ–≤", stats['total_brands'])
        st.subheader("–¢–æ–ø –±—Ä–µ–Ω–¥–æ–≤")
        st.dataframe(stats['top_brands'])
        st.subheader("–ö–∞—Ç–µ–≥–æ—Ä–∏–∏")
        st.dataframe(stats['categories'])

    elif menu == "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ":
        st.header("üóëÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
        action = st.radio("–î–µ–π—Å—Ç–≤–∏–µ", ["–£–¥–∞–ª–∏—Ç—å –ø–æ –±—Ä–µ–Ω–¥—É", "–£–¥–∞–ª–∏—Ç—å –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É"])
        if action == "–£–¥–∞–ª–∏—Ç—å –ø–æ –±—Ä–µ–Ω–¥—É":
            brands = catalog.conn.execute("SELECT DISTINCT brand FROM parts_data").fetchdf()
            if not brands.empty:
                brand = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –±—Ä–µ–Ω–¥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è", brands['brand'].to_list())
                norm = catalog.conn.execute("SELECT brand_norm FROM parts_data WHERE brand=?", [brand]).fetchone()
                if norm:
                    norm = norm[0]
                else:
                    norm = catalog.normalize_key(pl.Series([brand]))[0]
                count = catalog.delete_brand(norm)
                st.success(f"–£–¥–∞–ª–µ–Ω–æ {count} –∑–∞–ø–∏—Å–µ–π –ø–æ –±—Ä–µ–Ω–¥—É {brand}")
            else:
                st.info("–ù–µ—Ç –±—Ä–µ–Ω–¥–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è.")
        else:
            artikul = st.text_input("–í–≤–µ–¥–∏—Ç–µ –∞—Ä—Ç–∏–∫—É–ª –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
            if artikul:
                norm = catalog.normalize_key(pl.Series([artikul]))[0]
                count = catalog.delete_artikul(norm)
                st.success(f"–£–¥–∞–ª–µ–Ω–æ {count} –∑–∞–ø–∏—Å–µ–π –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É {artikul}")

if __name__ == "__main__":
    main()
