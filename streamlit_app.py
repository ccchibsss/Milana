import polars as pl
import duckdb
import streamlit as st
import os
import time
import io
import zipfile
import json
from pathlib import Path
from typing import Dict, List

# Ð’Ñ€ÐµÐ¼Ñ Ð»Ð¸Ð¼Ð¸Ñ‚ Ð´Ð»Ñ Excel
EXCEL_ROW_LIMIT = 1_000_000

class HighVolumeAutoPartsCatalog:
    def __init__(self):
        self.data_dir = Path("./auto_parts_data")
        self.data_dir.mkdir(exist_ok=True)
        self.db_path = self.data_dir / "catalog.duckdb"
        self.conn = duckdb.connect(database=str(self.db_path))
        self.setup_database()
        self.create_indexes()

        st.set_page_config(
            page_title="AutoParts Catalog 10M+",
            layout="wide",
            page_icon="ðŸš—"
        )

    def setup_database(self):
        # Ð¢Ð°Ð±Ð»Ð¸Ñ†Ñ‹
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE,
                width DOUBLE,
                height DOUBLE,
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS recommended_prices (
                artikul_norm VARCHAR PRIMARY KEY,
                price DOUBLE
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS price_list (
                artikul VARCHAR,
                brand VARCHAR,
                quantity INTEGER,
                price DOUBLE,
                PRIMARY KEY (artikul, brand)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS markup_settings (
                id INTEGER PRIMARY KEY,
                total_markup DOUBLE,
                brand_markup JSON
            )
        """)
        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐº
        if not self.conn.execute("SELECT 1 FROM markup_settings").fetchone():
            self.conn.execute("INSERT INTO markup_settings (id, total_markup, brand_markup) VALUES (1, 0, '{}')")

    def create_indexes(self):
        # Ð˜Ð½Ð´ÐµÐºÑÑ‹
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_oe_data_oe ON oe_data(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_parts_data_keys ON parts_data(artikul_norm, brand_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_oe ON cross_references(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_artikul ON cross_references(artikul_norm, brand_norm)"
        ]
        for index_sql in indexes:
            self.conn.execute(index_sql)

    # --- ÐœÐµÑ‚Ð¾Ð´Ñ‹ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¸ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… ---
    @staticmethod
    def normalize_key(key_series: pl.Series) -> pl.Series:
        return (
            key_series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-zÐ-Ð¯Ð°-ÑÐÑ‘`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
            .str.to_lowercase()
        )

    @staticmethod
    def clean_values(value_series: pl.Series) -> pl.Series:
        return (
            value_series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-zÐ-Ð¯Ð°-ÑÐÑ‘`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
        )

    def detect_columns(self, actual_columns: List[str], expected_columns: List[str]) -> Dict[str, str]:
        mapping = {}
        column_variants = {
            'oe_number': ['oe Ð½Ð¾Ð¼ÐµÑ€', 'oe', 'Ð¾e', 'Ð½Ð¾Ð¼ÐµÑ€', 'code', 'OE'],
            'artikul': ['Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»', 'article', 'sku'],
            'brand': ['Ð±Ñ€ÐµÐ½Ð´', 'brand', 'Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒ', 'manufacturer'],
            'name': ['Ð½Ð°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ', 'Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ', 'name', 'Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ', 'description'],
            'applicability': ['Ð¿Ñ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ð¾ÑÑ‚ÑŒ', 'Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»ÑŒ', 'vehicle', 'applicability'],
            'barcode': ['ÑˆÑ‚Ñ€Ð¸Ñ…-ÐºÐ¾Ð´', 'barcode', 'ÑˆÑ‚Ñ€Ð¸Ñ…ÐºÐ¾Ð´', 'ean', 'eac13'],
            'multiplicity': ['ÐºÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ ÑˆÑ‚', 'ÐºÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ', 'multiplicity'],
            'length': ['Ð´Ð»Ð¸Ð½Ð° (ÑÐ¼)', 'Ð´Ð»Ð¸Ð½Ð°', 'length', 'Ð´Ð»Ð¸Ð½Ð½Ð°'],
            'width': ['ÑˆÐ¸Ñ€Ð¸Ð½Ð° (ÑÐ¼)', 'ÑˆÐ¸Ñ€Ð¸Ð½Ð°', 'width'],
            'height': ['Ð²Ñ‹ÑÐ¾Ñ‚Ð° (ÑÐ¼)', 'Ð²Ñ‹ÑÐ¾Ñ‚Ð°', 'height'],
            'weight': ['Ð²ÐµÑ (ÐºÐ³)', 'Ð²ÐµÑ, ÐºÐ³', 'Ð²ÐµÑ', 'weight'],
            'image_url': ['ÑÑÑ‹Ð»ÐºÐ°', 'url', 'Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ', 'image', 'ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ°'],
            'dimensions_str': ['Ð²ÐµÑÐ¾Ð³Ð°Ð±Ð°Ñ€Ð¸Ñ‚Ñ‹', 'Ñ€Ð°Ð·Ð¼ÐµÑ€Ñ‹', 'dimensions', 'size']
        }
        actual_lower = {col.lower(): col for col in actual_columns}
        for expected in expected_columns:
            variants = [v.lower() for v in column_variants.get(expected, [expected])]
            for variant in variants:
                for actual_l, actual_orig in actual_lower.items():
                    if variant in actual_l:
                        mapping[actual_orig] = expected
                        break
                if expected in mapping.values():
                    break
        return mapping

    # --- ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ„Ð°Ð¹Ð»Ð¾Ð² ---
    def read_and_prepare_file(self, file_path: str, file_type: str) -> pl.DataFrame:
        logger.info(f"ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ„Ð°Ð¹Ð»Ð°: {file_type} ({file_path})")
        try:
            if not os.path.exists(file_path):
                logger.error(f"Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½: {file_path}")
                return pl.DataFrame()
            df = pl.read_excel(file_path, engine='calamine')
            if df.is_empty():
                logger.warning(f"ÐŸÑƒÑÑ‚Ð¾Ð¹ Ñ„Ð°Ð¹Ð»: {file_path}")
                return pl.DataFrame()
        except Exception as e:
            logger.exception(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ñ‡Ñ‚ÐµÐ½Ð¸Ð¸ Ñ„Ð°Ð¹Ð»Ð° {file_path}: {e}")
            return pl.DataFrame()

        schemas = {
            'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
            'barcode': ['brand', 'artikul', 'barcode', 'multiplicity'],
            'dimensions': ['artikul', 'brand', 'length', 'width', 'height', 'weight', 'dimensions_str'],
            'images': ['artikul', 'brand', 'image_url'],
            'cross': ['oe_number', 'artikul', 'brand']
        }
        expected_cols = schemas.get(file_type, [])
        column_mapping = self.detect_columns(df.columns, expected_cols)

        if not column_mapping:
            logger.warning(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ Ð´Ð»Ñ Ñ„Ð°Ð¹Ð»Ð° {file_type}. Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ: {df.columns}")
            return pl.DataFrame()

        df = df.rename(column_mapping)

        # ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹
        if 'artikul' in df.columns:
            df = df.with_columns(artikul=self.clean_values(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand=self.clean_values(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number=self.clean_values(pl.col('oe_number')))

        key_cols = [col for col in ['oe_number', 'artikul', 'brand'] if col in df.columns]
        if key_cols:
            df = df.unique(subset=key_cols, keep='first')

        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÐºÐ»ÑŽÑ‡ÐµÐ¹
        if 'artikul' in df.columns:
            df = df.with_columns(artikul_norm=self.normalize_key(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand_norm=self.normalize_key(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number_norm=self.normalize_key(pl.col('oe_number')))
        return df

    def upsert_data(self, table_name: str, df: pl.DataFrame, pk: List[str]):
        if df.is_empty():
            return
        df = df.unique(keep='first')
        cols = df.columns
        pk_str = ", ".join(f'"{c}"' for c in pk)
        temp_view_name = f"temp_{table_name}_{int(time.time())}"
        self.conn.register(temp_view_name, df.to_arrow())

        update_cols = [col for col in cols if col not in pk]
        if not update_cols:
            on_conflict_action = "DO NOTHING"
        else:
            update_clause = ", ".join([f'"{col}" = excluded."{col}"' for col in update_cols])
            on_conflict_action = f"DO UPDATE SET {update_clause}"

        sql = f"""
        INSERT INTO {table_name}
        SELECT * FROM {temp_view_name}
        ON CONFLICT ({pk_str}) {on_conflict_action};
        """
        try:
            self.conn.execute(sql)
            logger.info(f"ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾/Ð²ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¾ {len(df)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð² {table_name}")
        except Exception as e:
            logger.exception(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ UPSERT {table_name}: {e}")
            st.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð² Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ {table_name}")
        finally:
            self.conn.unregister(temp_view_name)

    def process_and_load_data(self, dataframes: Dict[str, pl.DataFrame]):
        st.info("ðŸ”„ ÐÐ°Ñ‡Ð¸Ð½Ð°ÑŽ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÑƒ Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
        steps = ['oe', 'cross', 'parts']
        num_steps = len(steps)
        progress_bar = st.progress(0, text="ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ°...")
        step_counter = 0

        # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° OE
        if 'oe' in dataframes:
            step_counter += 1
            progress_bar.progress(step_counter / (num_steps + 1), f"({step_counter}/{num_steps}) ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° OE")
            df_oe = dataframes['oe'].filter(pl.col('oe_number_norm') != "")
            oe_df = df_oe.select(['oe_number_norm', 'oe_number', 'name', 'applicability']).unique(subset=['oe_number_norm'])
            if 'name' in oe_df.columns:
                oe_df = oe_df.with_columns(self.determine_category_vectorized(pl.col('name')))
            else:
                oe_df = oe_df.with_columns(category=pl.lit('Ð Ð°Ð·Ð½Ð¾Ðµ'))
            self.upsert_data('oe_data', oe_df, ['oe_number_norm'])

            cross_df_from_oe = df_oe.filter(pl.col('artikul_norm') != "").select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df_from_oe, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° cross
        if 'cross' in dataframes:
            step_counter += 1
            progress_bar.progress(step_counter / (num_steps + 1), f"({step_counter}/{num_steps}) ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÐºÑ€Ð¾ÑÑÐ¾Ð²")
            df_cross = dataframes['cross'].filter(
                (pl.col('oe_number_norm') != "") & (pl.col('artikul_norm') != "")
            )
            self.upsert_data('cross_references', df_cross, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° parts
        step_counter += 1
        progress_bar.progress(step_counter / (num_steps + 1), f"({step_counter}/{num_steps}) ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð°")
        # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ð²ÑÐµÑ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð°
        parts_df = None
        files_order = ['oe', 'barcode', 'images', 'dimensions']
        key_files = {ftype: dataframes[ftype] for ftype in files_order if ftype in dataframes}
        if key_files:
            all_parts = pl.concat([
                df.select(['artikul', 'artikul_norm', 'brand', 'brand_norm'])
                for df in key_files.values()
                if 'artikul_norm' in df.columns and 'brand_norm' in df.columns
            ]).filter(pl.col('artikul_norm') != "").unique(subset=['artikul_norm', 'brand_norm'])
            parts_df = all_parts

            # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ð¾ Ñ„Ð°Ð¹Ð»Ð°Ð¼
            for ftype in files_order:
                if ftype not in key_files:
                    continue
                df = key_files[ftype]
                if df.is_empty() or 'artikul_norm' not in df.columns:
                    continue
                join_cols = [col for col in df.columns if col not in ['artikul', 'artikul_norm', 'brand', 'brand_norm']]
                if not join_cols:
                    continue
                existing_cols = set(parts_df.columns)
                join_cols = [col for col in join_cols if col not in existing_cols]
                if not join_cols:
                    continue
                df_subset = df.select(['artikul_norm', 'brand_norm'] + join_cols).unique(subset=['artikul_norm', 'brand_norm'])
                parts_df = parts_df.join(df_subset, on=['artikul_norm', 'brand_norm'], how='left', coalesce=True)

        # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ€Ð°Ð·Ð¼ÐµÑ€Ð¾Ð², Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ
        if parts_df is not None and not parts_df.is_empty():
            # multiplicity
            if 'multiplicity' not in parts_df.columns:
                parts_df = parts_df.with_columns(pl.lit(1).cast(pl.Int32).alias('multiplicity'))
            else:
                parts_df = parts_df.with_columns(pl.col('multiplicity').fill_null(1).cast(pl.Int32))
            # dimensions
            for col in ['length', 'width', 'height']:
                if col not in parts_df.columns:
                    parts_df = parts_df.with_columns(pl.lit(None).cast(pl.Float64).alias(col))
            if 'dimensions_str' not in parts_df.columns:
                parts_df = parts_df.with_columns(dimensions_str=pl.lit(None).cast(pl.Utf8))
            # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ dimensions_str
            parts_df = parts_df.with_columns([
                pl.col('length').cast(pl.Utf8).fill_null('').alias('_length_str'),
                pl.col('width').cast(pl.Utf8).fill_null('').alias('_width_str'),
                pl.col('height').cast(pl.Utf8).fill_null('').alias('_height_str')
            ])

            parts_df = parts_df.with_columns(
                pl.when(
                    (pl.col('dimensions_str').is_not_null()) &
                    (pl.col('dimensions_str') != '') &
                    (pl.col('dimensions_str').cast(pl.Utf8).str.to_upper().alias('dim_upper') != 'XX')
                )
                .then(pl.col('dimensions_str'))
                .otherwise(
                    pl.concat_str([pl.col('_length_str'), pl.lit('x'), pl.col('_width_str'), pl.lit('x'), pl.col('_height_str')], separator='')
                ).alias('dimensions_str')
            )
            parts_df = parts_df.drop(['_length_str', '_width_str', '_height_str'])

            # ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ
            if 'artikul' not in parts_df.columns:
                parts_df = parts_df.with_columns(artikul=pl.lit(''))
            if 'brand' not in parts_df.columns:
                parts_df = parts_df.with_columns(brand=pl.lit(''))

            parts_df = parts_df.with_columns([
                pl.col('artikul').cast(pl.Utf8).fill_null('').alias('_artikul'),
                pl.col('brand').cast(pl.Utf8).fill_null('').alias('_brand'),
                pl.col('multiplicity').cast(pl.Utf8).alias('_multiplicity')
            ])

            parts_df = parts_df.with_columns(
                pl.concat_str([
                    'ÐÑ€Ñ‚Ð¸ÐºÑƒÐ»: ', pl.col('_artikul'),
                    ', Ð‘Ñ€ÐµÐ½Ð´: ', pl.col('_brand'),
                    ', ÐšÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ: ', pl.col('_multiplicity'), ' ÑˆÑ‚.'
                ], separator='').alias('description')
            )
            parts_df = parts_df.drop(['_artikul', '_brand', '_multiplicity'])

            # Ð’Ñ‹Ð±Ð¾Ñ€ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº
            final_cols = [
                'artikul_norm', 'brand_norm', 'artikul', 'brand', 'multiplicity', 'barcode',
                'length', 'width', 'height', 'weight', 'image_url', 'dimensions_str', 'description'
            ]
            select_exprs = []
            for c in final_cols:
                if c in parts_df.columns:
                    select_exprs.append(pl.col(c))
                else:
                    select_exprs.append(pl.lit(None).alias(c))
            parts_df = parts_df.select(select_exprs)

            # Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ñ†ÐµÐ½Ñ‹ Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ Ð½Ð°Ñ†ÐµÐ½ÐºÐ¸
            # Ð’ Ñ†Ð¸ÐºÐ»Ðµ Ð¿Ñ€Ð¸ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ðµ
            self.upsert_data('parts_data', parts_df, ['artikul_norm', 'brand_norm'])

        progress_bar.progress(1.0, text="ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð±Ð°Ð·Ñ‹ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾")
        time.sleep(1)
        progress_bar.empty()
        st.success("ðŸ’¾ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°.")

    def merge_all_data_parallel(self, file_paths: Dict[str, str]) -> Dict:
        start_time = time.time()
        stats = {}
        st.info("ðŸš€ ÐÐ°Ñ‡Ð¸Ð½Ð°ÑŽ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½ÑƒÑŽ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ Ñ„Ð°Ð¹Ð»Ð¾Ð²")
        n_files = len(file_paths)
        file_progress = st.progress(0, text="ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ„Ð°Ð¹Ð»Ð¾Ð²...")

        dataframes = {}
        processed_files = 0
        with st.runtime.scriptrunner.ThreadPoolExecutor() as executor:
            futures = {
                executor.submit(self.read_and_prepare_file, path, ftype): ftype
                for ftype, path in file_paths.items()
            }
            for future in futures:
                ftype = futures[future]
                try:
                    df = future.result()
                    if not df.is_empty():
                        dataframes[ftype] = df
                        st.success(f"Ð¤Ð°Ð¹Ð» '{ftype}' Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½")
                    else:
                        st.warning(f"Ð¤Ð°Ð¹Ð» '{ftype}' Ð¿ÑƒÑÑ‚ Ð¸Ð»Ð¸ Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ")
                except Exception as e:
                    st.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ {ftype}: {e}")
                processed_files += 1
                file_progress.progress(processed_files / n_files)
        file_progress.empty()

        if not dataframes:
            st.error("ÐÐµÑ‚ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…")
            return {}

        self.process_and_load_data(dataframes)

        stats['processing_time'] = time.time() - start_time
        stats['total_records'] = self.get_total_records()
        st.success(f"ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð° Ð·Ð° {stats['processing_time']:.2f} ÑÐµÐºÑƒÐ½Ð´")
        st.success(f"Ð’ÑÐµÐ³Ð¾ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð¾Ð²: {stats['total_records']:,}")
        self.create_indexes()
        return stats

    def get_total_records(self) -> int:
        try:
            return self.conn.execute("SELECT COUNT(*) FROM parts_data").fetchone()[0]
        except:
            return 0

    def get_statistics(self) -> Dict:
        stats = {}
        try:
            stats['total_parts'] = self.get_total_records()
            if stats['total_parts'] == 0:
                return {
                    'total_parts': 0, 'total_oe': 0, 'total_brands': 0,
                    'top_brands': pl.DataFrame(), 'categories': pl.DataFrame()
                }
            stats['total_oe'] = self.conn.execute("SELECT COUNT(*) FROM oe_data").fetchone()[0]
            stats['total_brands'] = self.conn.execute("SELECT COUNT(DISTINCT brand) FROM parts_data WHERE brand IS NOT NULL").fetchone()[0]
            # top brands
            br_res = self.conn.execute(
                "SELECT brand, COUNT(*) as count FROM parts_data WHERE brand IS NOT NULL GROUP BY brand ORDER BY count DESC LIMIT 10"
            ).fetchall()
            stats['top_brands'] = pl.DataFrame(br_res, schema=["brand", "count"])
            # ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸
            cat_res = self.conn.execute(
                "SELECT category, COUNT(*) as count FROM oe_data WHERE category IS NOT NULL GROUP BY category ORDER BY count DESC"
            ).fetchall()
            stats['categories'] = pl.DataFrame(cat_res, schema=["category", "count"])
        except Exception as e:
            st.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ±Ð¾Ñ€Ðµ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸: {e}")
            stats = {
                'total_parts': 0,
                'total_oe': 0,
                'total_brands': 0,
                'top_brands': pl.DataFrame(),
                'categories': pl.DataFrame()
            }
        return stats

    # --- ÐœÐµÑ‚Ð¾Ð´Ñ‹ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ñ†ÐµÐ½ Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐº ---
    def load_price_recommendation(self, file_bytes):
        df = pl.read_excel(io.BytesIO(file_bytes))
        if 'Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»' not in df.columns or 'Ñ†ÐµÐ½Ð°' not in df.columns:
            st.error("Ð¤Ð°Ð¹Ð» Ð´Ð¾Ð»Ð¶ÐµÐ½ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ñ‚ÑŒ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ 'Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»' Ð¸ 'Ñ†ÐµÐ½Ð°'")
            return
        df = df.select([
            pl.col('Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»').alias('artikul'),
            pl.col('Ñ†ÐµÐ½Ð°').cast(pl.Float64)
        ])
        for row in df.iter_rows():
            artikul = row[0]
            price = row[1]
            artikul_norm_series = self.normalize_key(pl.Series([artikul]))
            artikul_norm = artikul_norm_series[0] if len(artikul_norm_series) > 0 else ''
            self.conn.execute("""
                INSERT INTO recommended_prices (artikul_norm, price)
                VALUES (?, ?)
                ON CONFLICT (artikul_norm) DO UPDATE SET price=excluded.price
            """, [artikul_norm, price])
        st.success("Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ñ†ÐµÐ½Ñ‹ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹.")

    def load_price_list(self, file_bytes):
        df = pl.read_excel(io.BytesIO(file_bytes))
        required_cols = ['Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»', 'Ð±Ñ€ÐµÐ½Ð´', 'ÐºÐ¾Ð»-Ð²Ð¾', 'Ñ†ÐµÐ½Ð°']
        for col in required_cols:
            if col not in df.columns:
                st.error(f"Ð¤Ð°Ð¹Ð» Ð´Ð¾Ð»Ð¶ÐµÐ½ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ñ‚ÑŒ ÐºÐ¾Ð»Ð¾Ð½ÐºÑƒ '{col}'")
                return
        df = df.select([
            pl.col('Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»'),
            pl.col('Ð±Ñ€ÐµÐ½Ð´'),
            pl.col('ÐºÐ¾Ð»-Ð²Ð¾').cast(pl.Int32),
            pl.col('Ñ†ÐµÐ½Ð°').cast(pl.Float64)
        ])
        for row in df.iter_rows():
            artikul = row[0]
            brand = row[1]
            qty = row[2]
            price = row[3]
            artikul_norm_series = self.normalize_key(pl.Series([artikul]))
            brand_norm_series = self.normalize_key(pl.Series([brand]))
            artikul_norm = artikul_norm_series[0] if len(artikul_norm_series) > 0 else ''
            brand_norm = brand_norm_series[0] if len(brand_norm_series) > 0 else ''
            self.conn.execute("""
                INSERT INTO price_list (artikul, brand, quantity, price)
                VALUES (?, ?, ?, ?)
                ON CONFLICT (artikul, brand) DO UPDATE SET
                quantity=excluded.quantity,
                price=excluded.price
            """, [artikul, brand, qty, price])
        st.success("ÐŸÑ€Ð°Ð¹Ñ-Ð»Ð¸ÑÑ‚ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½.")

    def set_markups(self, total_markup: float, brand_markups: Dict[str, float]):
        self.conn.execute("""
            UPDATE markup_settings SET total_markup = ?, brand_markup = ?
            WHERE id = 1
        """, [total_markup, json.dumps(brand_markups)])
        st.success("ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð½Ð°Ñ†ÐµÐ½ÐºÐ¸ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹.")

    def get_markups(self):
        row = self.conn.execute("SELECT total_markup, brand_markup FROM markup_settings WHERE id=1").fetchone()
        if row:
            total_markup = row[0]
            brand_markup = json.loads(row[1]) if row[1] else {}
            return total_markup, brand_markup
        return 0, {}

    def apply_markup(self, price, brand_norm=''):
        total_markup, brand_markups = self.get_markups()
        markup = total_markup
        if brand_norm and brand_norm in brand_markups:
            markup += brand_markups[brand_norm]
        return price * (1 + markup / 100)

    # --- Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ Ñ†ÐµÐ½ Ð¸ Ð½Ð°Ñ†ÐµÐ½ÐºÐ¸ ---
    def build_export_query(self, selected_columns: List[str] | None):
        standard_description = """Ð¡Ð¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ñ‚Ð¾Ð²Ð°Ñ€Ð°: Ð½Ð¾Ð²Ñ‹Ð¹ (Ð² ÑƒÐ¿Ð°ÐºÐ¾Ð²ÐºÐµ).
Ð’Ñ‹ÑÐ¾ÐºÐ¾ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð°Ð²Ñ‚Ð¾Ð·Ð°Ð¿Ñ‡Ð°ÑÑ‚Ð¸ Ð¸ Ð°Ð²Ñ‚Ð¾Ñ‚Ð¾Ð²Ð°Ñ€Ñ‹ â€” Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾Ðµ Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð²Ð°ÑˆÐµÐ³Ð¾ Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»Ñ. 
ÐžÐ±ÐµÑÐ¿ÐµÑ‡ÑŒÑ‚Ðµ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ, Ð´Ð¾Ð»Ð³Ð¾Ð²ÐµÑ‡Ð½Ð¾ÑÑ‚ÑŒ Ð¸ Ð²Ñ‹ÑÐ¾ÐºÑƒÑŽ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð²Ð°ÑˆÐµÐ³Ð¾ Ð°Ð²Ñ‚Ð¾ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð½Ð°ÑˆÐµÐ³Ð¾ ÑˆÐ¸Ñ€Ð¾ÐºÐ¾Ð³Ð¾ Ð°ÑÑÐ¾Ñ€Ñ‚Ð¸Ð¼ÐµÐ½Ñ‚Ð° Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¸ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ñ‹Ñ… Ð°Ð²Ñ‚Ð¾Ð·Ð°Ð¿Ñ‡Ð°ÑÑ‚ÐµÐ¹.

Ð’ Ð½Ð°ÑˆÐµÐ¼ ÐºÐ°Ñ‚Ð°Ð»Ð¾Ð³Ðµ Ð²Ñ‹ Ð½Ð°Ð¹Ð´ÐµÑ‚Ðµ Ñ‚Ð¾Ñ€Ð¼Ð¾Ð·Ð½Ñ‹Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹, Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹ (Ð¼Ð°ÑÐ»ÑÐ½Ñ‹Ðµ, Ð²Ð¾Ð·Ð´ÑƒÑˆÐ½Ñ‹Ðµ, ÑÐ°Ð»Ð¾Ð½Ð½Ñ‹Ðµ), ÑÐ²ÐµÑ‡Ð¸ Ð·Ð°Ð¶Ð¸Ð³Ð°Ð½Ð¸Ñ, Ñ€Ð°ÑÑ…Ð¾Ð´Ð½Ñ‹Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ñ‹, Ð°Ð²Ñ‚Ð¾Ñ…Ð¸Ð¼Ð¸ÑŽ, ÑÐ»ÐµÐºÑ‚Ñ€Ð¸ÐºÑƒ, Ð°Ð²Ñ‚Ð¾Ð¼Ð°ÑÐ»Ð°, Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚, Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ð´Ñ€ÑƒÐ³Ð¸Ðµ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑ‚ÑƒÑŽÑ‰Ð¸Ðµ, Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð°Ð¼ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð¸ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸. 

ÐœÑ‹ Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð±Ñ‹ÑÑ‚Ñ€ÑƒÑŽ Ð´Ð¾ÑÑ‚Ð°Ð²ÐºÑƒ, Ð²Ñ‹Ð³Ð¾Ð´Ð½Ñ‹Ðµ Ñ†ÐµÐ½Ñ‹ Ð¸ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½ÑƒÑŽ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸ÑŽ Ð´Ð»Ñ Ð»ÑŽÐ±Ð¾Ð³Ð¾ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° â€” Ð°Ð²Ñ‚Ð¾Ð»ÑŽÐ±Ð¸Ñ‚ÐµÐ»Ñ, ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸ÑÑ‚Ð° Ð¸Ð»Ð¸ Ð°Ð²Ñ‚Ð¾ÑÐµÑ€Ð²Ð¸ÑÐ°. 

Ð’Ñ‹Ð±Ð¸Ñ€Ð°Ð¹Ñ‚Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð»ÑƒÑ‡ÑˆÐµÐµ â€” Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚ÑŒ Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¾Ñ‚ Ð²ÐµÐ´ÑƒÑ‰Ð¸Ñ… Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÐµÐ¹."""
        columns_map = [
            ("ÐÑ€Ñ‚Ð¸ÐºÑƒÐ» Ð±Ñ€ÐµÐ½Ð´Ð°", 'r.artikul AS "ÐÑ€Ñ‚Ð¸ÐºÑƒÐ» Ð±Ñ€ÐµÐ½Ð´Ð°"'),
            ("Ð‘Ñ€ÐµÐ½Ð´", 'r.brand AS "Ð‘Ñ€ÐµÐ½Ð´"'),
            ("ÐÐ°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ", 'COALESCE(r.representative_name, r.analog_representative_name) AS "ÐÐ°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ"'),
            ("ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ð¾ÑÑ‚ÑŒ", 'COALESCE(r.representative_applicability, r.analog_representative_applicability) AS "ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ð¾ÑÑ‚ÑŒ"'),
            ("ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ", "CONCAT(COALESCE(r.description, ''), dt.text) AS \"ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ\""),
            ("ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ Ñ‚Ð¾Ð²Ð°Ñ€Ð°", 'COALESCE(r.representative_category, r.analog_representative_category) AS "ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ Ñ‚Ð¾Ð²Ð°Ñ€Ð°"'),
            ("ÐšÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ", 'r.multiplicity AS "ÐšÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ"'),
            ("Ð”Ð»Ð¸Ð½Ð½Ð°", 'COALESCE(r.length, r.analog_length) AS "Ð”Ð»Ð¸Ð½Ð½Ð°"'),
            ("Ð¨Ð¸Ñ€Ð¸Ð½Ð°", 'COALESCE(r.width, r.analog_width) AS "Ð¨Ð¸Ñ€Ð¸Ð½Ð°"'),
            ("Ð’Ñ‹ÑÐ¾Ñ‚Ð°", 'COALESCE(r.height, r.analog_height) AS "Ð’Ñ‹ÑÐ¾Ñ‚Ð°"'),
            ("Ð’ÐµÑ", 'COALESCE(r.weight, r.analog_weight) AS "Ð’ÐµÑ"'),
            ("Ð”Ð»Ð¸Ð½Ð½Ð°/Ð¨Ð¸Ñ€Ð¸Ð½Ð°/Ð’Ñ‹ÑÐ¾Ñ‚Ð°", "COALESCE(CASE WHEN r.dimensions_str IS NULL OR r.dimensions_str = '' OR UPPER(TRIM(r.dimensions_str)) = 'XX' THEN NULL ELSE r.dimensions_str END, r.analog_dimensions_str) AS \"Ð”Ð»Ð¸Ð½Ð½Ð°/Ð¨Ð¸Ñ€Ð¸Ð½Ð°/Ð’Ñ‹ÑÐ¾Ñ‚Ð°\""),
            ("OE Ð½Ð¾Ð¼ÐµÑ€", 'r.oe_list AS "OE Ð½Ð¾Ð¼ÐµÑ€"'),
            ("Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸", 'r.analog_list AS "Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸"'),
            ("Ð¡ÑÑ‹Ð»ÐºÐ° Ð½Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ", 'r.image_url AS "Ð¡ÑÑ‹Ð»ÐºÐ° Ð½Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ"'),
            ("Ð¦ÐµÐ½Ð° Ñ Ð½Ð°Ñ†ÐµÐ½ÐºÐ¾Ð¹", 'r.price_with_markup AS "Ð¦ÐµÐ½Ð° Ñ Ð½Ð°Ñ†ÐµÐ½ÐºÐ¾Ð¹"')
        ]

        if not selected_columns:
            selected_exprs = [expr for _, expr in columns_map]
        else:
            selected_exprs = [expr for name, expr in columns_map if name in selected_columns]
            if not selected_exprs:
                selected_exprs = [expr for _, expr in columns_map]

        ctes = f"""
        WITH DescriptionTemplate AS (
            SELECT CHR(10) || CHR(10) || $${standard_description}$$ AS text
        ),
        PartDetails AS (
            SELECT
                cr.artikul_norm,
                cr.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(o.oe_number, '''', ''), '[^0-9A-Za-zÐ-Ð¯Ð°-ÑÐÑ‘`\\-\\s]', '', 'g'), ', ') AS oe_list,
                ANY_VALUE(o.name) AS representative_name,
                ANY_VALUE(o.applicability) AS representative_applicability,
                ANY_VALUE(o.category) AS representative_category
            FROM cross_references cr
            JOIN oe_data o ON cr.oe_number_norm = o.oe_number_norm
            GROUP BY cr.artikul_norm, cr.brand_norm
        ),
        AllAnalogs AS (
            SELECT
                cr1.artikul_norm,
                cr1.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(p2.artikul, '''', ''), '[^0-9A-Za-zÐ-Ð¯Ð°-ÑÐÑ‘`\\-\\s]', '', 'g'), ', ') as analog_list
            FROM cross_references cr1
            JOIN cross_references cr2 ON cr1.oe_number_norm = cr2.oe_number_norm
            JOIN parts_data p2 ON cr2.artikul_norm = p2.artikul_norm AND cr2.brand_norm = p2.brand_norm
            WHERE (cr1.artikul_norm != p2.artikul_norm OR cr1.brand_norm != p2.brand_norm)
            GROUP BY cr1.artikul_norm, cr1.brand_norm
        ),
        InitialOENumbers AS (
            SELECT DISTINCT
                p.artikul_norm,
                p.brand_norm,
                cr.oe_number_norm
            FROM parts_data p
            LEFT JOIN cross_references cr ON p.artikul_norm = cr.artikul_norm AND p.brand_norm = cr.brand_norm
            WHERE cr.oe_number_norm IS NOT NULL
        ),
        Level1Analogs AS (
            SELECT DISTINCT
                i.artikul_norm AS source_artikul_norm,
                i.brand_norm AS source_brand_norm,
                cr2.artikul_norm AS related_artikul_norm,
                cr2.brand_norm AS related_brand_norm
            FROM InitialOENumbers i
            JOIN cross_references cr2 ON i.oe_number_norm = cr2.oe_number_norm
            WHERE NOT (i.artikul_norm = cr2.artikul_norm AND i.brand_norm = cr2.brand_norm)
        ),
        Level1OENumbers AS (
            SELECT DISTINCT
                l1.source_artikul_norm,
                l1.source_brand_norm,
                cr3.oe_number_norm
            FROM Level1Analogs l1
            JOIN cross_references cr3 ON l1.related_artikul_norm = cr3.artikul_norm 
                                        AND l1.related_brand_norm = cr3.brand_norm
            WHERE NOT EXISTS (
                SELECT 1 FROM InitialOENumbers i 
                WHERE i.artikul_norm = l1.source_artikul_norm 
                AND i.brand_norm = l1.source_brand_norm 
                AND i.oe_number_norm = cr3.oe_number_norm
            )
        ),
        Level2Analogs AS (
            SELECT DISTINCT
                loe.source_artikul_norm,
                loe.source_brand_norm,
                cr4.artikul_norm AS related_artikul_norm,
                cr4.brand_norm AS related_brand_norm
            FROM Level1OENumbers loe
            JOIN cross_references cr4 ON loe.oe_number_norm = cr4.oe_number_norm
            WHERE NOT (loe.source_artikul_norm = cr4.artikul_norm AND loe.source_brand_norm = cr4.brand_norm)
        ),
        AllRelatedParts AS (
            SELECT DISTINCT source_artikul_norm, source_brand_norm, related_artikul_norm, related_brand_norm
            FROM Level1Analogs
            UNION
            SELECT DISTINCT source_artikul_norm, source_brand_norm, related_artikul_norm, related_brand_norm
            FROM Level2Analogs
        ),
        AggregatedAnalogData AS (
            SELECT
                arp.source_artikul_norm AS artikul_norm,
                arp.source_brand_norm AS brand_norm,
                MAX(CASE WHEN p2.length IS NOT NULL THEN p2.length ELSE NULL END) AS length,
                MAX(CASE WHEN p2.width IS NOT NULL THEN p2.width ELSE NULL END) AS width,
                MAX(CASE WHEN p2.height IS NOT NULL THEN p2.height ELSE NULL END) AS height,
                MAX(CASE WHEN p2.weight IS NOT NULL THEN p2.weight ELSE NULL END) AS weight,
                ANY_VALUE(CASE WHEN p2.dimensions_str IS NOT NULL 
                               AND p2.dimensions_str != '' 
                               AND UPPER(TRIM(p2.dimensions_str)) != 'XX' 
                          THEN p2.dimensions_str ELSE NULL END) AS dimensions_str,
                ANY_VALUE(CASE WHEN pd2.representative_name IS NOT NULL AND pd2.representative_name != '' THEN pd2.representative_name ELSE NULL END) AS representative_name,
                ANY_VALUE(CASE WHEN pd2.representative_applicability IS NOT NULL AND pd2.representative_applicability != '' THEN pd2.representative_applicability ELSE NULL END) AS representative_applicability,
                ANY_VALUE(CASE WHEN pd2.representative_category IS NOT NULL AND pd2.representative_category != '' THEN pd2.representative_category ELSE NULL END) AS representative_category
            FROM AllRelatedParts arp
            JOIN parts_data p2 ON arp.related_artikul_norm = p2.artikul_norm AND arp.related_brand_norm = p2.brand_norm
            LEFT JOIN PartDetails pd2 ON p2.artikul_norm = pd2.artikul_norm AND p2.brand_norm = pd2.brand_norm
            GROUP BY arp.source_artikul_norm, arp.source_brand_norm
        ),
        RankedData AS (
            SELECT
                p.artikul,
                p.brand,
                p.description,
                p.multiplicity,
                p.length,
                p.width,
                p.height,
                p.weight,
                p.dimensions_str,
                p.image_url,
                pd.representative_name,
                pd.representative_applicability,
                pd.representative_category,
                pd.oe_list,
                aa.analog_list,
                p_analog.length AS analog_length,
                p_analog.width AS analog_width,
                p_analog.height AS analog_height,
                p_analog.weight AS analog_weight,
                p_analog.dimensions_str AS analog_dimensions_str,
                p_analog.representative_name AS analog_representative_name,
                p_analog.representative_applicability AS analog_representative_applicability,
                p_analog.representative_category AS analog_representative_category,
                ROW_NUMBER() OVER(PARTITION BY p.artikul_norm, p.brand_norm ORDER BY pd.representative_name DESC NULLS LAST, pd.oe_list DESC NULLS LAST) as rn
            FROM parts_data p
            LEFT JOIN PartDetails pd ON p.artikul_norm = pd.artikul_norm AND p.brand_norm = pd.brand_norm
            LEFT JOIN AllAnalogs aa ON p.artikul_norm = aa.artikul_norm AND p.brand_norm = aa.brand_norm
            LEFT JOIN AggregatedAnalogData p_analog ON p.artikul_norm = p_analog.artikul_norm AND p.brand_norm = p_analog.brand_norm
        )
        """

        select_clause = ",\n            ".join(selected_exprs)

        # Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ
        query = ctes + r"""
        SELECT
            """ + select_clause + r"""
        FROM RankedData r
        CROSS JOIN DescriptionTemplate dt
        WHERE r.rn = 1
        ORDER BY r.brand, r.artikul
        """

        return query

    def export_to_csv_optimized(self, output_path: str, selected_columns: List[str] | None = None):
        total_records = self.conn.execute("SELECT count(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data) AS t").fetchone()[0]
        if total_records == 0:
            st.warning("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°")
            return False
        st.info(f"ðŸ“¤ Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ {total_records:,} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð² CSV...")
        try:
            query = self.build_export_query(selected_columns)
            df = self.conn.execute(query).pl()

            # Ð’Ñ‹Ñ€Ð¾Ð²Ð½ÑÑ‚ÑŒ Ñ†ÐµÐ½Ñ‹ Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ Ð½Ð°Ñ†ÐµÐ½ÐºÐ¸
            if 'price' in df.columns:
                df = df.with_columns(
                    pl.col('price').apply(lambda p: self.apply_markup(p, brand_norm='')).alias('Ð¦ÐµÐ½Ð° Ñ Ð½Ð°Ñ†ÐµÐ½ÐºÐ¾Ð¹')
                )

            # Ð’ ÑÐ»ÑƒÑ‡Ð°Ðµ, ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ° 'Ð¦ÐµÐ½Ð° Ñ Ð½Ð°Ñ†ÐµÐ½ÐºÐ¾Ð¹', Ð¿ÐµÑ€ÐµÐ¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ñ‚ÑŒ
            if 'Ð¦ÐµÐ½Ð° Ñ Ð½Ð°Ñ†ÐµÐ½ÐºÐ¾Ð¹' in df.columns:
                df = df.rename({'Ð¦ÐµÐ½Ð° Ñ Ð½Ð°Ñ†ÐµÐ½ÐºÐ¾Ð¹': 'price_with_markup'})

            # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ñ… ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº
            for col_name in ["Ð”Ð»Ð¸Ð½Ð½Ð°", "Ð¨Ð¸Ñ€Ð¸Ð½Ð°", "Ð’Ñ‹ÑÐ¾Ñ‚Ð°", "Ð’ÐµÑ", "Ð”Ð»Ð¸Ð½Ð½Ð°/Ð¨Ð¸Ñ€Ð¸Ð½Ð°/Ð’Ñ‹ÑÐ¾Ñ‚Ð°", "ÐšÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ"]:
                if col_name in df.columns:
                    df = df.with_columns(
                        pl.when(pl.col(col_name).is_not_null())
                        .then(pl.col(col_name).cast(pl.Utf8))
                        .otherwise("")
                        .alias(col_name)
                    )

            buf = io.StringIO()
            df.write_csv(buf, separator=';')
            csv_bytes = buf.getvalue().encode('utf-8')
            with open(output_path, 'wb') as f:
                f.write(b'\xef\xbb\xbf')
                f.write(csv_bytes)
            size_mb = os.path.getsize(output_path) / (1024 * 1024)
            st.success(f"âœ… Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹: {output_path} ({size_mb:.2f} ÐœÐ‘)")
            return True
        except Exception as e:
            st.exception(e)
            return False

    def export_to_excel(self, output_path: Path, selected_columns: List[str] | None = None):
        total_records = self.conn.execute("SELECT count(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data) AS t").fetchone()[0]
        if total_records == 0:
            st.warning("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°")
            return False, None

        num_files = (total_records + EXCEL_ROW_LIMIT - 1) // EXCEL_ROW_LIMIT
        progress = st.progress(0, text=f"ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° {num_files} Ñ„Ð°Ð¹Ð»Ð°(Ð¾Ð²)...")
        exported_files = []

        base_query = self.build_export_query(selected_columns)

        for i in range(num_files):
            offset = i * EXCEL_ROW_LIMIT
            query = f"{base_query} LIMIT {EXCEL_ROW_LIMIT} OFFSET {offset}"
            df = self.conn.execute(query).pl()

            # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ†ÐµÐ½
            if 'price' in df.columns:
                df = df.with_columns(
                    pl.col('price').apply(lambda p: self.apply_markup(p, brand_norm='')).alias('Ð¦ÐµÐ½Ð° Ñ Ð½Ð°Ñ†ÐµÐ½ÐºÐ¾Ð¹')
                )

            # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ñ… ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº
            for col_name in ["Ð”Ð»Ð¸Ð½Ð½Ð°", "Ð¨Ð¸Ñ€Ð¸Ð½Ð°", "Ð’Ñ‹ÑÐ¾Ñ‚Ð°", "Ð’ÐµÑ", "Ð”Ð»Ð¸Ð½Ð½Ð°/Ð¨Ð¸Ñ€Ð¸Ð½Ð°/Ð’Ñ‹ÑÐ¾Ñ‚Ð°", "ÐšÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ"]:
                if col_name in df.columns:
                    df = df.with_columns(
                        pl.when(pl.col(col_name).is_not_null())
                        .then(pl.col(col_name).cast(pl.Utf8))
                        .otherwise("")
                        .alias(col_name)
                    )

            file_name = output_path.with_name(f"{output_path.stem}_part_{i + 1}.xlsx")
            df.write_excel(str(file_name))
            exported_files.append(file_name)
            progress.progress((i + 1) / num_files)

        # ÐÑ€Ñ…Ð¸Ð²Ð°Ñ†Ð¸Ñ, ÐµÑÐ»Ð¸ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾
        if len(exported_files) > 1:
            zip_path = output_path.with_suffix('.zip')
            with zipfile.ZipFile(zip_path, 'w') as zf:
                for f in exported_files:
                    zf.write(f, arcname=f.name)
                    os.remove(f)
            final_path = zip_path
        else:
            final_path = exported_files[0]
            if final_path != output_path:
                os.rename(final_path, output_path)
                final_path = output_path
        size_mb = os.path.getsize(final_path) / (1024 * 1024)
        st.success(f"âœ… Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½: {final_path.name} ({size_mb:.2f} ÐœÐ‘)")
        return True, final_path

    def export_to_parquet(self, output_path: str, selected_columns: List[str] | None = None):
        total_records = self.conn.execute("SELECT count(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data) AS t").fetchone()[0]
        if total_records == 0:
            st.warning("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°")
            return False
        st.info(f"ðŸ“¤ Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ {total_records:,} Ð² Parquet...")
        try:
            query = self.build_export_query(selected_columns)
            df = self.conn.execute(query).pl()
            if 'price' in df.columns:
                df = df.with_columns(
                    pl.col('price').apply(lambda p: self.apply_markup(p, brand_norm='')).alias('Ð¦ÐµÐ½Ð° Ñ Ð½Ð°Ñ†ÐµÐ½ÐºÐ¾Ð¹')
                )
            df.write_parquet(output_path)
            size_mb = os.path.getsize(output_path) / (1024 * 1024)
            st.success(f"âœ… Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½: {output_path} ({size_mb:.2f} ÐœÐ‘)")
            return True
        except Exception as e:
            st.exception(e)
            return False

    # --- Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹Ñ ---
    def show_export_interface(self):
        st.header("ðŸ“¤ Ð£Ð¼Ð½Ñ‹Ð¹ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ…")
        total_records = self.conn.execute("SELECT count(DISTINCT (artikul_norm, brand_norm)) FROM parts_data").fetchone()[0]
        st.info(f"Ð’ÑÐµÐ³Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð´Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°: {total_records:,}")
        if total_records == 0:
            st.warning("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°")
            return

        available_columns = [
            "ÐÑ€Ñ‚Ð¸ÐºÑƒÐ» Ð±Ñ€ÐµÐ½Ð´Ð°", "Ð‘Ñ€ÐµÐ½Ð´", "ÐÐ°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ", "ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ð¾ÑÑ‚ÑŒ", "ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ",
            "ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ Ñ‚Ð¾Ð²Ð°Ñ€Ð°", "ÐšÑ€Ð°Ñ‚Ð½Ð¾ÑÑ‚ÑŒ", "Ð”Ð»Ð¸Ð½Ð½Ð°", "Ð¨Ð¸Ñ€Ð¸Ð½Ð°", "Ð’Ñ‹ÑÐ¾Ñ‚Ð°",
            "Ð’ÐµÑ", "Ð”Ð»Ð¸Ð½Ð½Ð°/Ð¨Ð¸Ñ€Ð¸Ð½Ð°/Ð’Ñ‹ÑÐ¾Ñ‚Ð°", "OE Ð½Ð¾Ð¼ÐµÑ€", "Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸", "Ð¡ÑÑ‹Ð»ÐºÐ° Ð½Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ", "Ð¦ÐµÐ½Ð° Ñ Ð½Ð°Ñ†ÐµÐ½ÐºÐ¾Ð¹"
        ]
        selected_columns = st.multiselect("Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ ÑÑ‚Ð¾Ð»Ð±Ñ†Ñ‹ Ð´Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°", options=available_columns, default=available_columns)

        # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð² Ð¸ÑÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ð¹
        st.subheader("Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ Ð¸ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼Ñ‹Ð¼ Ð½Ð°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸ÑÐ¼")
        exclude_names_input = st.text_input("Ð˜ÑÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸, Ñ€Ð°Ð·Ð´ÐµÐ»Ñ‘Ð½Ð½Ñ‹Ðµ | (Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº)")
        exclude_names = [name.strip() for name in exclude_names_input.split('|')] if exclude_names_input else []

        # Ð’Ñ‹Ð±Ð¾Ñ€ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð°
        export_format = st.radio("Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°", ["CSV", "Excel (.xlsx)", "Parquet"], index=0)

        # ÐžÐ±Ñ‰Ð°Ñ Ð½Ð°Ñ†ÐµÐ½ÐºÐ°
        st.subheader("ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð½Ð°Ñ†ÐµÐ½ÐºÐ¸")
        total_markup = st.slider("ÐžÐ±Ñ‰Ð°Ñ Ð½Ð°Ñ†ÐµÐ½ÐºÐ° (%)", 0, 100, 0)
        brand_markups: Dict[str, float] = {}
        if st.checkbox("ÐÐ°ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ Ð½Ð°Ñ†ÐµÐ½ÐºÐ¸ Ð¿Ð¾ Ð±Ñ€ÐµÐ½Ð´Ð°Ð¼"):
            brands = self.conn.execute("SELECT DISTINCT brand, brand_norm FROM parts_data WHERE brand IS NOT NULL").fetchall()
            for b, bn in brands:
                markup = st.slider(f"ÐÐ°Ñ†ÐµÐ½ÐºÐ° Ð´Ð»Ñ Ð±Ñ€ÐµÐ½Ð´Ð° '{b}'", 0, 100, 0)
                brand_markups[bn] = markup
        if st.button("Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð½Ð°Ñ†ÐµÐ½ÐºÐ¸"):
            self.set_markups(total_markup, brand_markups)

        # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÐºÐ½Ð¾Ð¿ÐºÐ¸
        if st.button("ðŸš€ Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ", type="primary"):
            output_path = self.data_dir / "auto_parts_export"
            if export_format == "CSV":
                output_file = str(output_path.with_suffix('.csv'))
                success = self.export_to_csv_optimized(output_file, selected_columns)
                if success:
                    with open(output_file, "rb") as f:
                        st.download_button("ðŸ“¥ Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ CSV", f, "auto_parts_report.csv", "text/csv")
            elif export_format == "Excel (.xlsx)":
                output_file = output_path.with_suffix('.xlsx')
                success, final_path = self.export_to_excel(output_file, selected_columns)
                if success and final_path and final_path.exists():
                    with open(final_path, "rb") as f:
                        st.download_button("ðŸ“¥ Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ XLSX", f, final_path.name, "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
            elif export_format == "Parquet":
                output_file = str(output_path.with_suffix('.parquet'))
                success = self.export_to_parquet(output_file, selected_columns)
                if success:
                    with open(output_file, "rb") as f:
                        st.download_button("ðŸ“¥ Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ Parquet", f, "auto_parts_report.parquet", "application/octet-stream")

    # --- ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸ÑÐ¼Ð¸ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ ---
    def delete_by_brand(self, brand_norm: str) -> int:
        try:
            count = self.conn.execute("SELECT COUNT(*) FROM parts_data WHERE brand_norm = ?", [brand_norm]).fetchone()[0]
            if count:
                self.conn.execute("DELETE FROM parts_data WHERE brand_norm = ?", [brand_norm])
                self.conn.execute("DELETE FROM cross_references WHERE (artikul_norm, brand_norm) NOT IN (SELECT artikul_norm, brand_norm FROM parts_data)")
            return count
        except Exception as e:
            logger.exception(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ Ð¿Ð¾ Ð±Ñ€ÐµÐ½Ð´Ñƒ: {e}")
            return 0

    def delete_by_artikul(self, artikul_norm: str) -> int:
        try:
            count = self.conn.execute("SELECT COUNT(*) FROM parts_data WHERE artikul_norm = ?", [artikul_norm]).fetchone()[0]
            if count:
                self.conn.execute("DELETE FROM parts_data WHERE artikul_norm = ?", [artikul_norm])
                self.conn.execute("DELETE FROM cross_references WHERE (artikul_norm, brand_norm) NOT IN (SELECT artikul_norm, brand_norm FROM parts_data)")
            return count
        except Exception as e:
            logger.exception(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ Ð¿Ð¾ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð°: {e}")
            return 0

# --- ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹Ñ ---
def main():
    st.title("ðŸš— AutoParts Catalog - ÐŸÑ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð»Ñ 10+ Ð¼Ð»Ð½ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
    st.markdown("""
    ### ðŸ’ª ÐœÐ¾Ñ‰Ð½Ð°Ñ Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð° Ð´Ð»Ñ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð¼Ð¸ Ð¾Ð±ÑŠÐµÐ¼Ð°Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð°Ð²Ñ‚Ð¾Ð·Ð°Ð¿Ñ‡Ð°ÑÑ‚ÐµÐ¹
    - **Ð˜Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ**: Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐ¹Ñ‚Ðµ Ð½Ð¾Ð²Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾.
    - **ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…**: 5 Ñ‚Ð¸Ð¿Ð¾Ð² Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð² ÐµÐ´Ð¸Ð½ÑƒÑŽ Ð±Ð°Ð·Ñƒ.
    - **ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ**: DuckDB.
    - **Ð£Ð¼Ð½Ñ‹Ð¹ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚**: CSV, Excel, Parquet.
    """)

    catalog = HighVolumeAutoPartsCatalog()

    # ÐÐ°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ñ
    menu = st.sidebar.radio("Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ñ€Ð°Ð·Ð´ÐµÐ»", ["Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…", "Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚", "Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°", "Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸"])

    if menu == "Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…":
        # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ñ„Ð°Ð¹Ð»Ð¾Ð²
        is_empty_db = catalog.get_total_records() == 0
        st.subheader("Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…")
        col1, col2 = st.columns(2)
        with col1:
            file_oe = st.file_uploader("ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ (OE)", type=['xlsx', 'xls'])
            file_cross = st.file_uploader("ÐšÑ€Ð¾ÑÑÑ‹ (OE -> ÐÑ€Ñ‚Ð¸ÐºÑƒÐ»)", type=['xlsx', 'xls'])
            file_barcode = st.file_uploader("Ð¨Ñ‚Ñ€Ð¸Ñ…-ÐºÐ¾Ð´Ñ‹", type=['xlsx', 'xls'])
        with col2:
            file_dim = st.file_uploader("Ð’ÐµÑÐ¾Ð³Ð°Ð±Ð°Ñ€Ð¸Ñ‚Ñ‹", type=['xlsx', 'xls'])
            file_img = st.file_uploader("Ð˜Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ", type=['xlsx', 'xls'])

        files_map = {
            'oe': file_oe,
            'cross': file_cross,
            'barcode': file_barcode,
            'dimensions': file_dim,
            'images': file_img
        }

        if st.button("ðŸš€ ÐÐ°Ñ‡Ð°Ñ‚ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÑƒ"):
            paths = {}
            for ftype, uploaded in files_map.items():
                if uploaded:
                    filename = f"{ftype}_{int(time.time())}_{uploaded.name}"
                    path = catalog.data_dir / filename
                    with open(path, "wb") as f:
                        f.write(uploaded.getvalue())
                    paths[ftype] = str(path)

            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð´Ð»Ñ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ð¾Ð¹
            if is_empty_db:
                missing = [f for f in ['oe', 'cross', 'barcode', 'dimensions', 'images'] if f not in paths]
                if missing:
                    st.error(f"Ð”Ð»Ñ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð½ÑƒÐ¶Ð½Ñ‹ Ð²ÑÐµ 5 Ñ„Ð°Ð¹Ð»Ð¾Ð². ÐžÑ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚: {', '.join(missing)}")
                elif len(paths) == 5:
                    stats = catalog.merge_all_data_parallel(paths)
                    if stats:
                        st.subheader("ðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°")
                        st.metric("Ð’Ñ€ÐµÐ¼Ñ", f"{stats.get('processing_time', 0):.2f} ÑÐµÐº")
                        st.metric("ÐÑ€Ñ‚Ð¸ÐºÑƒÐ»Ð¾Ð²", f"{stats.get('total_records', 0):,}")
                else:
                    st.warning("Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ Ð½Ðµ Ð²ÑÐµ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹.")
            else:
                if len(paths) > 0:
                    stats = catalog.merge_all_data_parallel(paths)
                    if stats:
                        st.subheader("ðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°")
                        st.metric("Ð’Ñ€ÐµÐ¼Ñ", f"{stats.get('processing_time', 0):.2f} ÑÐµÐº")
                        st.metric("ÐÑ€Ñ‚Ð¸ÐºÑƒÐ»Ð¾Ð²", f"{stats.get('total_records', 0):,}")
                else:
                    st.info("Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ñ„Ð°Ð¹Ð»Ñ‹ Ð´Ð»Ñ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ.")

    elif menu == "Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚":
        catalog.show_export_interface()

    elif menu == "Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°":
        st.header("ðŸ“ˆ Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°")
        with st.spinner("Ð¡Ð±Ð¾Ñ€ Ð´Ð°Ð½Ð½Ñ‹Ñ…..."):
            stats = catalog.get_statistics()
        st.metric("Ð’ÑÐµÐ³Ð¾ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ð°", f"{stats.get('total_parts', 0):,}")
        st.metric("OE", f"{stats.get('total_oe', 0):,}")
        st.metric("Ð‘Ñ€ÐµÐ½Ð´Ð¾Ð²", f"{stats.get('total_brands', 0):,}")
        st.subheader("Ð¢Ð¾Ð¿ Ð±Ñ€ÐµÐ½Ð´Ð¾Ð²")
        st.dataframe(stats.get('top_brands', pl.DataFrame()).to_pandas())
        st.subheader("Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¿Ð¾ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸ÑÐ¼")
        st.bar_chart(stats.get('categories', pl.DataFrame()).to_pandas().set_index('category'))

    elif menu == "Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸":
        st.header("ðŸ—‘ï¸ Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ")
        op = st.radio("Ð”ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ", ["Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð¿Ð¾ Ð±Ñ€ÐµÐ½Ð´Ñƒ", "Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð¿Ð¾ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ñƒ"])
        if op == "Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð¿Ð¾ Ð±Ñ€ÐµÐ½Ð´Ñƒ":
            brands = catalog.conn.execute("SELECT DISTINCT brand, brand_norm FROM parts_data WHERE brand IS NOT NULL").fetchall()
            if brands:
                brand_list = [b for b, bn in brands]
                selected_b = st.selectbox("Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð±Ñ€ÐµÐ½Ð´", brand_list)
                # ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ normalized
                bn_row = catalog.conn.execute("SELECT brand_norm FROM parts_data WHERE brand=?", [selected_b]).fetchone()
                bn = bn_row[0] if bn_row else ''
                count_del = catalog.delete_by_brand(bn)
                st.success(f"Ð£Ð´Ð°Ð»ÐµÐ½Ð¾ {count_del} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð´Ð»Ñ Ð±Ñ€ÐµÐ½Ð´Ð° {selected_b}")
            else:
                st.info("ÐÐµÑ‚ Ð±Ñ€ÐµÐ½Ð´Ð¾Ð² Ð´Ð»Ñ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ.")
        else:
            art_input = st.text_input("ÐÑ€Ñ‚Ð¸ÐºÑƒÐ» Ð´Ð»Ñ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ")
            if art_input:
                norm_series = catalog.normalize_key(pl.Series([art_input]))
                artikul_norm = norm_series[0] if len(norm_series) > 0 else ''
                count = catalog.delete_by_artikul(artikul_norm)
                st.success(f"Ð£Ð´Ð°Ð»ÐµÐ½Ð¾ {count} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð¿Ð¾ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ñƒ {art_input}")

if __name__ == "__main__":
    main()
