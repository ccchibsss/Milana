import polars as pl
import duckdb
import streamlit as st
import io
import os
from pathlib import Path
from typing import Dict, List, Optional
import time
import warnings
import logging
import difflib
import textwrap
from urllib.parse import urlparse
import boto3  # –î–ª—è AWS S3
import requests  # –î–ª—è –¥—Ä—É–≥–∏—Ö –æ–±–ª–∞—á–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)

EXCEL_ROW_LIMIT = 1_000_000

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±–ª–∞—á–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ ---
CLOUD_PROVIDER = os.getenv("CLOUD_PROVIDER", "s3")  # s3, gcs, azure
S3_BUCKET = os.getenv("S3_BUCKET")
S3_REGION = os.getenv("S3_REGION", "us-east-1")
# –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –¥—Ä—É–≥–∏—Ö: GCS_BUCKET, AZURE_CONTAINER –∏ —Ç.–¥.

class HighVolumeAutoPartsCatalog:
    def __init__(self):
        self.data_dir = Path("./auto_parts_data")
        self.data_dir.mkdir(exist_ok=True)
        self.db_path = self.data_dir / "catalog.duckdb"
        self.sync_from_cloud()  # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∏–∑ –æ–±–ª–∞–∫–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        self.conn = duckdb.connect(str(self.db_path))
        self.setup_database()
        st.set_page_config(page_title="AutoParts Catalog 10M+", layout="wide", page_icon="üöó")

        self.prices_df = pl.DataFrame()
        self.price_markup = 1.0  # 1.0 = 0%
        self.brand_markup = {}   # –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∏–º—è –±—Ä–µ–Ω–¥–∞ ‚Üí –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç
        self.export_exclusions = []  # –°–ø–∏—Å–æ–∫ —à–∞–±–ª–æ–Ω–æ–≤ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ
        self.category_mapping = {}  # –†—É—á–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ: –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ ‚Üí –∫–∞—Ç–µ–≥–æ—Ä–∏—è

    def sync_from_cloud(self):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –±–∞–∑—É –∏–∑ –æ–±–ª–∞–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ."""
        if not self.db_path.exists():
            try:
                if CLOUD_PROVIDER == "s3" and S3_BUCKET:
                    s3 = boto3.client("s3", region_name=S3_REGION)
                    s3.download_file(S3_BUCKET, "catalog.duckdb", str(self.db_path))
                    st.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ S3.")
                # elif ... –¥—Ä—É–≥–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
            except Exception as e:
                st.warning(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É –∏–∑ –æ–±–ª–∞–∫–∞: {e}. –°–æ–∑–¥–∞—ë—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–∞—è.")

    def sync_to_cloud(self):
        """–í—ã–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –≤ –æ–±–ª–∞–∫–æ –ø–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π."""
        try:
            if CLOUD_PROVIDER == "s3" and S3_BUCKET:
                s3 = boto3.client("s3", region_name=S3_REGION)
                s3.upload_file(str(self.db_path), S3_BUCKET, "catalog.duckdb")
                st.info("‚òÅÔ∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –æ–±–ª–∞–∫–æ.")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –æ–±–ª–∞–∫–æ: {e}")

    def setup_database(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –≤ DuckDB, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç."""
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER DEFAULT 1,
                barcode VARCHAR,
                length DOUBLE, 
                width DOUBLE,
                height DOUBLE, 
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                price DOUBLE,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS prices (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                price DOUBLE,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)

    def normalize_key(self, key_series: pl.Series) -> pl.Series:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π: –æ—á–∏—Å—Ç–∫–∞ –∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É."""
        return (
            key_series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
            .str.to_lowercase()
        )

    def clean_values(self, value_series: pl.Series) -> pl.Series:
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–±–µ–∑ to_lowercase)."""
        return (
            value_series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
        )

    def detect_category(self, name_series: pl.Series) -> pl.Series:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–∏."""
        base_categories = {
            '–§–∏–ª—å—Ç—Ä': '—Ñ–∏–ª—å—Ç—Ä|filter',
            '–¢–æ—Ä–º–æ–∑–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞': '—Ç–æ—Ä–º–æ–∑|brake|–∫–æ–ª–æ–¥–∫|–¥–∏—Å–∫|—Å—É–ø–ø–æ—Ä—Ç',
            '–ü–æ–¥–≤–µ—Å–∫–∞': '–∞–º–æ—Ä—Ç–∏–∑–∞—Ç–æ—Ä|—Å—Ç–æ–π–∫|spring|–ø–æ–¥–≤–µ—Å–∫|—Ä—ã—á–∞–≥|—à–∞—Ä–æ–≤–∞—è|—Å–∞–π–ª–µ–Ω—Ç–±–ª–æ–∫|—Å—Ç—É–ø–∏—Ü|–ø–æ–¥—à–∏–ø–Ω–∏–∫',
            '–î–≤–∏–≥–∞—Ç–µ–ª—å': '–¥–≤–∏–≥–∞—Ç–µ–ª—å|engine|—Å–≤–µ—á|–ø–æ—Ä—à–µ–Ω—å|–∫–ª–∞–ø–∞–Ω',
            '–¢—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è': '—Ç—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è|—Å—Ü–µ–ø–ª–µ–Ω|–∫–æ—Ä–æ–±–∫|transmission',
            '–≠–ª–µ–∫—Ç—Ä–∏–∫–∞': '–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä|–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä|—Å—Ç–∞—Ä—Ç–µ—Ä|–ø—Ä–æ–≤–æ–¥|–ª–∞–º–ø',
            '–†—É–ª–µ–≤–æ–µ': '—Ä—É–ª–µ–≤–æ–π|—Ç—è–≥–∞|–Ω–∞–∫–æ–Ω–µ—á–Ω–∏–∫|steering',
            '–í—ã—Ö–ª–æ–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞': '–≥–ª—É—à–∏—Ç–µ–ª—å|–∫–∞—Ç–∞–ª–∏–∑–∞—Ç–æ—Ä|–≤—ã—Ö–ª–æ–ø|exhaust',
            '–û—Ö–ª–∞–∂–¥–µ–Ω–∏–µ': '—Ä–∞–¥–∏–∞—Ç–æ—Ä|–≤–µ–Ω—Ç–∏–ª—è—Ç–æ—Ä|—Ç–µ—Ä–º–æ—Å—Ç–∞—Ç|cooling',
            '–¢–æ–ø–ª–∏–≤–æ': '—Ç–æ–ø–ª–∏–≤–Ω—ã–π|–±–µ–Ω–∑–æ–Ω–∞—Å–æ—Å|—Ñ–æ—Ä—Å—É–Ω–∫|fuel',
        }
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        all_patterns = {**base_categories, **self.category_mapping}
        name_lower = name_series.str.to_lowercase()
        expr = pl.when(pl.lit(False)).then(pl.lit(None))
        for cat, pattern in all_patterns.items():
            expr = expr.when(name_lower.str.contains(pattern)).then(pl.lit(cat))
        return expr.otherwise(pl.lit('–†–∞–∑–Ω–æ–µ')).alias('category')

    def detect_columns(self, actual: List[str], expected: List[str]) -> Dict[str, str]:
        """–≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤ –ø–æ –∏–º–µ–Ω–∏."""
        mapping = {}
        for exp in expected:
            matches = difflib.get_close_matches(exp, [a.lower() for a in actual], n=1, cutoff=0.6)
            if matches:
                orig_col = actual[[a.lower() for a in actual].index(matches[0])]
                mapping[orig_col] = exp
        return mapping

    def read_and_prepare_file(self, file_path: str, file_type: str) -> pl.DataFrame:
        """–ß—Ç–µ–Ω–∏–µ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ Excel-—Ñ–∞–π–ª–∞."""
        try:
            df = pl.read_excel(file_path, engine='calamine')
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return pl.DataFrame()

        schemas = {
            'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
            'barcode': ['brand', 'artikul', 'barcode', 'multiplicity'],
            'dimensions': ['artikul', 'brand', 'length', 'width', 'height', 'weight', 'dimensions_str'],
            'images': ['artikul', 'brand', 'image_url'],
            'cross': ['oe_number', 'artikul', 'brand'],
            'price': ['–∞—Ä—Ç–∏–∫—É–ª', '–±—Ä–µ–Ω–¥', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', '—Ü–µ–Ω–∞']  # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–∞–π—Å–∞
        }
        expected_cols = schemas.get(file_type, [])
        column_mapping = self.detect_columns(df.columns, expected_cols)
        df = df.rename(column_mapping)

        for col in ['artikul', 'brand', 'oe_number']:
            if col in df.columns:
                df = df.with_columns(self.clean_values(pl.col(col)).alias(col))

        key_cols = [c for c in ['oe_number', 'artikul', 'brand'] if c in df.columns]
        if key_cols:
            df = df.unique(subset=key_cols, keep='first')

        for col in ['artikul', 'brand', 'oe_number']:
            norm_col = f"{col}_norm"
            if col in df.columns:
                df = df.with_columns(self.normalize_key(pl.col(col)).alias(norm_col))

        return df

    def upsert_data(self, table_name: str, df: pl.DataFrame, pk: List[str]):
        """–í—Å—Ç–∞–≤–∫–∞ –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü—É DuckDB."""
        if df.is_empty():
            return
        df = df.unique()
        cols = df.columns
        pk_str = ", ".join(f'"{c}"' for c in pk)
        temp_view = f"temp_{table_name}_{int(time.time() * 1000)}"
        self.conn.register(temp_view, df.to_arrow())

        update_cols = [c for c in cols if c not in pk]
        if update_cols:
            update_clause = ", ".join([f'"{c}" = excluded."{c}"' for c in update_cols])
            on_conflict = f"DO UPDATE SET {update_clause}"
        else:
            on_conflict = "DO NOTHING"

        query = f"""
        INSERT INTO {table_name}
        SELECT * FROM {temp_view}
        ON CONFLICT ({pk_str}) {on_conflict};
        """
        try:
            self.conn.execute(query)
        finally:
            self.conn.unregister(temp_view)

    def process_and_load_data(self, dataframes: Dict[str, pl.DataFrame]):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –≤ –±–∞–∑—É."""
        # OE –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if 'oe' in dataframes:
            df_oe = dataframes['oe'].filter(pl.col('oe_number_norm') != "")
            oe_data = df_oe.select(['oe_number_norm', 'oe_number', 'name', 'applicability']).unique(subset=['oe_number_norm'])
            oe_data = oe_data.with_columns(self.detect_category(pl.col('name')))
            self.upsert_data('oe_data', oe_data, ['oe_number_norm'])

            cross_df = df_oe.filter(pl.col('artikul_norm') != "").select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        # –ö—Ä–æ—Å—Å—ã
        if 'cross' in dataframes:
            df_cross = dataframes['cross'].filter(
                (pl.col('oe_number_norm') != "") & (pl.col('artikul_norm') != "")
            )
            cross_df = df_cross.select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        # –°–±–æ—Ä–∫–∞ parts_data
        file_priority = ['oe', 'barcode', 'images', 'dimensions']
        key_files = {k: v for k, v in dataframes.items() if k in file_priority and not v.is_empty()}

        if not key_files:
            return

        base_artikuls = pl.concat([
            df.select(['artikul_norm', 'brand_norm', 'artikul', 'brand'])
            for df in key_files.values()
            if {'artikul_norm', 'brand_norm'} <= set(df.columns)
        ]).unique(subset=['artikul_norm', 'brand_norm'])

        parts_df = base_artikuls

        for ftype in file_priority:
            if ftype not in key_files:
                continue
            df = key_files[ftype]
            join_cols = [c for c in df.columns if c not in parts_df.columns and c not in ['artikul', 'brand', 'artikul_norm', 'brand_norm']]
            if not join_cols:
                continue
            subset = df.select(['artikul_norm', 'brand_norm'] + join_cols).unique(subset=['artikul_norm', 'brand_norm'])
            parts_df = parts_df.join(subset, on=['artikul_norm', 'brand_norm'], how='left')

        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π
        defaults = {
            'multiplicity': 1,
            'length': None,
            'width': None,
            'height': None,
            'weight': None,
            'dimensions_str': None,
            'image_url': None,
            'price': None
        }
        for col, val in defaults.items():
            if col not in parts_df.columns:
                dtype = pl.Float64 if isinstance(val, float) else pl.Int32 if isinstance(val, int) else pl.Utf8
                parts_df = parts_df.with_columns(pl.lit(val).cast(dtype).alias(col))

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è dimensions_str
        parts_df = parts_df.with_columns(
            dimensions_str=pl.when(
                (pl.col('dimensions_str').is_null()) |
                (pl.col('dimensions_str') == '') |
                (pl.col('dimensions_str').str.to_lowercase() == 'xx')
            ).then(
                pl.concat_str([
                    pl.col('length').cast(pl.Utf8).fill_null(''),
                    pl.lit('x'),
                    pl.col('width').cast(pl.Utf8).fill_null(''),
                    pl.lit('x'),
                    pl.col('height').cast(pl.Utf8).fill_null('')
                ], separator='')
            ).otherwise(pl.col('dimensions_str'))
        )

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è
        parts_df = parts_df.with_columns(
            description=pl.concat_str([
                pl.lit('–ê—Ä—Ç–∏–∫—É–ª: '), pl.col('artikul'),
                pl.lit(', –ë—Ä–µ–Ω–¥: '), pl.col('brand'),
                pl.lit(', –ö—Ä–∞—Ç–Ω–æ—Å—Ç—å: '), pl.col('multiplicity').cast(pl.Utf8), pl.lit(' —à—Ç.')
            ], separator='').alias('description')
        )

        self.upsert_data('parts_data', parts_df, ['artikul_norm', 'brand_norm'])
        st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –≤ –±–∞–∑–µ.")
        self.sync_to_cloud()  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –æ–±–ª–∞–∫–æ

    def load_price_file(self, file_bytes: bytes):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–π—Å–∞ –∏–∑ —Ñ–∞–π–ª–∞ Excel —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞."""
        try:
            df = pl.read_excel(io.BytesIO(file_bytes), engine='calamine')
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø—Ä–∞–π—Å–∞: {e}")
            return

        required = ['–∞—Ä—Ç–∏–∫—É–ª', '–±—Ä–µ–Ω–¥', '—Ü–µ–Ω–∞']
        if not all(col in [c.lower() for c in df.columns] for col in required):
            st.warning(f"–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏: {required}")
            return

        df = (df.rename(mapping={c: k for c in df.columns for k in required if k in c.lower()})
              .with_columns([
                  self.normalize_key(pl.col('–∞—Ä—Ç–∏–∫—É–ª')).alias('artikul_norm'),
                  self.normalize_key(pl.col('–±—Ä–µ–Ω–¥')).alias('brand_norm'),
                  pl.col('—Ü–µ–Ω–∞').cast(pl.Float64)
              ])
              .select(['artikul_norm', 'brand_norm', '—Ü–µ–Ω–∞']))
        self.prices_df = pl.concat([self.prices_df, df]).unique(subset=['artikul_norm', 'brand_norm'])
        st.success("–ü—Ä–∞–π—Å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω.")
        self.apply_markups()  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –Ω–∞—Ü–µ–Ω–∫–∏

    def apply_markups(self):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—â–µ–π –∏ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π –Ω–∞—Ü–µ–Ω–∫–∏."""
        if self.prices_df.is_empty():
            return
        for row in self.prices_df.iter_rows():
            artikul_norm, brand_norm, base_price = row
            markup = self.brand_markup.get(brand_norm, self.price_markup)
            final_price = base_price * markup
            self.conn.execute("""
                UPDATE parts_data SET price = ? WHERE artikul_norm = ? AND brand_norm = ?
            """, [final_price, artikul_norm, brand_norm])
        self.sync_to_cloud()

    def set_brand_markup(self, brand: str, percent: float):
        """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–∞—Ü–µ–Ω–∫—É –¥–ª—è –±—Ä–µ–Ω–¥–∞."""
        normalized = self.normalize_key(pl.Series([brand]))[0]
        self.brand_markup[normalized] = 1 + percent / 100
        self.apply_markups()
        st.info(f"–ù–∞—Ü–µ–Ω–∫–∞ –¥–ª—è –±—Ä–µ–Ω–¥–∞ '{brand}': {percent}%")

    def set_global_markup(self, percent: float):
        """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—â—É—é –Ω–∞—Ü–µ–Ω–∫—É."""
        self.price_markup = 1 + percent / 100
        self.apply_markups()
        st.info(f"–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞: {percent}%")

    def build_export_query(self, selected_columns: List[str]) -> str:
        """–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ SQL-–∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–∫–ª—é—á–µ–Ω–∏—è–º–∏."""
        standard_description = textwrap.dedent("""
            –°–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: –Ω–æ–≤—ã–π (–≤ —É–ø–∞–∫–æ–≤–∫–µ).
            –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã ‚Äî –Ω–∞–¥–µ–∂–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è.
            ...
        """).strip()

        # –§–∏–ª—å—Ç—Ä –ø–æ –∏—Å–∫–ª—é—á–µ–Ω–∏—è–º –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
        exclusion_conditions = []
        for excl in self.export_exclusions:
            pattern = excl.lower().replace("*", "%").replace("?", "_")
            exclusion_conditions.append(f"LOWER(COALESCE(o.name, '')) NOT LIKE '%{pattern}%'")

        exclusion_where = " AND ".join(exclusion_conditions) if exclusion_conditions else "TRUE"

        column_map = {
            "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞": 'r.artikul AS "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞"',
            "–ë—Ä–µ–Ω–¥": 'r.brand AS "–ë—Ä–µ–Ω–¥"',
            "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": 'COALESCE(r.representative_name, r.analog_representative_name) AS "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"',
            "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å": 'COALESCE(r.representative_applicability, r.analog_representative_applicability) AS "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å"',
            "–û–ø–∏—Å–∞–Ω–∏–µ": "CONCAT(COALESCE(r.description, ''), dt.text) AS \"–û–ø–∏—Å–∞–Ω–∏–µ\"",
            "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞": 'COALESCE(r.representative_category, r.analog_representative_category) AS "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞"',
            "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å": 'r.multiplicity AS "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"',
            "–î–ª–∏–Ω–Ω–∞": 'COALESCE(r.length, r.analog_length) AS "–î–ª–∏–Ω–Ω–∞"',
            "–®–∏—Ä–∏–Ω–∞": 'COALESCE(r.width, r.analog_width) AS "–®–∏—Ä–∏–Ω–∞"',
            "–í—ã—Å–æ—Ç–∞": 'COALESCE(r.height, r.analog_height) AS "–í—ã—Å–æ—Ç–∞"',
            "–í–µ—Å": 'COALESCE(r.weight, r.analog_weight) AS "–í–µ—Å"',
            "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞": "COALESCE(NULLIF(TRIM(r.dimensions_str), ''), NULLIF(TRIM(r.analog_dimensions_str), '')) AS \"–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞\"",
            "OE –Ω–æ–º–µ—Ä": 'r.oe_list AS "OE –Ω–æ–º–µ—Ä"',
            "–∞–Ω–∞–ª–æ–≥–∏": 'r.analog_list AS "–∞–Ω–∞–ª–æ–≥–∏"',
            "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ": 'r.image_url AS "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"'
        }

        selected_exprs = [column_map[col] for col in selected_columns if col in column_map]
        if not selected_exprs:
            selected_exprs = list(column_map.values())

        ctes = textwrap.dedent(f"""
        WITH DescriptionTemplate AS (
            SELECT '\n\n' || $${standard_description}$$ AS text
        ),
        PartDetails AS (
            SELECT
                cr.artikul_norm,
                cr.brand_norm,
                STRING_AGG(DISTINCT REGEXP_REPLACE(o.oe_number, '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-]', '', 'g'), ', ') AS oe_list,
                ANY_VALUE(o.name) AS representative_name,
                ANY_VALUE(o.applicability) AS representative_applicability,
                ANY_VALUE(o.category) AS representative_category
            FROM cross_references cr
            JOIN oe_data o ON cr.oe_number_norm = o.oe_number_norm
            WHERE {exclusion_where}
            GROUP BY cr.artikul_norm, cr.brand_norm
        ),
        AllAnalogs AS (
            SELECT
                cr1.artikul_norm,
                cr1.brand_norm,
                STRING_AGG(DISTINCT REGEXP_REPLACE(p2.artikul, '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-]', '', 'g'), ', ') AS analog_list
            FROM cross_references cr1
            JOIN cross_references cr2 ON cr1.oe_number_norm = cr2.oe_number_norm
            JOIN parts_data p2 ON cr2.artikul_norm = p2.artikul_norm AND cr2.brand_norm = p2.brand_norm
            WHERE cr1.artikul_norm != p2.artikul_norm OR cr1.brand_norm != p2.brand_norm
            GROUP BY cr1.artikul_norm, cr1.brand_norm
        ),
        RankedData AS (
            SELECT
                p.artikul,
                p.brand,
                p.description,
                p.multiplicity,
                p.length,
                p.width,
                p.height,
                p.weight,
                p.dimensions_str,
                p.image_url,
                pd.representative_name,
                pd.representative_applicability,
                pd.representative_category,
                pd.oe_list,
                aa.analog_list,
                p_analog.length AS analog_length,
                p_analog.width AS analog_width,
                p_analog.height AS analog_height,
                p_analog.weight AS analog_weight,
                p_analog.dimensions_str AS analog_dimensions_str,
                p_analog.representative_name AS analog_representative_name,
                p_analog.representative_applicability AS analog_representative_applicability,
                p_analog.representative_category AS analog_representative_category,
                ROW_NUMBER() OVER(PARTITION BY p.artikul_norm, p.brand_norm ORDER BY pd.representative_name DESC NULLS LAST) AS rn
            FROM parts_data p
            LEFT JOIN PartDetails pd ON p.artikul_norm = pd.artikul_norm AND p.brand_norm = pd.brand_norm
            LEFT JOIN AllAnalogs aa ON p.artikul_norm = aa.artikul_norm AND p.brand_norm = aa.brand_norm
            LEFT JOIN parts_data p_analog ON p.artikul_norm = p_analog.artikul_norm AND p.brand_norm = p_analog.brand_norm
        )
        """)

        select_clause = ",\n            ".join(selected_exprs)
        query = f"""
        {ctes}
        SELECT
            {select_clause}
        FROM RankedData r
        CROSS JOIN DescriptionTemplate dt
        WHERE r.rn = 1
        ORDER BY r.brand, r.artikul
        """
        return query

    def delete_by_brand(self, brand_norm: str) -> int:
        res = self.conn.execute("DELETE FROM parts_data WHERE brand_norm = ?", [brand_norm])
        self.sync_to_cloud()
        return res.rowcount

    def delete_by_artikul(self, artikul_norm: str) -> int:
        res = self.conn.execute("DELETE FROM parts_data WHERE artikul_norm = ?", [artikul_norm])
        self.sync_to_cloud()
        return res.rowcount

    def assign_category_by_name(self, search_name: str, category_name: str, similarity_threshold: float = 0.5):
        """–ü—Ä–∏—Å–≤–æ–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –≤—Å–µ–º —Ç–æ–≤–∞—Ä–∞–º —Å –ø–æ—Ö–æ–∂–∏–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏."""
        res = self.conn.execute("SELECT DISTINCT name FROM oe_data WHERE name IS NOT NULL").fetchall()
        names = [row[0] for row in res]
        matched = [
            name for name in names
            if difflib.SequenceMatcher(None, name.lower(), search_name.lower()).ratio() >= similarity_threshold
        ]
        for name in matched:
            self.conn.execute("UPDATE oe_data SET category = ? WHERE name = ?", [category_name, name])
        st.success(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ {len(matched)} –∑–∞–ø–∏—Å–µ–π –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é '{category_name}'.")
        self.sync_to_cloud()

    def add_category_mapping(self, keyword: str, category: str):
        """–î–æ–±–∞–≤–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ –ø—Ä–∞–≤–∏–ª–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""
        self.category_mapping[keyword.lower()] = keyword.lower()  # –£–ø—Ä–æ—â—ë–Ω–Ω–æ
        # –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤: self.category_mapping[category] = keyword_pattern

    def get_total_records(self):
        res = self.conn.execute("SELECT COUNT(*) FROM parts_data").fetchone()
        return res[0] if res else 0

    def get_statistics(self):
        total_parts = self.get_total_records()
        total_oe = self.conn.execute("SELECT COUNT(*) FROM oe_data").fetchone()[0]
        total_brands = self.conn.execute("SELECT COUNT(DISTINCT brand) FROM parts_data").fetchone()[0]
        top_brands = self.conn.execute("""
            SELECT brand, COUNT(*) AS cnt FROM parts_data GROUP BY brand ORDER BY cnt DESC LIMIT 10
        """).pl()
        categories = self.conn.execute("""
            SELECT category, COUNT(*) AS cnt FROM oe_data WHERE category IS NOT NULL GROUP BY category ORDER BY cnt DESC
        """).pl()
        return {
            'total_parts': total_parts,
            'total_oe': total_oe,
            'total_brands': total_brands,
            'top_brands': top_brands,
            'categories': categories
        }

    def export_to_excel(self, selected_columns: List[str], file_path: Path):
        query = self.build_export_query(selected_columns)
        result_df = self.conn.execute(query).pl()
        result_df.write_excel(file_path)

# === –ì–õ–ê–í–ù–´–ô –ò–ù–¢–ï–†–§–ï–ô–° ===
def main():
    st.title("üöó AutoParts Catalog - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 10+ –º–ª–Ω –∑–∞–ø–∏—Å–µ–π")
    catalog = HighVolumeAutoPartsCatalog()

    st.sidebar.title("üß≠ –ú–µ–Ω—é")
    menu = st.sidebar.radio("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:", [
        "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö", "–≠–∫—Å–ø–æ—Ä—Ç", "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω–∞–º–∏", "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏"
    ])

    if menu == "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö":
        st.header("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã Excel –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–∞—Ç–∞–ª–æ–≥–∞.")
        col1, col2 = st.columns(2)

        with col1:
            oe_file = st.file_uploader("–û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (OE)", type=['xlsx', 'xls'], key="oe")
            cross_file = st.file_uploader("–ö—Ä–æ—Å—Å—ã", type=['xlsx', 'xls'], key="cross")
            barcode_file = st.file_uploader("–®—Ç—Ä–∏—Ö-–∫–æ–¥—ã", type=['xlsx', 'xls'], key="barcode")
        with col2:
            dimensions_file = st.file_uploader("–í–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã", type=['xlsx', 'xls'], key="dim")
            images_file = st.file_uploader("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", type=['xlsx', 'xls'], key="img")

        uploaded_files = {
            'oe': oe_file,
            'cross': cross_file,
            'barcode': barcode_file,
            'dimensions': dimensions_file,
            'images': images_file
        }

        if st.button("üöÄ –ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É"):
            file_paths = {}
            for key, uploaded in uploaded_files.items():
                if uploaded:
                    path = catalog.data_dir / f"{key}_{int(time.time())}_{uploaded.name}"
                    with open(path, 'wb') as f:
                        f.write(uploaded.getvalue())
                    file_paths[key] = str(path)
            if file_paths:
                catalog.merge_all_data_parallel(file_paths)
                st.success("‚úÖ –î–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —Å –æ–±–ª–∞–∫–æ–º.")
            else:
                st.warning("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–π–ª.")

    elif menu == "–≠–∫—Å–ø–æ—Ä—Ç":
        catalog.show_export_interface()

    elif menu == "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞":
        st.header("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–∞–ª–æ–≥—É")
        stats = catalog.get_statistics()
        col1, col2, col3 = st.columns(3)
        col1.metric("–í—Å–µ–≥–æ –∞—Ä—Ç–∏–∫—É–ª–æ–≤", f"{stats['total_parts']:,}")
        col2.metric("OE –Ω–æ–º–µ—Ä–æ–≤", f"{stats['total_oe']:,}")
        col3.metric("–ë—Ä–µ–Ω–¥–æ–≤", stats['total_brands'])
        st.subheader("–¢–æ–ø –±—Ä–µ–Ω–¥–æ–≤")
        st.dataframe(stats['top_brands'].to_pandas())
        st.subheader("–ö–∞—Ç–µ–≥–æ—Ä–∏–∏")
        st.bar_chart(stats['categories'].to_pandas().set_index('category'))

    elif menu == "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω–∞–º–∏":
        st.header("üõ†Ô∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω–∞–º–∏")
        uploaded_price = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∞–π—Å (–∞—Ä—Ç–∏–∫—É–ª, –±—Ä–µ–Ω–¥, —Ü–µ–Ω–∞)", type=['xlsx', 'xls'])
        if uploaded_price:
            catalog.load_price_file(uploaded_price.read())

        markup = st.number_input("–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞ (%)", 0.0, 100.0, 0.0)
        if st.button("–ü—Ä–∏–º–µ–Ω–∏—Ç—å –æ–±—â—É—é –Ω–∞—Ü–µ–Ω–∫—É"):
            catalog.set_global_markup(markup)

        st.subheader("–ù–∞—Ü–µ–Ω–∫–∞ –ø–æ –±—Ä–µ–Ω–¥—É")
        brand_name = st.text_input("–ë—Ä–µ–Ω–¥")
        brand_markup_percent = st.number_input("–ù–∞—Ü–µ–Ω–∫–∞ (%)", 0.0, 100.0, 0.0, key="brand_markup")
        if st.button("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –¥–ª—è –±—Ä–µ–Ω–¥–∞"):
            if brand_name.strip():
                catalog.set_brand_markup(brand_name, brand_markup_percent)
            else:
                st.warning("–í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –±—Ä–µ–Ω–¥–∞.")

    elif menu == "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏":
        st.header("üóëÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏")
        action = st.radio("–î–µ–π—Å—Ç–≤–∏–µ", [
            "–£–¥–∞–ª–∏—Ç—å –ø–æ –±—Ä–µ–Ω–¥—É",
            "–£–¥–∞–ª–∏—Ç—å –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É",
            "–ù–∞–∑–Ω–∞—á–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é",
            "–î–æ–±–∞–≤–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é",
            "–ò—Å–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ"
        ])

        if action == "–£–¥–∞–ª–∏—Ç—å –ø–æ –±—Ä–µ–Ω–¥—É":
            brands = [r[0] for r in catalog.conn.execute("SELECT DISTINCT brand FROM parts_data").fetchall()]
            selected = st.selectbox("–ë—Ä–µ–Ω–¥", brands)
            if selected:
                norm = catalog.normalize_key(pl.Series([selected]))[0]
                count = catalog.delete_by_brand(norm)
                st.success(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ {count} –∑–∞–ø–∏—Å–µ–π.")

        elif action == "–£–¥–∞–ª–∏—Ç—å –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É":
            artikul = st.text_input("–ê—Ä—Ç–∏–∫—É–ª")
            if artikul:
                norm = catalog.normalize_key(pl.Series([artikul]))[0]
                count = catalog.delete_by_artikul(norm)
                st.success(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ {count} –∑–∞–ø–∏—Å–µ–π.")

        elif action == "–ù–∞–∑–Ω–∞—á–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é":
            name_input = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞")
            cat_input = st.text_input("–ö–∞—Ç–µ–≥–æ—Ä–∏—è")
            threshold = st.slider("–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏", 0.0, 1.0, 0.5, 0.05)
            if st.button("–ù–∞–∑–Ω–∞—á–∏—Ç—å"):
                if name_input and cat_input:
                    catalog.assign_category_by_name(name_input, cat_input, threshold)
                else:
                    st.warning("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –æ–±–∞ –ø–æ–ª—è.")

        elif action == "–î–æ–±–∞–≤–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é":
            keyword = st.text_input("–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏")
            category = st.text_input("–ö–∞—Ç–µ–≥–æ—Ä–∏—è")
            if st.button("–î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∞–≤–∏–ª–æ"):
                if keyword and category:
                    catalog.add_category_mapping(keyword, category)
                    st.success(f"‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è '{category}' –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –¥–ª—è —Ç–æ–≤–∞—Ä–æ–≤ —Å '{keyword}' –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏.")
                else:
                    st.warning("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –æ–±–∞ –ø–æ–ª—è.")

        elif action == "–ò—Å–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ":
            exclusion_input = st.text_input("–®–∞–±–ª–æ–Ω –∏—Å–∫–ª—é—á–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä: *–∫—É–∑–æ–≤*, *—Å—Ç–µ–∫–ª–∞*)")
            if st.button("–î–æ–±–∞–≤–∏—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ"):
                if exclusion_input:
                    catalog.export_exclusions.append(exclusion_input.strip().strip("*"))
                    st.success(f"üö´ –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ: {exclusion_input}")
                st.write("–¢–µ–∫—É—â–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è:", ", ".join(catalog.export_exclusions))

if __name__ == "__main__":
    main()
