import platform
import sys
import polars as pl
import duckdb
import streamlit as st
import os
import time
import logging
import io
import zipfile
from pathlib import Path
from typing import Dict, List, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings
import json

warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ Excel
EXCEL_ROW_LIMIT = 1_000_000

class HighVolumeAutoPartsCatalog:
    def __init__(self):
        self.data_dir = Path("./auto_parts_data")
        self.data_dir.mkdir(exist_ok=True)

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±–ª–∞—á–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        self.cloud_config = self.load_cloud_config()
        self.db_path = self.data_dir / "catalog.duckdb"
        self.conn = duckdb.connect(database=str(self.db_path))
        self.setup_database()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∏
        self.price_rules = self.load_price_rules()
        self.exclusion_rules = self.load_exclusion_rules()
        self.category_mapping = self.load_category_mapping()

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ Streamlit
        st.set_page_config(
            page_title="AutoParts Catalog 10M+",
            layout="wide",
            page_icon="üöó"
        )

    def load_cloud_config(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±–ª–∞—á–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        config_path = self.data_dir / "cloud_config.json"
        default_config = {
            "enabled": False,
            "provider": "s3",
            "bucket": "",
            "region": "",
            "sync_interval": 3600,
            "last_sync": 0
        }
        if config_path.exists():
            try:
                return json.loads(config_path.read_text(encoding='utf-8'))
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è cloud_config.json: {e}")
                return default_config
        else:
            config_path.write_text(json.dumps(default_config, indent=2, ensure_ascii=False), encoding='utf-8')
            return default_config

    def save_cloud_config(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±–ª–∞—á–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        config_path = self.data_dir / "cloud_config.json"
        self.cloud_config["last_sync"] = int(time.time())
        config_path.write_text(
            json.dumps(self.cloud_config, indent=2, ensure_ascii=False),
            encoding='utf-8'
        )

    def load_price_rules(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"""
        price_rules_path = self.data_dir / "price_rules.json"
        default_rules = {
            "global_markup": 0.2,
            "brand_markups": {},
            "min_price": 0.0,
            "max_price": 99999.0
        }
        if price_rules_path.exists():
            try:
                return json.loads(price_rules_path.read_text(encoding='utf-8'))
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è price_rules.json: {e}")
                return default_rules
        else:
            price_rules_path.write_text(
                json.dumps(default_rules, indent=2, ensure_ascii=False),
                encoding='utf-8'
            )
            return default_rules

    def save_price_rules(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"""
        price_rules_path = self.data_dir / "price_rules.json"
        price_rules_path.write_text(
            json.dumps(self.price_rules, indent=2, ensure_ascii=False),
            encoding='utf-8'
        )

    def load_exclusion_rules(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –∏—Å–∫–ª—é—á–µ–Ω–∏—è"""
        exclusion_path = self.data_dir / "exclusion_rules.txt"
        if exclusion_path.exists():
            try:
                return [
                    line.strip()
                    for line in exclusion_path.read_text(encoding='utf-8').splitlines()
                    if line.strip()
                ]
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è exclusion_rules.txt: {e}")
                return []
        else:
            content = "–ö—É–∑–æ–≤\n–°—Ç–µ–∫–ª–∞\n–ú–∞—Å–ª–∞"
            exclusion_path.write_text(content, encoding='utf-8')
            return ["–ö—É–∑–æ–≤", "–°—Ç–µ–∫–ª–∞", "–ú–∞—Å–ª–∞"]

    def save_exclusion_rules(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –∏—Å–∫–ª—é—á–µ–Ω–∏—è"""
        exclusion_path = self.data_dir / "exclusion_rules.txt"
        exclusion_path.write_text(
            "\n".join(self.exclusion_rules),
            encoding='utf-8'
        )

    def load_category_mapping(self) -> Dict[str, str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–∞–ø–ø–∏–Ω–≥–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        category_path = self.data_dir / "category_mapping.txt"
        default_mapping = {
            "–†–∞–¥–∏–∞—Ç–æ—Ä": "–û—Ö–ª–∞–∂–¥–µ–Ω–∏–µ",
            "–®–∞—Ä–æ–≤–∞—è –æ–ø–æ—Ä–∞": "–ü–æ–¥–≤–µ—Å–∫–∞",
            "–§–∏–ª—å—Ç—Ä –º–∞—Å–ª—è–Ω—ã–π": "–§–∏–ª—å—Ç—Ä—ã",
            "–¢–æ—Ä–º–æ–∑–Ω—ã–µ –∫–æ–ª–æ–¥–∫–∏": "–¢–æ—Ä–º–æ–∑–∞"
        }
        if category_path.exists():
            try:
                mapping = {}
                for line in category_path.read_text(encoding='utf-8').splitlines():
                    if line.strip() and "|" in line:
                        key, value = line.split("|", 1)
                        mapping[key.strip()] = value.strip()
                return mapping
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è category_mapping.txt: {e}")
                return default_mapping
        else:
            content = "\n".join([f"{k}|{v}" for k, v in default_mapping.items()])
            category_path.write_text(content, encoding='utf-8')
            return default_mapping

    def save_category_mapping(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        category_path = self.data_dir / "category_mapping.txt"
        content = "\n".join([f"{k}|{v}" for k, v in self.category_mapping.items()])
        category_path.write_text(content, encoding='utf-8')

    def setup_database(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –≤ DuckDB"""
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE,
                width DOUBLE,
                height DOUBLE,
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS prices (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                price DOUBLE,
                currency VARCHAR DEFAULT 'RUB',
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        
        self.create_indexes()

    def create_indexes(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤"""
        st.info("–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞...")
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_oe_data_oe ON oe_data(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_parts_data_keys ON parts_data(artikul_norm, brand_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_oe ON cross_references(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_artikul ON cross_references(artikul_norm, brand_norm)",
            "CREATE INDEX IF NOT EXISTS idx_prices_keys ON prices(artikul_norm, brand_norm)"
        ]
        for index_sql in indexes:
            self.conn.execute(index_sql)
        st.success("–ò–Ω–¥–µ–∫—Å—ã —Å–æ–∑–¥–∞–Ω—ã.")

    @staticmethod
    def normalize_key(key_series: pl.Series) -> pl.Series:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π (–∞—Ä—Ç–∏–∫—É–ª, –±—Ä–µ–Ω–¥, OE)"""
        return (key_series
               .fill_null("")
               .cast(pl.Utf8)
               .str.replace_all("'", "")
               .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]", "")
               .str.replace_all(r"\s+", " ")
               .str.strip_chars()
               .str.to_lowercase())

    @staticmethod
    def clean_values(value_series: pl.Series) -> pl.Series:
        """–û—á–∏—Å—Ç–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –æ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        return (value_series
               .fill_null("")
               .cast(pl.Utf8)
               .str.replace_all("'", "")
               .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]", "")
               .str.replace_all(r"\s+", " ")
               .str.strip_chars())

    def determine_category_vectorized(self, name_series: pl.Series) -> pl.Series:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –≤—Ö–æ–∂–¥–µ–Ω–∏—é –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (—Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–∞–≤–∏–ª)"""
        name_lower = name_series.str.to_lowercase()
        categorization_expr = pl.when(pl.lit(False)).then(pl.lit(None))

        # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ ‚Äî –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≤—ã—à–µ
        for key, category in self.category_mapping.items():
            categorization_expr = categorization_expr.when(
                name_lower.str.contains(key.lower())
            ).then(pl.lit(category))

        # –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
        categories_map = {
            '–§–∏–ª—å—Ç—Ä': '—Ñ–∏–ª—å—Ç—Ä|filter',
            '–¢–æ—Ä–º–æ–∑–∞': '—Ç–æ—Ä–º–æ–∑|brake|–∫–æ–ª–æ–¥–∫|–¥–∏—Å–∫|—Å—É–ø–ø–æ—Ä—Ç',
            '–ü–æ–¥–≤–µ—Å–∫–∞': '–∞–º–æ—Ä—Ç–∏–∑–∞—Ç–æ—Ä|—Å—Ç–æ–π–∫|spring|–ø–æ–¥–≤–µ—Å–∫|—Ä—ã—á–∞–≥',
            '–î–≤–∏–≥–∞—Ç–µ–ª—å': '–¥–≤–∏–≥–∞—Ç–µ–ª—å|engine|—Å–≤–µ—á|–ø–æ—Ä—à–µ–Ω—å|–∫–ª–∞–ø–∞–Ω',
            '–¢—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è': '—Ç—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è|—Å—Ü–µ–ø–ª–µ–Ω|–∫–æ—Ä–æ–±–∫|transmission',
            '–≠–ª–µ–∫—Ç—Ä–∏–∫–∞': '–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä|–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä|—Å—Ç–∞—Ä—Ç–µ—Ä|–ø—Ä–æ–≤–æ–¥|–ª–∞–º–ø',
            '–†—É–ª–µ–≤–æ–µ': '—Ä—É–ª–µ–≤–æ–π|—Ç—è–≥–∞|–Ω–∞–∫–æ–Ω–µ—á–Ω–∏–∫|steering',
            '–í—ã–ø—É—Å–∫': '–≥–ª—É—à–∏—Ç–µ–ª—å|–∫–∞—Ç–∞–ª–∏–∑–∞—Ç–æ—Ä|–≤—ã—Ö–ª–æ–ø|exhaust',
            '–û—Ö–ª–∞–∂–¥–µ–Ω–∏–µ': '—Ä–∞–¥–∏–∞—Ç–æ—Ä|–≤–µ–Ω—Ç–∏–ª—è—Ç–æ—Ä|—Ç–µ—Ä–º–æ—Å—Ç–∞—Ç|cooling',
            '–¢–æ–ø–ª–∏–≤–æ': '—Ç–æ–ø–ª–∏–≤–Ω—ã–π|–±–µ–Ω–∑–æ–Ω–∞—Å–æ—Å|—Ñ–æ—Ä—Å—É–Ω–∫|fuel'
        }
        for category, pattern in categories_map.items():
            categorization_expr = categorization_expr.when(
                name_lower.str.contains(pattern, literal=False)
            ).then(pl.lit(category))

        return categorization_expr.otherwise(pl.lit('–†–∞–∑–Ω–æ–µ')).alias('category')

    def detect_columns(self, actual_columns: List[str], expected_columns: List[str]) -> Dict[str, str]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        column_variants = {
            'oe_number': ['oe –Ω–æ–º–µ—Ä', 'oe', '–æe', '–Ω–æ–º–µ—Ä', 'code', 'OE'],
            'artikul': ['–∞—Ä—Ç–∏–∫—É–ª', 'article', 'sku'],
            'brand': ['–±—Ä–µ–Ω–¥', 'brand', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', 'manufacturer'],
            'name': ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ', 'name', '–æ–ø–∏—Å–∞–Ω–∏–µ', 'description'],
            'applicability': ['–ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', 'vehicle', 'applicability'],
            'barcode': ['—à—Ç—Ä–∏—Ö-–∫–æ–¥', 'barcode', '—à—Ç—Ä–∏—Ö–∫–æ–¥', 'ean', 'eac13'],
            'multiplicity': ['–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å —à—Ç', '–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å', 'multiplicity'],
            'length': ['–¥–ª–∏–Ω–∞ (—Å–º)', '–¥–ª–∏–Ω–∞', 'length', '–¥–ª–∏–Ω–Ω–∞'],
            'width': ['—à–∏—Ä–∏–Ω–∞ (—Å–º)', '—à–∏—Ä–∏–Ω–∞', 'width'],
            'height': ['–≤—ã—Å–æ—Ç–∞ (—Å–º)', '–≤—ã—Å–æ—Ç–∞', 'height'],
            'weight': ['–≤–µ—Å (–∫–≥)', '–≤–µ—Å, –∫–≥', '–≤–µ—Å', 'weight'],
            'image_url': ['—Å—Å—ã–ª–∫–∞', 'url', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 'image', '–∫–∞—Ä—Ç–∏–Ω–∫–∞'],
            'dimensions_str': ['–≤–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã', '—Ä–∞–∑–º–µ—Ä—ã', 'dimensions', 'size'],
            'price': ['—Ü–µ–Ω–∞', 'price', '—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞', 'retail price'],
            'currency': ['–≤–∞–ª—é—Ç–∞', 'currency']
        }
        actual_lower = {col.lower(): col for col in actual_columns}
        mapping = {}
        for expected in expected_columns:
            variants = column_variants.get(expected, [expected])
            for variant in variants:
                variant_lower = variant.lower()
                for actual_l, actual_orig in actual_lower.items():
                    if variant_lower in actual_l and expected not in mapping.values():
                        mapping[actual_orig] = expected
                        break
                if expected in mapping.values():
                    break
        return mapping

    def read_and_prepare_file(self, file_path: str, file_type: str) -> pl.DataFrame:
        """–ß—Ç–µ–Ω–∏–µ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞"""
        logger.info(f"–ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞: {file_type} ({file_path})")
        try:
            if not os.path.exists(file_path):
                logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                return pl.DataFrame()

            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logger.warning(f"–§–∞–π–ª –ø—É—Å—Ç: {file_path}")
                return pl.DataFrame()

            df = pl.read_excel(file_path, engine='calamine')
            if df.is_empty():
                logger.warning(f"–§–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω, –Ω–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã—Ö: {file_path}")
                return pl.DataFrame()

        except Exception as e:
            logger.exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {file_path}: {e}")
            return pl.DataFrame()

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ö–µ–º—ã –ø–æ —Ç–∏–ø—É —Ñ–∞–π–ª–∞
        schemas = {
            'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
            'barcode': ['brand', 'artikul', 'barcode', 'multiplicity'],
            'dimensions': ['artikul', 'brand', 'length', 'width', 'height', 'weight', 'dimensions_str'],
            'images': ['artikul', 'brand', 'image_url'],
            'cross': ['oe_number', 'artikul', 'brand'],
            'prices': ['artikul', 'brand', 'price', 'currency']
        }

        expected_cols = schemas.get(file_type, [])
        column_mapping = self.detect_columns(df.columns, expected_cols)
        if not column_mapping:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ñ–∞–π–ª–∞ {file_type}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {df.columns}")
            return pl.DataFrame()

        df = df.rename(column_mapping)

        # –û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π
        for col in ['artikul', 'brand', 'oe_number']:
            if col in df.columns:
                df = df.with_columns(self.clean_values(pl.col(col)).alias(col))

        key_cols = [col for col in ['oe_number', 'artikul', 'brand'] if col in df.columns]
        if key_cols:
            df = df.unique(subset=key_cols, keep='first')

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π
        for col in ['artikul', 'brand', 'oe_number']:
            if col in df.columns:
                df = df.with_columns(
                    self.normalize_key(pl.col(col)).alias(f"{col}_norm")
                )

        return df

    def upsert_data(self, table_name: str, df: pl.DataFrame, pk: List[str]):
        """UPSERT –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü—É DuckDB"""
        if df.is_empty():
            return

        df = df.unique(keep='first')
        cols = df.columns
        pk_str = ", ".join(f'"{c}"' for c in pk)
        temp_view_name = f"temp_{table_name}_{int(time.time())}"

        self.conn.register(temp_view_name, df.to_arrow())
        update_cols = [col for col in cols if col not in pk]

        if not update_cols:
            on_conflict_action = "DO NOTHING"
        else:
            update_clause = ", ".join([f'"{col}" = excluded."{col}"' for col in update_cols])
            on_conflict_action = f"DO UPDATE SET {update_clause}"

        sql = f"""
            INSERT INTO {table_name}
            SELECT * FROM {temp_view_name}
            ON CONFLICT ({pk_str}) {on_conflict_action};
        """

        try:
            self.conn.execute(sql)
            logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–æ/–≤—Å—Ç–∞–≤–ª–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ UPSERT –≤ {table_name}: {e}")
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}. –î–µ—Ç–∞–ª–∏ –≤ –ª–æ–≥–µ.")
        finally:
            self.conn.unregister(temp_view_name)

    def upsert_prices(self, price_df: pl.DataFrame):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        if price_df.is_empty():
            return

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–π
        if 'artikul' in price_df.columns and 'brand' in price_df.columns:
            price_df = price_df.with_columns([
                self.normalize_key(pl.col('artikul')).alias('artikul_norm'),
                self.normalize_key(pl.col('brand')).alias('brand_norm')
            ])

        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤–∞–ª—é—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if 'currency' not in price_df.columns:
            price_df = price_df.with_columns(pl.lit('RUB').alias('currency'))

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω—É —Ü–µ–Ω
        price_df = price_df.filter(
            (pl.col('price') >= self.price_rules['min_price']) &
            (pl.col('price') <= self.price_rules['max_price'])
        )

        self.upsert_data('prices', price_df, ['artikul_norm', 'brand_norm'])

    def process_and_load_data(self, dataframes: Dict[str, pl.DataFrame]):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—É —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
        st.info("üîÑ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑–µ...")
        steps = [s for s in ['oe', 'cross', 'parts'] if s in dataframes]
import sys
import polars as pl
import duckdb
import streamlit as st
import os
import time
import logging
import io
import zipfile
from pathlib import Path
from typing import Dict, List, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings
import json

warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ Excel
EXCEL_ROW_LIMIT = 1_000_000

class HighVolumeAutoPartsCatalog:
    def __init__(self):
        self.data_dir = Path("./auto_parts_data")
        self.data_dir.mkdir(exist_ok=True)

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±–ª–∞—á–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        self.cloud_config = self.load_cloud_config()
        self.db_path = self.data_dir / "catalog.duckdb"
        self.conn = duckdb.connect(database=str(self.db_path))
        self.setup_database()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∏
        self.price_rules = self.load_price_rules()
        self.exclusion_rules = self.load_exclusion_rules()
        self.category_mapping = self.load_category_mapping()

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ Streamlit
        st.set_page_config(
            page_title="AutoParts Catalog 10M+",
            layout="wide",
            page_icon="üöó"
        )

    def load_cloud_config(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±–ª–∞—á–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        config_path = self.data_dir / "cloud_config.json"
        default_config = {
            "enabled": False,
            "provider": "s3",
            "bucket": "",
            "region": "",
            "sync_interval": 3600,
            "last_sync": 0
        }
        if config_path.exists():
            try:
                return json.loads(config_path.read_text(encoding='utf-8'))
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è cloud_config.json: {e}")
                return default_config
        else:
            config_path.write_text(json.dumps(default_config, indent=2, ensure_ascii=False), encoding='utf-8')
            return default_config

    def save_cloud_config(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±–ª–∞—á–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        config_path = self.data_dir / "cloud_config.json"
        self.cloud_config["last_sync"] = int(time.time())
        config_path.write_text(
            json.dumps(self.cloud_config, indent=2, ensure_ascii=False),
            encoding='utf-8'
        )

    def load_price_rules(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"""
        price_rules_path = self.data_dir / "price_rules.json"
        default_rules = {
            "global_markup": 0.2,
            "brand_markups": {},
            "min_price": 0.0,
            "max_price": 99999.0
        }
        if price_rules_path.exists():
            try:
                return json.loads(price_rules_path.read_text(encoding='utf-8'))
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è price_rules.json: {e}")
                return default_rules
        else:
            price_rules_path.write_text(
                json.dumps(default_rules, indent=2, ensure_ascii=False),
                encoding='utf-8'
            )
            return default_rules

    def save_price_rules(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"""
        price_rules_path = self.data_dir / "price_rules.json"
        price_rules_path.write_text(
            json.dumps(self.price_rules, indent=2, ensure_ascii=False),
            encoding='utf-8'
        )

    def load_exclusion_rules(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –∏—Å–∫–ª—é—á–µ–Ω–∏—è"""
        exclusion_path = self.data_dir / "exclusion_rules.txt"
        if exclusion_path.exists():
            try:
                return [
                    line.strip()
                    for line in exclusion_path.read_text(encoding='utf-8').splitlines()
                    if line.strip()
                ]
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è exclusion_rules.txt: {e}")
                return []
        else:
            content = "–ö—É–∑–æ–≤\n–°—Ç–µ–∫–ª–∞\n–ú–∞—Å–ª–∞"
            exclusion_path.write_text(content, encoding='utf-8')
            return ["–ö—É–∑–æ–≤", "–°—Ç–µ–∫–ª–∞", "–ú–∞—Å–ª–∞"]

    def save_exclusion_rules(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –∏—Å–∫–ª—é—á–µ–Ω–∏—è"""
        exclusion_path = self.data_dir / "exclusion_rules.txt"
        exclusion_path.write_text(
            "\n".join(self.exclusion_rules),
            encoding='utf-8'
        )

    def load_category_mapping(self) -> Dict[str, str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–∞–ø–ø–∏–Ω–≥–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        category_path = self.data_dir / "category_mapping.txt"
        default_mapping = {
            "–†–∞–¥–∏–∞—Ç–æ—Ä": "–û—Ö–ª–∞–∂–¥–µ–Ω–∏–µ",
            "–®–∞—Ä–æ–≤–∞—è –æ–ø–æ—Ä–∞": "–ü–æ–¥–≤–µ—Å–∫–∞",
            "–§–∏–ª—å—Ç—Ä –º–∞—Å–ª—è–Ω—ã–π": "–§–∏–ª—å—Ç—Ä—ã",
            "–¢–æ—Ä–º–æ–∑–Ω—ã–µ –∫–æ–ª–æ–¥–∫–∏": "–¢–æ—Ä–º–æ–∑–∞"
        }
        if category_path.exists():
            try:
                mapping = {}
                for line in category_path.read_text(encoding='utf-8').splitlines():
                    if line.strip() and "|" in line:
                        key, value = line.split("|", 1)
                        mapping[key.strip()] = value.strip()
                return mapping
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è category_mapping.txt: {e}")
                return default_mapping
        else:
            content = "\n".join([f"{k}|{v}" for k, v in default_mapping.items()])
            category_path.write_text(content, encoding='utf-8')
            return default_mapping

    def save_category_mapping(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        category_path = self.data_dir / "category_mapping.txt"
        content = "\n".join([f"{k}|{v}" for k, v in self.category_mapping.items()])
        category_path.write_text(content, encoding='utf-8')

    def setup_database(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –≤ DuckDB"""
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE,
                width DOUBLE,
                height DOUBLE,
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS prices (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                price DOUBLE,
                currency VARCHAR DEFAULT 'RUB',
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        
        self.create_indexes()

    def create_indexes(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤"""
        st.info("–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞...")
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_oe_data_oe ON oe_data(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_parts_data_keys ON parts_data(artikul_norm, brand_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_oe ON cross_references(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_artikul ON cross_references(artikul_norm, brand_norm)",
            "CREATE INDEX IF NOT EXISTS idx_prices_keys ON prices(artikul_norm, brand_norm)"
        ]
        for index_sql in indexes:
            self.conn.execute(index_sql)
        st.success("–ò–Ω–¥–µ–∫—Å—ã —Å–æ–∑–¥–∞–Ω—ã.")

    @staticmethod
    def normalize_key(key_series: pl.Series) -> pl.Series:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π (–∞—Ä—Ç–∏–∫—É–ª, –±—Ä–µ–Ω–¥, OE)"""
        return (key_series
               .fill_null("")
               .cast(pl.Utf8)
               .str.replace_all("'", "")
               .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]", "")
               .str.replace_all(r"\s+", " ")
               .str.strip_chars()
               .str.to_lowercase())

    @staticmethod
    def clean_values(value_series: pl.Series) -> pl.Series:
        """–û—á–∏—Å—Ç–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –æ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        return (value_series
               .fill_null("")
               .cast(pl.Utf8)
               .str.replace_all("'", "")
               .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]", "")
               .str.replace_all(r"\s+", " ")
               .str.strip_chars())

    def determine_category_vectorized(self, name_series: pl.Series) -> pl.Series:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –≤—Ö–æ–∂–¥–µ–Ω–∏—é –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (—Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–∞–≤–∏–ª)"""
        name_lower = name_series.str.to_lowercase()
        categorization_expr = pl.when(pl.lit(False)).then(pl.lit(None))

        # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ ‚Äî –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≤—ã—à–µ
        for key, category in self.category_mapping.items():
            categorization_expr = categorization_expr.when(
                name_lower.str.contains(key.lower())
            ).then(pl.lit(category))

        # –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
        categories_map = {
            '–§–∏–ª—å—Ç—Ä': '—Ñ–∏–ª—å—Ç—Ä|filter',
            '–¢–æ—Ä–º–æ–∑–∞': '—Ç–æ—Ä–º–æ–∑|brake|–∫–æ–ª–æ–¥–∫|–¥–∏—Å–∫|—Å—É–ø–ø–æ—Ä—Ç',
            '–ü–æ–¥–≤–µ—Å–∫–∞': '–∞–º–æ—Ä—Ç–∏–∑–∞—Ç–æ—Ä|—Å—Ç–æ–π–∫|spring|–ø–æ–¥–≤–µ—Å–∫|—Ä—ã—á–∞–≥',
            '–î–≤–∏–≥–∞—Ç–µ–ª—å': '–¥–≤–∏–≥–∞—Ç–µ–ª—å|engine|—Å–≤–µ—á|–ø–æ—Ä—à–µ–Ω—å|–∫–ª–∞–ø–∞–Ω',
            '–¢—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è': '—Ç—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è|—Å—Ü–µ–ø–ª–µ–Ω|–∫–æ—Ä–æ–±–∫|transmission',
            '–≠–ª–µ–∫—Ç—Ä–∏–∫–∞': '–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä|–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä|—Å—Ç–∞—Ä—Ç–µ—Ä|–ø—Ä–æ–≤–æ–¥|–ª–∞–º–ø',
            '–†—É–ª–µ–≤–æ–µ': '—Ä—É–ª–µ–≤–æ–π|—Ç—è–≥–∞|–Ω–∞–∫–æ–Ω–µ—á–Ω–∏–∫|steering',
            '–í—ã–ø—É—Å–∫': '–≥–ª—É—à–∏—Ç–µ–ª—å|–∫–∞—Ç–∞–ª–∏–∑–∞—Ç–æ—Ä|–≤—ã—Ö–ª–æ–ø|exhaust',
            '–û—Ö–ª–∞–∂–¥–µ–Ω–∏–µ': '—Ä–∞–¥–∏–∞—Ç–æ—Ä|–≤–µ–Ω—Ç–∏–ª—è—Ç–æ—Ä|—Ç–µ—Ä–º–æ—Å—Ç–∞—Ç|cooling',
            '–¢–æ–ø–ª–∏–≤–æ': '—Ç–æ–ø–ª–∏–≤–Ω—ã–π|–±–µ–Ω–∑–æ–Ω–∞—Å–æ—Å|—Ñ–æ—Ä—Å—É–Ω–∫|fuel'
        }
        for category, pattern in categories_map.items():
            categorization_expr = categorization_expr.when(
                name_lower.str.contains(pattern, literal=False)
            ).then(pl.lit(category))

        return categorization_expr.otherwise(pl.lit('–†–∞–∑–Ω–æ–µ')).alias('category')

    def detect_columns(self, actual_columns: List[str], expected_columns: List[str]) -> Dict[str, str]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        column_variants = {
            'oe_number': ['oe –Ω–æ–º–µ—Ä', 'oe', '–æe', '–Ω–æ–º–µ—Ä', 'code', 'OE'],
            'artikul': ['–∞—Ä—Ç–∏–∫—É–ª', 'article', 'sku'],
            'brand': ['–±—Ä–µ–Ω–¥', 'brand', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', 'manufacturer'],
            'name': ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ', 'name', '–æ–ø–∏—Å–∞–Ω–∏–µ', 'description'],
            'applicability': ['–ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', 'vehicle', 'applicability'],
            'barcode': ['—à—Ç—Ä–∏—Ö-–∫–æ–¥', 'barcode', '—à—Ç—Ä–∏—Ö–∫–æ–¥', 'ean', 'eac13'],
            'multiplicity': ['–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å —à—Ç', '–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å', 'multiplicity'],
            'length': ['–¥–ª–∏–Ω–∞ (—Å–º)', '–¥–ª–∏–Ω–∞', 'length', '–¥–ª–∏–Ω–Ω–∞'],
            'width': ['—à–∏—Ä–∏–Ω–∞ (—Å–º)', '—à–∏—Ä–∏–Ω–∞', 'width'],
            'height': ['–≤—ã—Å–æ—Ç–∞ (—Å–º)', '–≤—ã—Å–æ—Ç–∞', 'height'],
            'weight': ['–≤–µ—Å (–∫–≥)', '–≤–µ—Å, –∫–≥', '–≤–µ—Å', 'weight'],
            'image_url': ['—Å—Å—ã–ª–∫–∞', 'url', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 'image', '–∫–∞—Ä—Ç–∏–Ω–∫–∞'],
            'dimensions_str': ['–≤–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã', '—Ä–∞–∑–º–µ—Ä—ã', 'dimensions', 'size'],
            'price': ['—Ü–µ–Ω–∞', 'price', '—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞', 'retail price'],
            'currency': ['–≤–∞–ª—é—Ç–∞', 'currency']
        }
        actual_lower = {col.lower(): col for col in actual_columns}
        mapping = {}
        for expected in expected_columns:
            variants = column_variants.get(expected, [expected])
            for variant in variants:
                variant_lower = variant.lower()
                for actual_l, actual_orig in actual_lower.items():
                    if variant_lower in actual_l and expected not in mapping.values():
                        mapping[actual_orig] = expected
                        break
                if expected in mapping.values():
                    break
        return mapping

    def read_and_prepare_file(self, file_path: str, file_type: str) -> pl.DataFrame:
        """–ß—Ç–µ–Ω–∏–µ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞"""
        logger.info(f"–ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞: {file_type} ({file_path})")
        try:
            if not os.path.exists(file_path):
                logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                return pl.DataFrame()

            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logger.warning(f"–§–∞–π–ª –ø—É—Å—Ç: {file_path}")
                return pl.DataFrame()

            df = pl.read_excel(file_path, engine='calamine')
            if df.is_empty():
                logger.warning(f"–§–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω, –Ω–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã—Ö: {file_path}")
                return pl.DataFrame()

        except Exception as e:
            logger.exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {file_path}: {e}")
            return pl.DataFrame()

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ö–µ–º—ã –ø–æ —Ç–∏–ø—É —Ñ–∞–π–ª–∞
        schemas = {
            'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
            'barcode': ['brand', 'artikul', 'barcode', 'multiplicity'],
            'dimensions': ['artikul', 'brand', 'length', 'width', 'height', 'weight', 'dimensions_str'],
            'images': ['artikul', 'brand', 'image_url'],
            'cross': ['oe_number', 'artikul', 'brand'],
            'prices': ['artikul', 'brand', 'price', 'currency']
        }

        expected_cols = schemas.get(file_type, [])
        column_mapping = self.detect_columns(df.columns, expected_cols)
        if not column_mapping:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ñ–∞–π–ª–∞ {file_type}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {df.columns}")
            return pl.DataFrame()

        df = df.rename(column_mapping)

        # –û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π
        for col in ['artikul', 'brand', 'oe_number']:
            if col in df.columns:
                df = df.with_columns(self.clean_values(pl.col(col)).alias(col))

        key_cols = [col for col in ['oe_number', 'artikul', 'brand'] if col in df.columns]
        if key_cols:
            df = df.unique(subset=key_cols, keep='first')

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π
        for col in ['artikul', 'brand', 'oe_number']:
            if col in df.columns:
                df = df.with_columns(
                    self.normalize_key(pl.col(col)).alias(f"{col}_norm")
                )

        return df

    def upsert_data(self, table_name: str, df: pl.DataFrame, pk: List[str]):
        """UPSERT –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü—É DuckDB"""
        if df.is_empty():
            return

        df = df.unique(keep='first')
        cols = df.columns
        pk_str = ", ".join(f'"{c}"' for c in pk)
        temp_view_name = f"temp_{table_name}_{int(time.time())}"

        self.conn.register(temp_view_name, df.to_arrow())
        update_cols = [col for col in cols if col not in pk]

        if not update_cols:
            on_conflict_action = "DO NOTHING"
        else:
            update_clause = ", ".join([f'"{col}" = excluded."{col}"' for col in update_cols])
            on_conflict_action = f"DO UPDATE SET {update_clause}"

        sql = f"""
            INSERT INTO {table_name}
            SELECT * FROM {temp_view_name}
            ON CONFLICT ({pk_str}) {on_conflict_action};
        """

        try:
            self.conn.execute(sql)
            logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–æ/–≤—Å—Ç–∞–≤–ª–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ UPSERT –≤ {table_name}: {e}")
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}. –î–µ—Ç–∞–ª–∏ –≤ –ª–æ–≥–µ.")
        finally:
            self.conn.unregister(temp_view_name)

    def upsert_prices(self, price_df: pl.DataFrame):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        if price_df.is_empty():
            return

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–π
        if 'artikul' in price_df.columns and 'brand' in price_df.columns:
            price_df = price_df.with_columns([
                self.normalize_key(pl.col('artikul')).alias('artikul_norm'),
                self.normalize_key(pl.col('brand')).alias('brand_norm')
            ])

        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤–∞–ª—é—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if 'currency' not in price_df.columns:
            price_df = price_df.with_columns(pl.lit('RUB').alias('currency'))

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω—É —Ü–µ–Ω
        price_df = price_df.filter(
            (pl.col('price') >= self.price_rules['min_price']) &
            (pl.col('price') <= self.price_rules['max_price'])
        )

        self.upsert_data('prices', price_df, ['artikul_norm', 'brand_norm'])

    def process_and_load_data(self, dataframes: Dict[str, pl.DataFrame]):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—É —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
        st.info("üîÑ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑–µ...")
        steps = [s for s in ['oe', 'cross', 'parts'] if s in dataframes]
    num_steps = len(steps)
    progress_bar = st.progress(0, text="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
    step_counter = 0

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ OE-–¥–∞–Ω–Ω—ã—Ö
if 'oe' in dataframes:
    step_counter += 1
    progress_bar.progress(step_counter / (num_steps + 1), text=f"({step_counter}/{num_steps}) –û–±—Ä–∞–±–æ—Ç–∫–∞ OE –¥–∞–Ω–Ω—ã—Ö...")
    df = dataframes['oe'].filter(pl.col('oe_number_norm') != "")
    oe_df = df.select(['oe_number_norm', 'oe_number', 'name', 'applicability']).unique(subset=['oe_number_norm'], keep='first')

    if 'name' in oe_df.columns:
        oe_df = oe_df.with_columns(self.determine_category_vectorized(pl.col('name')))
    else:
        oe_df = oe_df.with_columns(category=pl.lit('–†–∞–∑–Ω–æ–µ'))

    self.upsert_data('oe_data', oe_df, ['oe_number_norm'])

    cross_df_from_oe = df.filter(pl.col('artikul_norm') != "").select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
    self.upsert_data('cross_references', cross_df_from_oe, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫—Ä–æ—Å—Å–æ–≤
if 'cross' in dataframes:
    step_counter += 1
    progress_bar.progress(step_counter / (num_steps + 1), text=f"({step_counter}/{num_steps}) –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫—Ä–æ—Å—Å–æ–≤...")
    df = dataframes['cross'].filter((pl.col('oe_number_norm') != "") & (pl.col('artikul_norm') != ""))
    cross_df_from_cross = df.select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
    self.upsert_data('cross_references', cross_df_from_cross, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–µ–Ω
if 'prices' in dataframes:
    price_df = dataframes['prices']
    if not price_df.is_empty():
        st.info("üí∞ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–µ–Ω...")
        self.upsert_prices(price_df)
        st.success(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–æ {len(price_df)} —Ü–µ–Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π")

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
step_counter += 1
progress_bar.progress(step_counter / (num_steps + 1), text=f"({step_counter}/{num_steps}) –°–±–æ—Ä–∫–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∞—Ä—Ç–∏–∫—É–ª–∞–º...")

# –û—Å—Ç–∞–≤—à–∞—è—Å—è –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Å—Ç–µ–π...

progress_bar.progress(1.0, text="–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
time.sleep(1)
progress_bar.empty()

def build_export_query(self, selected_columns: Optional[List[str]] = None, include_prices: bool = True, apply_markup: bool = True) -> str:
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ–≥–æ SQL-–∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ü–µ–Ω, –Ω–∞—Ü–µ–Ω–æ–∫ –∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π"""
    standard_description = """–°–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: –Ω–æ–≤—ã–π (–≤ —É–ø–∞–∫–æ–≤–∫–µ). –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã ‚Äî –Ω–∞–¥–µ–∂–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è. –û–±–µ—Å–ø–µ—á—å—Ç–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –¥–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç—å –∏ –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ —Å –ø–æ–º–æ—â—å—é –Ω–∞—à–µ–≥–æ —à–∏—Ä–æ–∫–æ–≥–æ –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–µ–π. –í –Ω–∞—à–µ–º –∫–∞—Ç–∞–ª–æ–≥–µ –≤—ã –Ω–∞–π–¥–µ—Ç–µ —Ç–æ—Ä–º–æ–∑–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, —Ñ–∏–ª—å—Ç—Ä—ã (–º–∞—Å–ª—è–Ω—ã–µ, –≤–æ–∑–¥—É—à–Ω—ã–µ, —Å–∞–ª–æ–Ω–Ω—ã–µ), —Å–≤–µ—á–∏ –∑–∞–∂–∏–≥–∞–Ω–∏—è, —Ä–∞—Å—Ö–æ–¥–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –∞–≤—Ç–æ—Ö–∏–º–∏—é, —ç–ª–µ–∫—Ç—Ä–æ–º–∞—Ç–µ—Ä–∏–∞–ª—ã, –∞–≤—Ç–æ–º–∞—Å–ª–∞, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∞ —Ç–∞–∫–∂–µ –¥—Ä—É–≥–∏–µ –∫–æ–º–ø–ª–µ–∫—Ç—É—é—â–∏–µ, –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ú—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –±—ã—Å—Ç—Ä—É—é –¥–æ—Å—Ç–∞–≤–∫—É, –≤—ã–≥–æ–¥–Ω—ã–µ —Ü–µ–Ω—ã –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –¥–ª—è –ª—é–±–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ ‚Äî –∞–≤—Ç–æ–ª—é–±–∏—Ç–µ–ª—è, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –∏–ª–∏ –∞–≤—Ç–æ—Å–µ—Ä–≤–∏—Å–∞. –í—ã–±–∏—Ä–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –ª—É—á—à–µ–µ ‚Äî –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç –≤–µ–¥—É—â–∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π."""

    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è —Ü–µ–Ω
    price_column = ""
    if include_prices:
        if apply_markup:
            global_markup = self.price_rules['global_markup']
            price_column = f"""
                CASE 
                    WHEN p_brand.brand IS NOT NULL AND pr.price IS NOT NULL 
                    THEN pr.price * (1 + COALESCE(brm.markup, {global_markup}))
                    ELSE pr.price 
                END AS "–¶–µ–Ω–∞",
                COALESCE(pr.currency, 'RUB') AS "–í–∞–ª—é—Ç–∞",
            """
        else:
            price_column = """
                pr.price AS "–¶–µ–Ω–∞",
                COALESCE(pr.currency, 'RUB') AS "–í–∞–ª—é—Ç–∞",
            """
    else:
        price_column = ""

    # –£—Å–ª–æ–≤–∏—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    exclusion_conditions = " OR ".join([f"r.representative_name NOT ILIKE '%{ex}%'" for ex in self.exclusion_rules if ex.strip()])
    exclusion_where = f"AND ({exclusion_conditions})" if exclusion_conditions else ""

    # –ö–∞—Ä—Ç–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –∏ SQL-–≤—ã—Ä–∞–∂–µ–Ω–∏–π
    columns_map = [
        ("–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", 'r.artikul AS "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞"'),
        ("–ë—Ä–µ–Ω–¥", 'r.brand AS "–ë—Ä–µ–Ω–¥"'),
        ("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", 'COALESCE(r.representative_name, r.analog_representative_name) AS "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"'),
        ("–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", 'COALESCE(r.representative_applicability, r.analog_representative_applicability) AS "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å"'),
        ("–û–ø–∏—Å–∞–Ω–∏–µ", "CONCAT(COALESCE(r.description, ''), dt.text) AS \"–û–ø–∏—Å–∞–Ω–∏–µ\""),
        ("–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", 'COALESCE(r.representative_category, r.analog_representative_category) AS "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞"'),
        ("–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", 'r.multiplicity AS "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"'),
        ("–î–ª–∏–Ω–Ω–∞", 'COALESCE(r.length, r.analog_length) AS "–î–ª–∏–Ω–Ω–∞"'),
        ("–®–∏—Ä–∏–Ω–∞", 'COALESCE(r.width, r.analog_width) AS "–®–∏—Ä–∏–Ω–∞"'),
        ("–í—ã—Å–æ—Ç–∞", 'COALESCE(r.height, r.analog_height) AS "–í—ã—Å–æ—Ç–∞"'),
        ("–í–µ—Å", 'COALESCE(r.weight, r.analog_weight) AS "–í–µ—Å"'),
        ("–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", """
            COALESCE(
                CASE 
                    WHEN r.dimensions_str IS NULL OR r.dimensions_str = '' OR UPPER(TRIM(r.dimensions_str)) = 'XX' 
                    THEN NULL 
                    ELSE r.dimensions_str 
                END, 
                r.analog_dimensions_str
            ) AS "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞"
        """),
        ("OE –Ω–æ–º–µ—Ä", 'r.oe_list AS "OE –Ω–æ–º–µ—Ä"'),
        ("–∞–Ω–∞–ª–æ–≥–∏", 'r.analog_list AS "–∞–Ω–∞–ª–æ–≥–∏"'),
        ("–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", 'r.image_url AS "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"')
    ]

    if include_prices:
        columns_map.extend([("–¶–µ–Ω–∞", '"–¶–µ–Ω–∞"'), ("–í–∞–ª—é—Ç–∞", '"–í–∞–ª—é—Ç–∞"')])

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
    if selected_columns:
        selected_exprs = [expr for name, expr in columns_map if name in selected_columns]
    else:
        selected_exprs = [expr for _, expr in columns_map]

    if not selected_exprs:
        selected_exprs = [expr for _, expr in columns_map]

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è CTE –∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    ctes = f"""
        WITH DescriptionTemplate AS (
            SELECT CHR(10) || CHR(10) || $${standard_description}$$ AS text
        ),
        BrandMarkups AS (
            SELECT brand, markup FROM (
                {self._get_brand_markups_sql()}
            ) AS tmp
        ),
        PartDetails AS (
            SELECT 
                cr.artikul_norm, 
                cr.brand_norm,
                STRING_AGG(
                    DISTINCT regexp_replace(
                        regexp_replace(o.oe_number, '''', ''), 
                        '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]', '', 'g'
                    ), 
                    ', '
                ) AS oe_list,
                ANY_VALUE(o.name) AS representative_name,
                ANY_VALUE(o.applicability) AS representative_applicability,
                ANY_VALUE(o.category) AS representative_category
            FROM cross_references cr
            JOIN oe_data o ON cr.oe_number_norm = o.oe_number_norm
            GROUP BY cr.artikul_norm, cr.brand_norm
        ),
        AllAnalogs AS (
            SELECT 
                cr1.artikul_norm, 
                cr1.brand_norm,
                STRING_AGG(
                    DISTINCT regexp_replace(
                        regexp_replace(p2.artikul, '''', ''), 
                        '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]', '', 'g'
                    ), 
                    ', '
                ) AS analog_list
            FROM cross_references cr1
            JOIN cross_references cr2 ON cr1.oe_number_norm = cr2.oe_number_norm
            JOIN parts_data p2 ON cr2.artikul_norm = p2.artikul_norm AND cr2.brand_norm = p2.brand_norm
            WHERE (cr1.artikul_norm != p2.artikul_norm OR cr1.brand_norm != p2.brand_norm)
            GROUP BY cr1.artikul_norm, cr1.brand_norm
        ),
        InitialOENumbers AS (
            SELECT DISTINCT p.artikul_norm, p.brand_norm, cr.oe_number_norm
            FROM parts_data p
            LEFT JOIN cross_references cr ON p.artikul_norm = cr.artikul_norm AND p.brand_norm = cr.brand_norm
            WHERE cr.oe_number_norm IS NOT NULL
        ),
        Level1Analogs AS (
            SELECT DISTINCT 
                i.artikul_norm AS source_artikul_norm, 
                i.brand_norm AS source_brand_norm,
                cr2.artikul_norm AS related_artikul_norm, 
                cr2.brand_norm AS related_brand_norm
            FROM InitialOENumbers i
            JOIN cross_references cr2 ON i.oe_number_norm = cr2.oe_number_norm
            WHERE NOT (i.artikul_norm = cr2.artikul_norm AND i.brand_norm = cr2.brand_norm)
        ),
        Level1OENumbers AS (
            SELECT DISTINCT 
                l1.source_artikul_norm, 
                l1.source_brand_norm, 
                cr3.oe_number_norm
            FROM Level1Analogs l1
            JOIN cross_references cr3 ON l1.related_artikul_norm = cr3.artikul_norm AND l1.related_brand_norm = cr3.brand_norm
            WHERE NOT EXISTS (
                SELECT 1 FROM InitialOENumbers i
                WHERE i.artikul_norm = l1.source_artikul_norm 
                  AND i.brand_norm = l1.source_brand_norm 
                  AND i.oe_number_norm = cr3.oe_number_norm
            )
        ),
        Level2Analogs AS (
            SELECT DISTINCT 
                loe.source_artikul_norm, 
                loe.source_brand_norm,
                cr4.artikul_norm AS related_artikul_norm, 
                cr4.brand_norm AS related_brand_norm
            FROM Level1OENumbers loe
            JOIN cross_references cr4 ON loe.oe_number_norm = cr4.oe_number_norm
            WHERE NOT (loe.source_artikul_norm = cr4.artikul_norm AND loe.source_brand_norm = cr4.brand_norm)
        ),
        AllRelatedParts AS (
            SELECT source_artikul_norm, source_brand_norm, related_artikul_norm, related_brand_norm
            FROM Level1Analogs
            UNION
            SELECT source_artikul_norm, source_brand_norm, related_artikul_norm, related_brand_norm
            FROM Level2Analogs
        ),
        AggregatedAnalogData AS (
            SELECT 
                arp.source_artikul_norm AS artikul_norm,
                arp.source_brand_norm AS brand_norm,
                MAX(CASE WHEN p2.length IS NOT NULL THEN p2.length ELSE NULL END) AS length,
                MAX(CASE WHEN p2.width IS NOT NULL THEN p2.width ELSE NULL END) AS width,
                MAX(CASE WHEN p2.height IS NOT NULL THEN p2.height ELSE NULL END) AS height,
                MAX(CASE WHEN p2.weight IS NOT NULL THEN p2.weight ELSE NULL END) AS weight,
                ANY_VALUE(
                    CASE 
                        WHEN p2.dimensions_str IS NOT NULL 
                         AND p2.dimensions_str != '' 
                         AND UPPER(TRIM(p2.dimensions_str)) != 'XX' 
                        THEN p2.dimensions_str 
                        ELSE NULL 
                    END
                ) AS dimensions_str,
                ANY_VALUE(
                    CASE 
                        WHEN pd2.representative_name IS NOT NULL AND pd2.representative_name != '' 
                        THEN pd2.representative_name 
                        ELSE NULL 
                    END
                ) AS representative_name,
                ANY_VALUE(
                    CASE 
                        WHEN pd2.representative_applicability IS NOT NULL AND pd2.representative_applicability != '' 
                        THEN pd2.representative_applicability 
                        ELSE NULL 
                    END
                ) AS representative_applicability,
                ANY_VALUE(
                    CASE 
                        WHEN pd2.representative_category IS NOT NULL AND pd2.representative_category != '' 
                        THEN pd2.representative_category 
                        ELSE NULL 
                    END
                ) AS representative_category
            FROM AllRelatedParts arp
            JOIN parts_data p2 ON arp.related_artikul_norm = p2.artikul_norm AND arp.related_brand_norm = p2.brand_norm
            LEFT JOIN PartDetails pd2 ON p2.artikul_norm = pd2.artikul_norm AND p2.brand_norm = pd2.brand_norm
            GROUP BY arp.source_artikul_norm, arp.source_brand_norm
        ),
        RankedData AS (
            SELECT 
                p.artikul,
                p.brand,
                p.description,
                p.multiplicity,
                p.length,
                p.width,
                p.height,
                p.weight,
                p.dimensions_str,
                p.image_url,
                pd.representative_name,
                pd.representative_applicability,
                pd.representative_category,
                pd.oe_list,
                aa.analog_list,
                p_analog.length AS analog_length,
                p_analog.width AS analog_width,
                p_analog.height AS analog_height,
                p_analog.weight AS analog_weight,
                p_analog.dimensions_str AS analog_dimensions_str,
                p_analog.representative_name AS analog_representative_name,
                p_analog.representative_applicability AS analog_representative_applicability,
                p_analog.representative_category AS analog_representative_category,
                ROW_NUMBER() OVER (
                    PARTITION BY p.artikul_norm, p.brand_norm 
                    ORDER BY pd.representative_name DESC NULLS LAST, pd.oe_list DESC NULLS LAST
                ) AS rn
            FROM parts_data p
            LEFT JOIN PartDetails pd ON p.artikul_norm = pd.artikul_norm AND p.brand_norm = pd.brand_norm
            LEFT JOIN AllAnalogs aa ON p.artikul_norm = aa.artikul_norm AND p.brand_norm = aa.brand_norm
            LEFT JOIN AggregatedAnalogData p_analog ON p.artikul_norm = p_analog.artikul_norm AND p.brand_norm = p_analog.brand_norm
        )
    """

    select_clause = ",\n        ".join(selected_exprs)

    price_join = """
        LEFT JOIN prices pr ON r.artikul_norm = pr.artikul_norm AND r.brand_norm = pr.brand_norm
        LEFT JOIN BrandMarkups brm ON r.brand = brm.brand
    """ if include_prices else ""

    query = f"""
        {ctes}
        SELECT 
            {price_column}
            {select_clause}
        FROM RankedData r
        CROSS JOIN DescriptionTemplate dt
        {price_join}
        WHERE r.rn = 1
        {exclusion_where}
        ORDER BY r.brand, r.artikul
    """

    return query.strip()

def _get_brand_markups_sql(self) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SQL-–ø–æ–¥–∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –Ω–∞—Ü–µ–Ω–æ–∫ –ø–æ –±—Ä–µ–Ω–¥–∞–º"""
    rows = []
    for brand, markup in self.price_rules['brand_markups'].items():
        rows.append(f"SELECT '{brand}' AS brand, {markup} AS markup")
    return " UNION ALL ".join(rows) if rows else "SELECT NULL AS brand, NULL AS markup LIMIT 0"

def export_to_csv_optimized(self, output_path: str, selected_columns: Optional[List[str]] = None, include_prices: bool = True, apply_markup: bool = True) -> bool:
    """–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤ CSV —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π —Ç–∏–ø–æ–≤ –∏ —Ä–∞–∑–º–µ—Ä–∞"""
    total_records = self.conn.execute("""
        SELECT count(*) 
        FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data) AS t
    """).fetchone()[0]

    if total_records == 0:
        st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
        return False

    st.info(f"üì§ –≠–∫—Å–ø–æ—Ä—Ç {total_records:,} –∑–∞–ø–∏—Å–µ–π –≤ CSV...")
    try:
        query = self.build_export_query(selected_columns, include_prices, apply_markup)
        df = self.conn.execute(query).pl()

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ —Å—Ç—Ä–æ–∫–∏
        dimension_cols = ["–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞", "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"]
        for col in dimension_cols:
            if col in df.columns:
                df = df.with_columns(
                    pl.when(pl.col(col).is_not_null())
                     .then(pl.col(col).cast(pl.Utf8))
                     .otherwise(pl.lit(""))
                     .alias(col)
                )

        # –ó–∞–ø–∏—Å—å –≤ CSV —Å BOM –¥–ª—è Excel
        buf = io.StringIO()
        df.write_csv(buf, separator=';')
        csv_text = buf.getvalue()

        with open(output_path, 'wb') as f:
            f.write(b'\xef\xbb\xbf')  # UTF-8 BOM
            f.write(csv_text.encode('utf-8'))

        file_size = os.path.getsize(output_path) / (1024 * 1024)
        st.success(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ CSV: {output_path} ({file_size:.1f} –ú–ë)")
        return True

    except Exception as e:
        logger.exception("–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ CSV")
        st.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ CSV: {e}")
        return False

def show_price_settings(self):
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ü–µ–Ω –∏ –Ω–∞—Ü–µ–Ω–æ–∫"""
    st.header("üí∞ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω–∞–º–∏ –∏ –Ω–∞—Ü–µ–Ω–∫–∞–º–∏")

    # –û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞
st.subheader("–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞")
global_markup = st.number_input(
    "–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞ (%):",
    min_value=0.0,
    max_value=100.0,
    value=self.price_rules['global_markup'] * 100,
    step=0.1
)
self.price_rules['global_markup'] = global_markup / 100

       # –ù–∞—Ü–µ–Ω–∫–∏ –ø–æ –±—Ä–µ–Ω–¥–∞–º
st.subheader("–ù–∞—Ü–µ–Ω–∫–∏ –ø–æ –±—Ä–µ–Ω–¥–∞–º")
brand_markups = self.price_rules.get('brand_markups', {})

try:
    brands_result = self.conn.execute("SELECT DISTINCT brand FROM parts_data WHERE brand IS NOT NULL ORDER BY brand").fetchall()
    available_brands = [row[0] for row in brands_result] if brands_result else []
except Exception as e:
    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –±—Ä–µ–Ω–¥–æ–≤: {e}")
    st.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –±—Ä–µ–Ω–¥–æ–≤")
    available_brands = []

if available_brands:
    col1, col2 = st.columns([2, 1])
    with col1:
        selected_brand = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –±—Ä–µ–Ω–¥:", available_brands)
    with col2:
        current_markup = brand_markups.get(selected_brand, self.price_rules.get('global_markup', 0))
        brand_markup = st.number_input(
            "–ù–∞—Ü–µ–Ω–∫–∞ (%):",
            min_value=0.0,
            max_value=100.0,
            value=current_markup * 100,
            step=0.1,
            key=f"markup_{selected_brand}"
        )
    if st.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞—Ü–µ–Ω–∫—É", key=f"save_{selected_brand}"):
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ª–æ–≤–∞—Ä—å –Ω–∞—Ü–µ–Ω–æ–∫
        brand_markups[selected_brand] = brand_markup / 100
        self.price_rules['brand_markups'] = brand_markups
        self.save_price_rules()
        st.success(f"‚úÖ –ù–∞—Ü–µ–Ω–∫–∞ –¥–ª—è {selected_brand} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
        
    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ü–µ–Ω
    st.subheader("–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ —Ü–µ–Ω–∞–º")
    col1, col2 = st.columns(2)
    with col1:
        min_price = st.number_input("–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞:", min_value=0.0, value=float(self.price_rules['min_price']), step=0.01)
        self.price_rules['min_price'] = min_price
    with col2:
        max_price = st.number_input("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞:", min_value=0.0, value=float(self.price_rules['max_price']), step=0.01)
        self.price_rules['max_price'] = max_price

    if st.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ü–µ–Ω"):
        self.save_price_rules()
        st.success("‚úÖ –í—Å–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ü–µ–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")

def show_exclusion_settings(self):
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–ø–∏—Å–∫–æ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–π –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ"""
    st.header("üö´ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è–º–∏ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ")
    st.info("–¢–æ–≤–∞—Ä—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —ç—Ç–∏ —Å–ª–æ–≤–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏, –±—É–¥—É—Ç –∏—Å–∫–ª—é—á–µ–Ω—ã –∏–∑ —ç–∫—Å–ø–æ—Ä—Ç–∞")

    current_exclusions = "\n".join(self.exclusion_rules)
    new_exclusions = st.text_area(
        "–°–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É):",
        value=current_exclusions,
        height=200,
        placeholder="–í–≤–µ–¥–∏—Ç–µ —Å–ª–æ–≤–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä:\n–ö—É–∑–æ–≤\n–°—Ç–µ–∫–ª–∞\n–ú–∞—Å–ª–∞"
    )

    if st.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏—è"):
        # –û—á–∏—Å—Ç–∫–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤–≤–æ–¥–∞
        cleaned = [line.strip() for line in new_exclusions.splitlines() if line.strip()]
        if len(cleaned) != len(set(cleaned)):
            st.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –∑–∞–ø–∏—Å–∏. –û–Ω–∏ –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–¥–∞–ª–µ–Ω—ã.")
        self.exclusion_rules = list(dict.fromkeys(cleaned))  # –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ—Ä—è–¥–æ–∫
        self.save_exclusion_rules()
        st.success("‚úÖ –ü—Ä–∞–≤–∏–ª–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")

def show_category_mapping(self):
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
    st.header("üóÇÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ —Ç–æ–≤–∞—Ä–æ–≤")
    st.info("–ù–∞—Å—Ç—Ä–æ–π—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ —Ç–æ–≤–∞—Ä–æ–≤ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏")

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö –ø—Ä–∞–≤–∏–ª
    st.subheader("–¢–µ–∫—É—â–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏")
    if self.category_mapping:
        mapping_df = pl.DataFrame({
            "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞": list(self.category_mapping.keys()),
            "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": list(self.category_mapping.values())
        }).to_pandas()
        st.dataframe(mapping_df, use_container_width=True, hide_index=True)
    else:
        st.write("–ù–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–∞–≤–∏–ª –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏")

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞
    st.subheader("–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤–æ–µ –ø—Ä–∞–≤–∏–ª–æ")
    col1, col2 = st.columns(2)
    with col1:
        name_pattern = st.text_input("–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏:")
    with col2:
        category = st.text_input("–ö–∞—Ç–µ–≥–æ—Ä–∏—è:")

    if st.button("‚ûï –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∞–≤–∏–ª–æ"):
        if name_pattern.strip() and category.strip():
            # –†–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–µ–π
            normalized_key = name_pattern.strip().lower()
            existing_keys = {k.lower(): k for k in self.category_mapping.keys()}
            if normalized_key in existing_keys:
                st.warning(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ø—Ä–∞–≤–∏–ª–æ –¥–ª—è '{existing_keys[normalized_key]}' –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ")
            self.category_mapping[name_pattern.strip()] = category.strip()
            self.save_category_mapping()
            st.success(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ/–æ–±–Ω–æ–≤–ª–µ–Ω–æ –ø—Ä–∞–≤–∏–ª–æ: `{name_pattern.strip()}` ‚Üí `{category.strip()}`")
            st.rerun()
        else:
            st.error("‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –æ–±–∞ –ø–æ–ª—è")

    # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞
    if self.category_mapping:
        st.subheader("üóëÔ∏è –£–¥–∞–ª–∏—Ç—å –ø—Ä–∞–≤–∏–ª–æ")
        rule_to_delete = st.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∞–≤–∏–ª–æ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:",
            options=list(self.category_mapping.keys()),
            format_func=lambda x: f"{x} ‚Üí {self.category_mapping[x]}"
        )
        if st.button("–£–¥–∞–ª–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ", type="primary"):
            del self.category_mapping[rule_to_delete]
            self.save_category_mapping()
            st.success(f"‚úÖ –ü—Ä–∞–≤–∏–ª–æ —É–¥–∞–ª–µ–Ω–æ: `{rule_to_delete}`")
            st.rerun()

def show_data_management(self):
    """–û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã–º–∏: —É–¥–∞–ª–µ–Ω–∏–µ, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞, —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è"""
    st.header("üîß –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏ –≤ –±–∞–∑–µ")
    st.warning("‚ö†Ô∏è –û–ø–µ—Ä–∞—Ü–∏–∏ —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ–æ–±—Ä–∞—Ç–∏–º—ã. –ë—É–¥—å—Ç–µ –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã.")

    management_option = st.radio(
        "–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:",
        [
            "–£–¥–∞–ª–∏—Ç—å –ø–æ –±—Ä–µ–Ω–¥—É",
            "–£–¥–∞–ª–∏—Ç—å –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É",
            "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω–∞–º–∏",
            "–ò—Å–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ",
            "–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–æ–≤–∞—Ä–æ–≤",
            "–û–±–ª–∞—á–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è"
        ],
        format_func=lambda x: {
            "–£–¥–∞–ª–∏—Ç—å –ø–æ –±—Ä–µ–Ω–¥—É": "üè≠ –£–¥–∞–ª–∏—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏ –±—Ä–µ–Ω–¥–∞",
            "–£–¥–∞–ª–∏—Ç—å –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É": "üì¶ –£–¥–∞–ª–∏—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏ –∞—Ä—Ç–∏–∫—É–ª–∞",
            "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω–∞–º–∏": "üí∞ –ù–∞—Ü–µ–Ω–∫–∏ –∏ –ª–∏–º–∏—Ç—ã —Ü–µ–Ω",
            "–ò—Å–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ": "üö´ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ",
            "–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–æ–≤–∞—Ä–æ–≤": "üóÇÔ∏è –†—É—á–Ω–æ–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π",
            "–û–±–ª–∞—á–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è": "‚òÅÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±—ç–∫–∞–ø–∞"
        }[x]
    )

    if management_option == "–£–¥–∞–ª–∏—Ç—å –ø–æ –±—Ä–µ–Ω–¥—É":
        self._show_delete_by_brand()
    elif management_option == "–£–¥–∞–ª–∏—Ç—å –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É":
        self._show_delete_by_artikul()
    elif management_option == "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω–∞–º–∏":
        self.show_price_settings()
    elif management_option == "–ò—Å–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ":
        self.show_exclusion_settings()
    elif management_option == "–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–æ–≤–∞—Ä–æ–≤":
        self.show_category_mapping()
    elif management_option == "–û–±–ª–∞—á–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è":
        self.show_cloud_sync()

def _show_delete_by_brand(self):
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å —É–¥–∞–ª–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π –ø–æ –±—Ä–µ–Ω–¥—É"""
    st.subheader("üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π –±—Ä–µ–Ω–¥–∞")
    try:
        brands_result = self.conn.execute("""
            SELECT DISTINCT brand 
            FROM parts_data 
            WHERE brand IS NOT NULL 
            ORDER BY brand
        """).fetchall()
        available_brands = [row[0] for row in brands_result] if brands_result else []
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –±—Ä–µ–Ω–¥–æ–≤: {e}")
        st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø–∏—Å–æ–∫ –±—Ä–µ–Ω–¥–æ–≤")
        return

    if not available_brands:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –±—Ä–µ–Ω–¥–∞—Ö –≤ –±–∞–∑–µ.")
        return

    selected_brand = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –±—Ä–µ–Ω–¥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:", available_brands)

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–ª—é—á–∞
    brand_norm_result = self.conn.execute("""
        SELECT brand_norm FROM parts_data WHERE brand = ? LIMIT 1
    """, [selected_brand]).fetchone()
    if brand_norm_result:
        brand_norm = brand_norm_result[0]
    else:
        brand_norm = self.normalize_key(pl.Series([selected_brand]))[0]

    # –ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π
    count_result = self.conn.execute("""
        SELECT COUNT(*) FROM parts_data WHERE brand_norm = ?
    """, [brand_norm]).fetchone()
    count_to_delete = count_result[0] if count_result else 0

    st.info(f"–ë—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ: **{count_to_delete}** –∑–∞–ø–∏—Å–µ–π –±—Ä–µ–Ω–¥–∞ `{selected_brand}`")

    confirm = st.checkbox("–Ø –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é —É–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π —ç—Ç–æ–≥–æ –±—Ä–µ–Ω–¥–∞", key=f"confirm_{selected_brand}")
    if st.button("‚ùå –£–¥–∞–ª–∏—Ç—å –±—Ä–µ–Ω–¥", type="primary", disabled=not confirm):
        try:
            deleted = self.delete_by_brand(brand_norm)
            st.success(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–æ {deleted} –∑–∞–ø–∏—Å–µ–π –±—Ä–µ–Ω–¥–∞ `{selected_brand}`")
            st.rerun()
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {e}")

def _show_delete_by_artikul(self):
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å —É–¥–∞–ª–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É"""
    st.subheader("üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π –∞—Ä—Ç–∏–∫—É–ª–∞")
    st.info("üîç –ü–æ–∏—Å–∫ –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É (–±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤)")

    input_artikul = st.text_input("–í–≤–µ–¥–∏—Ç–µ –∞—Ä—Ç–∏–∫—É–ª –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:")

    if input_artikul:
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞—Ä—Ç–∏–∫—É–ª–∞
        artikul_series = pl.Series([input_artikul])
        artikul_norm = self.normalize_key(artikul_series)[0]

        # –ü–æ–¥—Å—á–µ—Ç –∑–∞–ø–∏—Å–µ–π
        count_result = self.conn.execute("""
            SELECT COUNT(*) FROM parts_data WHERE artikul_norm = ?
        """, [artikul_norm]).fetchone()
        count_to_delete = count_result[0] if count_result else 0

        col1, col2 = st.columns([3, 1])
        with col1:
            if count_to_delete > 0:
                st.info(f"–ù–∞–π–¥–µ–Ω–æ: **{count_to_delete}** –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞—Ä—Ç–∏–∫—É–ª–∞ `{input_artikul}`")
            else:
                st.warning(f"–ê—Ä—Ç–∏–∫—É–ª `{input_artikul}` –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –±–∞–∑–µ.")
        with col2:
            if count_to_delete > 0:
                confirm = st.checkbox("–ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å", key=f"confirm_art_{artikul_norm}")
                if st.button("–£–¥–∞–ª–∏—Ç—å", type="primary", disabled=not confirm):
                    try:
                        deleted = self.delete_by_artikul(artikul_norm)
                        st.success(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–æ {deleted} –∑–∞–ø–∏—Å–µ–π –∞—Ä—Ç–∏–∫—É–ª–∞ `{input_artikul}`")
                        st.rerun()
                    except Exception as e:
                        st.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")

def delete_by_brand(self, brand_norm: str) -> int:
    """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É –±—Ä–µ–Ω–¥—É"""
    with self.conn.transaction():
        # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü
        deleted = self.conn.execute("""
            DELETE FROM parts_data WHERE brand_norm = ?
        """, [brand_norm]).rowcount

        self.conn.execute("""
            DELETE FROM cross_references
            WHERE brand_norm = ?
        """, [brand_norm])

        return deleted

def delete_by_artikul(self, artikul_norm: str) -> int:
    """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É –∞—Ä—Ç–∏–∫—É–ª—É"""
    with self.conn.transaction():
        deleted = self.conn.execute("""
            DELETE FROM parts_data WHERE artikul_norm = ?
        """, [artikul_norm]).rowcount

        self.conn.execute("""
            DELETE FROM cross_references
            WHERE artikul_norm = ?
        """, [artikul_norm])

        return deleted

def show_cloud_sync(self):
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±–ª–∞—á–Ω–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏"""
    st.header("‚òÅÔ∏è –û–±–ª–∞—á–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    st.subheader("üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è")
    col1, col2 = st.columns(2)
    with col1:
        self.cloud_config['enabled'] = st.checkbox(
            "–í–∫–ª—é—á–∏—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é",
            value=self.cloud_config['enabled']
        )
    with col2:
        providers = ["s3", "gcs", "azure"]
        current_idx = providers.index(self.cloud_config['provider']) if self.cloud_config['provider'] in providers else 0
        self.cloud_config['provider'] = st.selectbox("–ü—Ä–æ–≤–∞–π–¥–µ—Ä", providers, index=current_idx)

    self.cloud_config['bucket'] = st.text_input("Bucket / Container", value=self.cloud_config['bucket'])
    self.cloud_config['region'] = st.text_input("–†–µ–≥–∏–æ–Ω", value=self.cloud_config['region'])
    self.cloud_config['sync_interval'] = st.number_input(
        "–ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ (—Å–µ–∫—É–Ω–¥—ã)",
        min_value=300,
        max_value=86400,
        value=int(self.cloud_config['sync_interval'])
    )

    if st.button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"):
        self.save_cloud_config()
        st.success("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

    # –°–æ—Å—Ç–æ—è–Ω–∏–µ
    st.subheader("üìä –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ")
    if self.cloud_config['last_sync'] > 0:
        last_sync_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(self.cloud_config['last_sync']))
        st.info(f"–ü–æ—Å–ª–µ–¥–Ω—è—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è: {last_sync_str}")
    else:
        st.info("–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –µ—â—ë –Ω–µ –≤—ã–ø–æ–ª–Ω—è–ª–∞—Å—å")

    if st.button("üîÑ –í—ã–ø–æ–ª–Ω–∏—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é —Å–µ–π—á–∞—Å"):
        self.perform_cloud_sync()

def perform_cloud_sync(self):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ —Å –æ–±–ª–∞–∫–æ–º (–∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏)"""
    if not self.cloud_config['enabled']:
        st.warning("‚ùå –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö")
        return
    if not self.cloud_config['bucket']:
        st.error("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω bucket")
        return

    with st.spinner("–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è..."):
        try:
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º –æ–±–ª–∞–∫–∞ (boto3, google-cloud-storage –∏ –¥—Ä.)
            time.sleep(1.5)  # –ò–º–∏—Ç–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–∫–∏
            st.success(f"üì§ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤ {self.cloud_config['provider']}://{self.cloud_config['bucket']}")
            self.cloud_config['last_sync'] = int(time.time())
            self.save_cloud_config()
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏: {str(e)}")

def show_export_interface(self):
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –≤ CSV/Excel/Parquet"""
    st.header("üì§ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")

    total_records = self.conn.execute("""
        SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)
    """).fetchone()[0]
    st.info(f"üì¶ –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä (–∞—Ä—Ç–∏–∫—É–ª + –±—Ä–µ–Ω–¥): **{total_records:,}**")

    if total_records == 0:
        st.warning("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç–∞. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ —ç–∫—Å–ø–æ—Ä—Ç–æ–º.")
        return

    # –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    available_columns = [
        "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", "–ë—Ä–µ–Ω–¥", "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", "–û–ø–∏—Å–∞–Ω–∏–µ",
        "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", "–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞", "–í–µ—Å",
        "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "OE –Ω–æ–º–µ—Ä", "–∞–Ω–∞–ª–æ–≥–∏", "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
    ]

    # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ü–µ–Ω
    prices_count = self.conn.execute("SELECT COUNT(*) FROM prices").fetchone()[0]
    if prices_count > 0:
        available_columns.extend(["–¶–µ–Ω–∞", "–í–∞–ª—é—Ç–∞"])

    selected_columns = st.multiselect(
        "–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞",
        options=available_columns,
        default=available_columns
    )

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–∫—Å–ø–æ—Ä—Ç–∞
    col1, col2 = st.columns(2)
    with col1:
        export_format = st.radio("–§–æ—Ä–º–∞—Ç", ["CSV", "Excel (.xlsx)", "Parquet"])
    with col2:
        include_prices = st.checkbox("–í–∫–ª—é—á–∏—Ç—å —Ü–µ–Ω—ã", value=True)
        apply_markup = st.checkbox("–ü—Ä–∏–º–µ–Ω–∏—Ç—å –Ω–∞—Ü–µ–Ω–∫—É", value=True, disabled=not include_prices)

    # –ö–Ω–æ–ø–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞
    if st.button("üöÄ –í—ã–ø–æ–ª–Ω–∏—Ç—å —ç–∫—Å–ø–æ—Ä—Ç", type="primary"):
        output_path = self.data_dir / f"auto_parts_export.{export_format.lower().replace(' ', '_')}"
        with st.spinner("–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞..."):
            if export_format == "CSV":
                success = self.export_to_csv_optimized(
                    str(output_path),
                    selected_columns if selected_columns else None,
                    include_prices,
                    apply_markup
                )
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Excel –∏ Parquet
            else:
                st.warning(f"–§–æ—Ä–º–∞—Ç {export_format} –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è. –í—ã–±–µ—Ä–∏—Ç–µ CSV.")
                success = False

            if success:
                with open(output_path, "rb") as f:
                    st.download_button(
                        "‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª",
                        f,
                        file_name=output_path.name,
                        mime="application/octet-stream"
                    )

def export_to_excel_optimized(self, output_path: str, selected_columns: Optional[List[str]] = None, include_prices: bool = True, apply_markup: bool = True) -> bool:
    """–≠–∫—Å–ø–æ—Ä—Ç –≤ Excel (.xlsx) —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –ª–∏—Å—Ç—ã –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞ —Å—Ç—Ä–æ–∫"""
    total_records = self.conn.execute("""
        SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)
    """).fetchone()[0]

    if total_records == 0:
        st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Excel")
        return False

    st.info(f"üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Excel: {total_records:,} –∑–∞–ø–∏—Å–µ–π...")

try:
    import pandas as pd
    query = self.build_export_query(selected_columns, include_prices, apply_markup)
    df = pd.read_sql(query, self.conn)
    
    # –î–∞–ª–µ–µ –≤–∞—à –∫–æ–¥ –ø–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—é df –≤ Excel
except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
    
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–∞–∑–º–µ—Ä–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ —Å—Ç—Ä–æ–∫–∏
dimension_cols = ["–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞", "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"]
expressions = []

for col in dimension_cols:
    if col in df.columns:
        expressions.append(
            pl.when(pl.col(col).is_not_null())
              .then(pl.col(col).cast(pl.Utf8))
              .otherwise("")
              .alias(col)
        )

if expressions:
    df = df.with_columns(expressions)
    
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ pandas (—Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è openpyxl)
    pdf = df.to_pandas()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ª–∏–º–∏—Ç Excel
    if len(pdf) <= EXCEL_ROW_LIMIT:
                # –ü—Ä–æ—Å—Ç–æ–π —ç–∫—Å–ø–æ—Ä—Ç –≤ –æ–¥–∏–Ω –ª–∏—Å—Ç
                with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                    pdf.to_excel(writer, index=False, sheet_name='–î–∞–Ω–Ω—ã–µ')
    else:
                # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ª–∏—Å—Ç–æ–≤
                num_sheets = (len(pdf) // EXCEL_ROW_LIMIT) + 1
                st.warning(f"–ó–∞–ø–∏—Å–µ–π –±–æ–ª—å—à–µ {EXCEL_ROW_LIMIT}, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–æ {num_sheets} –ª–∏—Å—Ç–æ–≤")

                with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                    for i in range(num_sheets):
                        start_idx = i * EXCEL_ROW_LIMIT
                        end_idx = min((i + 1) * EXCEL_ROW_LIMIT, len(pdf))
                        chunk = pdf.iloc[start_idx:end_idx]
                        sheet_name = f"–î–∞–Ω–Ω—ã–µ_{i + 1}"
                        chunk.to_excel(writer, index=False, sheet_name=sheet_name)

     file_size = os.path.getsize(output_path) / (1024 * 1024)
            st.success(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ Excel: {output_path} ({file_size:.1f} –ú–ë)")
            return True

        except Exception as e:
            logger.exception("–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Excel")
            st.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Excel: {e}")
            return False

    def export_to_parquet(self, output_path: str, selected_columns: Optional[List[str]] = None, include_prices: bool = True, apply_markup: bool = True) -> bool:
        """–≠–∫—Å–ø–æ—Ä—Ç –≤ Parquet ‚Äî –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
        st.info("üì¶ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Parquet...")

        try:
            query = self.build_export_query(selected_columns, include_prices, apply_markup)
            df = self.conn.execute(query).pl()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º Polars –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Parquet

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ Polars
            df.write_parquet(output_path)

            file_size = os.path.getsize(output_path) / (1024 * 1024)
            st.success(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ Parquet: {output_path} ({file_size:.1f} –ú–ë)")
            return True

        except Exception as e:
            logger.exception("–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Parquet")
            st.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Parquet: {e}")
            return False

    def show_statistics(self):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        st.header("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")

        stats = {}
        try:
            stats['parts'] = self.conn.execute("SELECT COUNT(*) FROM parts_data").fetchone()[0]
            stats['oe'] = self.conn.execute("SELECT COUNT(*) FROM oe_data").fetchone()[0]
            stats['cross'] = self.conn.execute("SELECT COUNT(*) FROM cross_references").fetchone()[0]
            stats['prices'] = self.conn.execute("SELECT COUNT(*) FROM prices").fetchone()[0]
            stats['brands'] = self.conn.execute("SELECT COUNT(DISTINCT brand) FROM parts_data").fetchone()[0]
            stats['unique_parts'] = self.conn.execute("""
                SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)
            """).fetchone()[0]

            avg_price_result = self.conn.execute("SELECT AVG(price) FROM prices WHERE price IS NOT NULL").fetchone()
            stats['avg_price'] = round(avg_price_result[0], 2) if avg_price_result and avg_price_result[0] else 0.0

        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return

        # –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫
        col1, col2, col3 = st.columns(3)
        col1.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã", f"{stats['unique_parts']:,}")
        col2.metric("–ë—Ä–µ–Ω–¥—ã", f"{stats['brands']:,}")
        col3.metric("–°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞", f"{stats['avg_price']} ‚ÇΩ")

        col1, col2, col3 = st.columns(3)
        col1.metric("–ó–∞–ø–∏—Å–∏ (parts)", f"{stats['parts']:,}")
        col2.metric("OE-–Ω–æ–º–µ—Ä–∞", f"{stats['oe']:,}")
        col3.metric("–ö—Ä–æ—Å—Å—ã", f"{stats['cross']:,}")

        col1, col2 = st.columns(2)
        col1.metric("–¶–µ–Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏", f"{stats['prices']:,}")
        col2.metric("–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –ë–î", f"{os.path.getsize(self.db_path) / (1024**2):.1f} –ú–ë")

        # –¢–æ–ø –±—Ä–µ–Ω–¥–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É
        st.subheader("üèÜ –¢–æ–ø-10 –±—Ä–µ–Ω–¥–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∞—Ä—Ç–∏–∫—É–ª–æ–≤")
        try:
            top_brands = self.conn.execute("""
                SELECT brand, COUNT(*) as cnt
                FROM parts_data
                WHERE brand IS NOT NULL
                GROUP BY brand
                ORDER BY cnt DESC
                LIMIT 10
            """).pl()
            st.dataframe(top_brands.to_pandas(), use_container_width=True)
        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–ø –±—Ä–µ–Ω–¥–æ–≤: {e}")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (–µ—Å–ª–∏ –µ—Å—Ç—å)
        st.subheader("üóÇÔ∏è –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
        try:
            category_stats = self.conn.execute("""
                SELECT 
                    COALESCE(representative_category, '–†–∞–∑–Ω–æ–µ') as category,
                    COUNT(*) as cnt
                FROM (
                    SELECT DISTINCT p.artikul_norm, p.brand_norm, pd.representative_category
                    FROM parts_data p
                    LEFT JOIN part_details_view pd ON p.artikul_norm = pd.artikul_norm AND p.brand_norm = pd.brand_norm
                )
                GROUP BY category
                ORDER BY cnt DESC
                LIMIT 15
            """).pl()
            st.dataframe(category_stats.to_pandas(), use_container_width=True)
        except Exception as e:
            st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")

    def merge_all_data_parallel(self, file_paths: Dict[str, str], max_workers: int = 4) -> Dict[str, pl.DataFrame]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        """
        results = {}
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {}
            for file_type, file_path in file_paths.items():
                if file_path and os.path.exists(file_path):
                    future = executor.submit(self.read_and_prepare_file, file_path, file_type)
                    futures[future] = file_type

            for future in as_completed(futures):
                file_type = futures[future]
                try:
                    df = future.result()
                    if not df.is_empty():
                        results[file_type] = df
                        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω —Ñ–∞–π–ª: {file_type}")
                    else:
                        logger.warning(f"‚ö†Ô∏è –§–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {file_type}")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_type}: {e}")

        return results


def main():
    st.title("üöó AutoParts Catalog ‚Äî –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è 10+ –º–ª–Ω –∑–∞–ø–∏—Å–µ–π")
    st.markdown("""
    ### üíº –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–∞—Ç–∞–ª–æ–≥–∞–º–∏ –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–µ–π
    - **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö**: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –∑–∞–ø–∏—Å–µ–π.
    - **–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è**: –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö.
    - **–ú—É–ª—å—Ç–∏—Ñ–æ—Ä–º–∞—Ç–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç**: CSV, Excel, Parquet ‚Äî –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
    - **–ì–∏–±–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞**: –ö–∞—Ç–µ–≥–æ—Ä–∏–∏, –Ω–∞—Ü–µ–Ω–∫–∏, –∏—Å–∫–ª—é—á–µ–Ω–∏—è, —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è.
    """)

    catalog = HighVolumeAutoPartsCatalog()

    # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
    st.sidebar.title("üß≠ –ù–∞–≤–∏–≥–∞—Ü–∏—è")
    menu_option = st.sidebar.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª:", [
        "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö",
        "–≠–∫—Å–ø–æ—Ä—Ç",
        "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞",
        "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏"
    ])

    if menu_option == "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö":
        st.header("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö")

        col1, col2 = st.columns(2)
        with col1:
            oe_file = st.file_uploader("1. –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (OE)", type=['xlsx', 'xls'])
            cross_file = st.file_uploader("2. –ö—Ä–æ—Å—Å—ã (OE ‚Üí –ê—Ä—Ç–∏–∫—É–ª)", type=['xlsx', 'xls'])
            barcode_file = st.file_uploader("3. –®—Ç—Ä–∏—Ö-–∫–æ–¥—ã –∏ –∫—Ä–∞—Ç–Ω–æ—Å—Ç—å", type=['xlsx', 'xls'])
        with col2:
            dimensions_file = st.file_uploader("4. –í–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã", type=['xlsx', 'xls'])
            images_file = st.file_uploader("5. –°—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", type=['xlsx', 'xls'])
            prices_file = st.file_uploader("6. –ü—Ä–∞–π—Å-–ª–∏—Å—Ç —Å —Ü–µ–Ω–∞–º–∏", type=['xlsx', 'xls'])

        file_map = {
            'oe': oe_file,
            'cross': cross_file,
            'barcode': barcode_file,
            'dimensions': dimensions_file,
            'images': images_file,
            'prices': prices_file
        }

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        saved_paths = {}
        for file_type, uploaded_file in file_map.items():
            if uploaded_file is not None:
                save_path = catalog.data_dir / f"upload_{file_type}_{int(time.time())}.xlsx"
                with open(save_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                saved_paths[file_type] = str(save_path)

        if st.button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ"):
            if not saved_paths:
                st.warning("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–π–ª")
            else:
                with st.spinner("–ß—Ç–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤..."):
                    dataframes = catalog.merge_all_data_parallel(saved_paths)
                if dataframes:
                    with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –≤ –±–∞–∑—É..."):
                        catalog.process_and_load_data(dataframes)
                else:
                    st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∏ –æ–¥–∏–Ω —Ñ–∞–π–ª")

    elif menu_option == "–≠–∫—Å–ø–æ—Ä—Ç":
        catalog.show_export_interface()

    elif menu_option == "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞":
        catalog.show_statistics()

    elif menu_option == "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏":
        catalog.show_data_management()


if __name__ == "__main__":
    main()
        
        
