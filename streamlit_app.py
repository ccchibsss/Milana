import polars as pl
import duckdb
import streamlit as st
import os
import time
import io
import zipfile
from pathlib import Path
from typing import Dict, List
import warnings
import logging
import difflib

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)

EXCEL_ROW_LIMIT = 1_000_000

class HighVolumeAutoPartsCatalog:
    def __init__(self):
        self.data_dir = Path("./auto_parts_data")
        self.data_dir.mkdir(exist_ok=True)
        self.db_path = self.data_dir / "catalog.duckdb"
        self.conn = duckdb.connect(str(self.db_path))
        self.setup_database()
        st.set_page_config(
            page_title="AutoParts Catalog 10M+",
            layout="wide",
            page_icon="üöó"
        )
        # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ü–µ–Ω –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫
        self.prices_df = pl.DataFrame()
        self.price_markup = 1.0  # –û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞
        self.brand_markup = {}   # –ù–∞—Ü–µ–Ω–∫–∞ –ø–æ –±—Ä–µ–Ω–¥–∞–º

    def setup_database(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE, 
                width DOUBLE,
                height DOUBLE, 
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                price DOUBLE,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)
        # –¢–∞–±–ª–∏—Ü–∞ —Ü–µ–Ω (–µ—Å–ª–∏ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS prices (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                price DOUBLE,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)

    def normalize_key(self, key_series: pl.Series) -> pl.Series:
        return (
            key_series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
            .str.to_lowercase()
        )

    def clean_values(self, value_series: pl.Series) -> pl.Series:
        return (
            value_series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
        )

    def detect_category(self, name_series: pl.Series) -> pl.Series:
        categories_map = {
            '–§–∏–ª—å—Ç—Ä': '—Ñ–∏–ª—å—Ç—Ä|filter',
            '–¢–æ—Ä–º–æ–∑–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞': '—Ç–æ—Ä–º–æ–∑|brake|–∫–æ–ª–æ–¥–∫|–¥–∏—Å–∫|—Å—É–ø–ø–æ—Ä—Ç',
            '–ü–æ–¥–≤–µ—Å–∫–∞': '–∞–º–æ—Ä—Ç–∏–∑–∞—Ç–æ—Ä|—Å—Ç–æ–π–∫|spring|–ø–æ–¥–≤–µ—Å–∫|–†—ã—á–∞–≥|–†—ã—á–∞–≥–∏|–®–∞—Ä–æ–≤–∞—è –æ–ø–æ—Ä–∞|–û–ø–æ—Ä–∞ —à–∞—Ä–æ–≤–∞—è|–°–∞–π–ª–µ–Ω—Ç–±–ª–æ–∫|–°—Ç—É–ø–∏—Ü|–ø–æ–¥—à–∏–ø–Ω–∏–∫ —Å—Ç—É–ø–∏—Ü—ã|–ø–æ–¥—à–∏–ø–Ω–∏–∫–∏ —Å—Ç—É–ø–∏—Ü—ã',
            '–î–≤–∏–≥–∞—Ç–µ–ª—å': '–¥–≤–∏–≥–∞—Ç–µ–ª—å|engine|—Å–≤–µ—á|–ø–æ—Ä—à–µ–Ω—å|–∫–ª–∞–ø–∞–Ω',
            '–¢—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è': '—Ç—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è|—Å—Ü–µ–ø–ª–µ–Ω|–∫–æ—Ä–æ–±–∫|transmission',
            '–≠–ª–µ–∫—Ç—Ä–∏–∫–∞': '–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä|–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä|—Å—Ç–∞—Ä—Ç–µ—Ä|–ø—Ä–æ–≤–æ–¥|–ª–∞–º–ø',
            '–†—É–ª–µ–≤–æ–µ': '—Ä—É–ª–µ–≤–æ–π|—Ç—è–≥–∞|–Ω–∞–∫–æ–Ω–µ—á–Ω–∏–∫|steering',
            '–í—ã—Ö–ª–æ–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞': '–≥–ª—É—à–∏—Ç–µ–ª—å|–≥–ª—É—à–∏—Ç–µ–ª|–∫–∞—Ç–∞–ª–∏–∑–∞—Ç–æ—Ä|–≤—ã—Ö–ª–æ–ø|exhaust',
            '–û—Ö–ª–∞–∂–¥–µ–Ω–∏–µ': '—Ä–∞–¥–∏–∞—Ç–æ—Ä|–≤–µ–Ω—Ç–∏–ª—è—Ç–æ—Ä|—Ç–µ—Ä–º–æ—Å—Ç–∞—Ç|cooling',
            '–¢–æ–ø–ª–∏–≤–æ': '—Ç–æ–ø–ª–∏–≤–Ω—ã–π|–±–µ–Ω–∑–æ–Ω–∞—Å–æ—Å|—Ñ–æ—Ä—Å—É–Ω–∫|fuel',
        }
        name_lower = name_series.str.to_lowercase()
        categorization_expr = pl.when(pl.lit(False)).then(pl.lit(None))
        for cat, pattern in categories_map.items():
            categorization_expr = categorization_expr.when(name_lower.str.contains(pattern)).then(pl.lit(cat))
        return categorization_expr.otherwise(pl.lit('–†–∞–∑–Ω–æ–µ')).alias('category')

    def read_and_prepare_file(self, file_path: str, file_type: str) -> pl.DataFrame:
        try:
            df = pl.read_excel(file_path, engine='calamine')
        except Exception as e:
            st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {file_path}: {e}")
            return pl.DataFrame()

        schemas = {
            'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
            'barcode': ['brand', 'artikul', 'barcode', 'multiplicity'],
            'dimensions': ['artikul', 'brand', 'length', 'width', 'height', 'weight', 'dimensions_str'],
            'images': ['artikul', 'brand', 'image_url'],
            'cross': ['oe_number', 'artikul', 'brand']
        }
        expected_cols = schemas.get(file_type, [])
        column_mapping = self.detect_columns(df.columns, expected_cols)
        df = df.rename(column_mapping)

        # –û—á–∏—Å—Ç–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π
        if 'artikul' in df.columns:
            df = df.with_columns(artikul=self.clean_values(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand=self.clean_values(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number=self.clean_values(pl.col('oe_number')))

        key_cols = [col for col in ['oe_number', 'artikul', 'brand'] if col in df.columns]
        if key_cols:
            df = df.unique(subset=key_cols, keep='first')

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–π
        if 'artikul' in df.columns:
            df = df.with_columns(artikul_norm=self.normalize_key(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand_norm=self.normalize_key(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number_norm= self.normalize_key(pl.col('oe_number')))
        return df

    def upsert_data(self, table_name: str, df: pl.DataFrame, pk: List[str]):
        if df.is_empty():
            return
        df = df.unique(keep='first')
        cols = df.columns
        pk_str = ", ".join(f'"{c}"' for c in pk)
        temp_view_name = f"temp_{table_name}_{int(time.time())}"
        self.conn.register(temp_view_name, df.to_arrow())

        update_cols = [col for col in cols if col not in pk]
        if not update_cols:
            on_conflict_action = "DO NOTHING"
        else:
            update_clause = ", ".join([f'"{col}" = excluded."{col}"' for col in update_cols])
            on_conflict_action = f"DO UPDATE SET {update_clause}"

        sql = f"""
        INSERT INTO {table_name}
        SELECT * FROM {temp_view_name}
        ON CONFLICT ({pk_str}) {on_conflict_action};
        """
        try:
            self.conn.execute(sql)
        finally:
            self.conn.unregister(temp_view_name)

    def process_and_load_data(self, dataframes: Dict[str, pl.DataFrame]):
        if 'oe' in dataframes:
            df = dataframes['oe'].filter(pl.col('oe_number_norm') != "")
            oe_df = df.select(['oe_number_norm', 'oe_number', 'name', 'applicability']).unique(subset=['oe_number_norm'], keep='first')
            if 'name' in oe_df.columns:
                oe_df = oe_df.with_columns(self.detect_category(pl.col('name')))
            else:
                oe_df = oe_df.with_columns(category=pl.lit('–†–∞–∑–Ω–æ–µ'))
            self.upsert_data('oe_data', oe_df, ['oe_number_norm'])
            cross_df = df.filter(pl.col('artikul_norm') != "").select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        if 'cross' in dataframes:
            df = dataframes['cross'].filter((pl.col('oe_number_norm') != "") & (pl.col('artikul_norm') != ""))
            cross_df = df.select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—Ä—Ç–∏–∫—É–ª–∞ –∏ –ø—Ä–æ—á–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        file_priority = ['oe', 'barcode', 'images', 'dimensions']
        key_files = {ftype: df for ftype, df in dataframes.items() if ftype in file_priority}
        if key_files:
            all_parts = pl.concat([
                df.select(['artikul', 'artikul_norm', 'brand', 'brand_norm']) 
                for ftype, df in key_files.items()
                if 'artikul_norm' in df.columns and 'brand_norm' in df.columns
            ]).filter(pl.col('artikul_norm') != "").unique(subset=['artikul_norm', 'brand_norm'])
            parts_df = all_parts

            for ftype in file_priority:
                if ftype not in key_files:
                    continue
                df = key_files[ftype]
                if df.is_empty() or 'artikul_norm' not in df.columns:
                    continue
                join_cols = [col for col in df.columns if col not in ['artikul', 'artikul_norm', 'brand', 'brand_norm']]
                existing_cols = set(parts_df.columns)
                join_cols = [col for col in join_cols if col not in existing_cols]
                if not join_cols:
                    continue
                df_subset = df.select(['artikul_norm', 'brand_norm'] + join_cols).unique(subset=['artikul_norm', 'brand_norm'])
                parts_df = parts_df.join(df_subset, on=['artikul_norm', 'brand_norm'], how='left', coalesce=True)

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤, –æ–ø–∏—Å–∞–Ω–∏–µ, —Ü–µ–Ω—ã
            if 'multiplicity' not in parts_df.columns:
                parts_df = parts_df.with_columns(multiplicity=pl.lit(1).cast(pl.Int32))
            else:
                parts_df = parts_df.with_columns(pl.col('multiplicity').fill_null(1).cast(pl.Int32))
            for col in ['length', 'width', 'height']:
                if col not in parts_df.columns:
                    parts_df = parts_df.with_columns(pl.lit(None).cast(pl.Float64).alias(col))
            if 'dimensions_str' not in parts_df.columns:
                parts_df = parts_df.with_columns(dimensions_str=pl.lit(None).cast(pl.Utf8))
            # –°–æ–∑–¥–∞–µ–º dimensions_str
            parts_df = parts_df.with_columns([
                pl.col('length').cast(pl.Utf8).fill_null('').alias('_length_str'),
                pl.col('width').cast(pl.Utf8).fill_null('').alias('_width_str'),
                pl.col('height').cast(pl.Utf8).fill_null('').alias('_height_str'),
            ])
            parts_df = parts_df.with_columns(
                dimensions_str=pl.when(
                    (pl.col('dimensions_str').is_not_null()) & (pl.col('dimensions_str') != '')
                ).then(pl.col('dimensions_str')).otherwise(
                    pl.concat_str([pl.col('_length_str'), pl.lit('x'), pl.col('_width_str'), pl.lit('x'), pl.col('_height_str')], separator='')
                )
            ).drop(['_length_str', '_width_str', '_height_str'])

            # –û–ø–∏—Å–∞–Ω–∏–µ
            if 'artikul' not in parts_df.columns:
                parts_df = parts_df.with_columns(artikul=pl.lit(''))
            if 'brand' not in parts_df.columns:
                parts_df = parts_df.with_columns(brand=pl.lit(''))
            parts_df = parts_df.with_columns([
                pl.col('artikul').cast(pl.Utf8).fill_null('').alias('_artikul_str'),
                pl.col('brand').cast(pl.Utf8).fill_null('').alias('_brand_str'),
                pl.col('multiplicity').cast(pl.Utf8).alias('_multiplicity_str'),
            ])
            parts_df = parts_df.with_columns(
                description=pl.concat_str([
                    '–ê—Ä—Ç–∏–∫—É–ª: ', pl.col('_artikul_str'),
                    ', –ë—Ä–µ–Ω–¥: ', pl.col('_brand_str'),
                    ', –ö—Ä–∞—Ç–Ω–æ—Å—Ç—å: ', pl.col('_multiplicity_str'), ' —à—Ç.'
                ], separator='')
            ).drop(['_artikul_str', '_brand_str', '_multiplicity_str'])

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–µ–Ω
            if 'price' not in parts_df.columns:
                parts_df = parts_df.with_columns(pl.lit(None).cast(pl.Float64).alias('price'))

            self.upsert_data('parts_data', parts_df, ['artikul_norm', 'brand_norm'])

        st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –≤ –±–∞–∑–µ.")

    def merge_all_data_parallel(self, file_paths: Dict[str, str]) -> dict:
        start_time = time.time()
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ü–µ–Ω –∏–∑ –ø—Ä–∞–π—Å–∞
        self.load_price_file(file_paths.get('price'))

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        dataframes = {}
        for key, path in file_paths.items():
            if key != 'price':  # —Ü–µ–Ω–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ
                df = self.read_and_prepare_file(path, key)
                if not df.is_empty():
                    dataframes[key] = df
        self.process_and_load_data(dataframes)

        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞—Ü–µ–Ω–∫–∏
        self.apply_markups()

        total_time = time.time() - start_time
        total_records = self.get_total_records()
        return {
            'processing_time': total_time,
            'total_records': total_records
        }

    def load_price_file(self, price_file_path: str):
        if not price_file_path:
            return
        try:
            df_price = pl.read_excel(io.BytesIO(open(price_file_path, 'rb').read()), engine='calamine')
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –ø—Ä–∞–π—Å–∞: {e}")
            return
        if all(c in df_price.columns for c in ['–∞—Ä—Ç–∏–∫—É–ª', '–±—Ä–µ–Ω–¥', '—Ü–µ–Ω–∞']):
            df_price = df_price.rename({'–∞—Ä—Ç–∏–∫—É–ª': 'artikul', '–±—Ä–µ–Ω–¥': 'brand', '—Ü–µ–Ω–∞': 'price'})
            df_price = df_price.with_columns([
                self.normalize_key(pl.col('artikul')).alias('artikul_norm'),
                self.normalize_key(pl.col('brand')).alias('brand_norm')
            ])
            self.prices_df = self.prices_df.vstack(df_price.select(['artikul_norm', 'brand_norm', 'price']))
            st.success("–ü—Ä–∞–π—Å –æ–±–Ω–æ–≤–ª–µ–Ω.")
        else:
            st.warning("–ü—Ä–∞–π—Å –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏: '–∞—Ä—Ç–∏–∫—É–ª', '–±—Ä–µ–Ω–¥', '—Ü–µ–Ω–∞'.")

    def apply_price_markup(self):
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω –ø–æ —Ç–µ–∫—É—â–µ–π –Ω–∞—Ü–µ–Ω–∫–µ –∏ –±—Ä–µ–Ω–¥—É
        if self.prices_df.is_empty():
            return
        df_prices = self.prices_df
        for row in df_prices.iter_rows():
            artikul_norm = row[0]
            brand_norm = row[1]
            price = row[2]
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–∞—Ü–µ–Ω–∫–∏
            markup = self.brand_markup.get(brand_norm, self.price_markup)
            final_price = price * markup
            self.conn.execute("""
                UPDATE parts_data SET price = ? WHERE artikul_norm = ? AND brand_norm = ?
            """, [final_price, artikul_norm, brand_norm])

    def set_brand_markup(self, brand: str, percent: float):
        normalized_brand = self.normalize_key(pl.Series([brand]))[0]
        self.brand_markup[normalized_brand] = 1 + percent / 100
        self.apply_price_markup()
        st.info(f"–ù–∞—Ü–µ–Ω–∫–∞ –¥–ª—è –±—Ä–µ–Ω–¥–∞ '{brand}': {percent}% —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")

    def set_global_markup(self, percent: float):
        self.price_markup = 1 + percent / 100
        self.apply_price_markup()
        st.info(f"–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞: {percent}% —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")

    def add_price_from_uploaded_file(self, file_bytes):
        try:
            df_price = pl.read_excel(io.BytesIO(file_bytes), engine='calamine')
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø—Ä–∞–π—Å–∞: {e}")
            return
        if all(c in df_price.columns for c in ['–∞—Ä—Ç–∏–∫—É–ª', '–±—Ä–µ–Ω–¥', '—Ü–µ–Ω–∞']):
            df_price = df_price.rename({'–∞—Ä—Ç–∏–∫—É–ª': 'artikul', '–±—Ä–µ–Ω–¥': 'brand', '—Ü–µ–Ω–∞': 'price'})
            df_price = df_price.with_columns([
                self.normalize_key(pl.col('artikul')).alias('artikul_norm'),
                self.normalize_key(pl.col('brand')).alias('brand_norm')
            ])
            self.prices_df = self.prices_df.vstack(df_price.select(['artikul_norm', 'brand_norm', 'price']))
            st.success("–¶–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω—ã.")
        else:
            st.warning("–ü—Ä–∞–π—Å-—Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏: '–∞—Ä—Ç–∏–∫—É–ª', '–±—Ä–µ–Ω–¥', '—Ü–µ–Ω–∞'.")

    def set_brand_markup(self, brand: str, percent: float):
        normalized_brand = self.normalize_key(pl.Series([brand]))[0]
        self.brand_markup[normalized_brand] = 1 + percent / 100
        self.apply_price_markup()

    def set_global_markup(self, percent: float):
        self.price_markup = 1 + percent / 100
        self.apply_price_markup()

    def build_export_query(self, selected_columns: List[str]):
        standard_description = """–°–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: –Ω–æ–≤—ã–π (–≤ —É–ø–∞–∫–æ–≤–∫–µ).
–í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã ‚Äî –Ω–∞–¥–µ–∂–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è. 
–û–±–µ—Å–ø–µ—á—å—Ç–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –¥–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç—å –∏ –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ —Å –ø–æ–º–æ—â—å—é –Ω–∞—à–µ–≥–æ —à–∏—Ä–æ–∫–æ–≥–æ –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–µ–π.

–í –Ω–∞—à–µ–º –∫–∞—Ç–∞–ª–æ–≥–µ –≤—ã –Ω–∞–π–¥–µ—Ç–µ —Ç–æ—Ä–º–æ–∑–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, —Ñ–∏–ª—å—Ç—Ä—ã (–º–∞—Å–ª—è–Ω—ã–µ, –≤–æ–∑–¥—É—à–Ω—ã–µ, —Å–∞–ª–æ–Ω–Ω—ã–µ), —Å–≤–µ—á–∏ –∑–∞–∂–∏–≥–∞–Ω–∏—è, —Ä–∞—Å—Ö–æ–¥–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –∞–≤—Ç–æ—Ö–∏–º–∏—é, —ç–ª–µ–∫—Ç—Ä–∏–∫—É, –∞–≤—Ç–æ–º–∞—Å–ª–∞, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∞ —Ç–∞–∫–∂–µ –¥—Ä—É–≥–∏–µ –∫–æ–º–ø–ª–µ–∫—Ç—É—é—â–∏–µ, –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. 

–ú—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –±—ã—Å—Ç—Ä—É—é –¥–æ—Å—Ç–∞–≤–∫—É, –≤—ã–≥–æ–¥–Ω—ã–µ —Ü–µ–Ω—ã –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –¥–ª—è –ª—é–±–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ ‚Äî –∞–≤—Ç–æ–ª—é–±–∏—Ç–µ–ª—è, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –∏–ª–∏ –∞–≤—Ç–æ—Å–µ—Ä–≤–∏—Å–∞. 

–í—ã–±–∏—Ä–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –ª—É—á—à–µ–µ ‚Äî –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç –≤–µ–¥—É—â–∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π."""
        columns_map = [
            ("–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", 'r.artikul AS "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞"'),
            ("–ë—Ä–µ–Ω–¥", 'r.brand AS "–ë—Ä–µ–Ω–¥"'),
            ("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", 'COALESCE(r.representative_name, r.analog_representative_name) AS "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"'),
            ("–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", 'COALESCE(r.representative_applicability, r.analog_representative_applicability) AS "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å"'),
            ("–û–ø–∏—Å–∞–Ω–∏–µ", "CONCAT(COALESCE(r.description, ''), dt.text) AS \"–û–ø–∏—Å–∞–Ω–∏–µ\""),
            ("–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", 'COALESCE(r.representative_category, r.analog_representative_category) AS "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞"'),
            ("–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", 'r.multiplicity AS "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"'),
            ("–î–ª–∏–Ω–Ω–∞", 'COALESCE(r.length, r.analog_length) AS "–î–ª–∏–Ω–Ω–∞"'),
            ("–®–∏—Ä–∏–Ω–∞", 'COALESCE(r.width, r.analog_width) AS "–®–∏—Ä–∏–Ω–∞"'),
            ("–í—ã—Å–æ—Ç–∞", 'COALESCE(r.height, r.analog_height) AS "–í—ã—Å–æ—Ç–∞"'),
            ("–í–µ—Å", 'COALESCE(r.weight, r.analog_weight) AS "–í–µ—Å"'),
            ("–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "COALESCE(CASE WHEN r.dimensions_str IS NULL OR r.dimensions_str = '' OR UPPER(TRIM(r.dimensions_str)) = 'XX' THEN NULL ELSE r.dimensions_str END, r.analog_dimensions_str) AS \"–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞\""),
            ("OE –Ω–æ–º–µ—Ä", 'r.oe_list AS "OE –Ω–æ–º–µ—Ä"'),
            ("–∞–Ω–∞–ª–æ–≥–∏", 'r.analog_list AS "–∞–Ω–∞–ª–æ–≥–∏"'),
            ("–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", 'r.image_url AS "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"')
        ]
        if not selected_columns:
            selected_exprs = [expr for _, expr in columns_map]
        else:
            selected_exprs = [expr for name, expr in columns_map if name in selected_columns]
            if not selected_exprs:
                selected_exprs = [expr for _, expr in columns_map]

        ctes = f"""
        WITH DescriptionTemplate AS (
            SELECT CHR(10) || CHR(10) || $${standard_description}$$ AS text
        ),
        PartDetails AS (
            SELECT
                cr.artikul_norm,
                cr.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(o.oe_number, '''', ''), '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]', '', 'g'), ', ') AS oe_list,
                ANY_VALUE(o.name) AS representative_name,
                ANY_VALUE(o.applicability) AS representative_applicability,
                ANY_VALUE(o.category) AS representative_category
            FROM cross_references cr
            JOIN oe_data o ON cr.oe_number_norm = o.oe_number_norm
            GROUP BY cr.artikul_norm, cr.brand_norm
        ),
        AllAnalogs AS (
            SELECT
                cr1.artikul_norm,
                cr1.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(p2.artikul, '''', ''), '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]', '', 'g'), ', ') as analog_list
            FROM cross_references cr1
            JOIN cross_references cr2 ON cr1.oe_number_norm = cr2.oe_number_norm
            JOIN parts_data p2 ON cr2.artikul_norm = p2.artikul_norm AND cr2.brand_norm = p2.brand_norm
            WHERE (cr1.artikul_norm != p2.artikul_norm OR cr1.brand_norm != p2.brand_norm)
            GROUP BY cr1.artikul_norm, cr1.brand_norm
        ),
        RankedData AS (
            SELECT
                p.artikul,
                p.brand,
                p.description,
                p.multiplicity,
                p.length,
                p.width,
                p.height,
                p.weight,
                p.dimensions_str,
                p.image_url,
                pd.representative_name,
                pd.representative_applicability,
                pd.representative_category,
                pd.oe_list,
                aa.analog_list,
                p_analog.length AS analog_length,
                p_analog.width AS analog_width,
                p_analog.height AS analog_height,
                p_analog.weight AS analog_weight,
                p_analog.dimensions_str AS analog_dimensions_str,
                p_analog.representative_name AS analog_representative_name,
                p_analog.representative_applicability AS analog_representative_applicability,
                p_analog.representative_category AS analog_representative_category,
                ROW_NUMBER() OVER(PARTITION BY p.artikul_norm, p.brand_norm ORDER BY pd.representative_name DESC NULLS LAST, pd.oe_list DESC NULLS LAST) as rn
            FROM parts_data p
            LEFT JOIN PartDetails pd ON p.artikul_norm = pd.artikul_norm AND p.brand_norm = pd.brand_norm
            LEFT JOIN AllAnalogs aa ON p.artikul_norm = aa.artikul_norm AND p.brand_norm = aa.brand_norm
            LEFT JOIN (
                SELECT artikul_norm, brand_norm, length, width, height, weight, dimensions_str, representative_name, representative_applicability, representative_category
                FROM parts_data
            ) p_analog ON p.artikul_norm = p_analog.artikul_norm AND p.brand_norm = p_analog.brand_norm
        )
        """

        select_clause = ",\n            ".join(selected_exprs)
        query = ctes + f"""
        SELECT
            {select_clause}
        FROM RankedData r
        CROSS JOIN DescriptionTemplate dt
        WHERE r.rn = 1
        ORDER BY r.brand, r.artikul
        """
        return query

    def show_export_interface(self):
        st.header("üì§ –£–º–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")
        total_records = self.get_total_records()
        st.info(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞: {total_records:,}")
        if total_records == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞.")
            return
        options = [
            "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", "–ë—Ä–µ–Ω–¥", "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", "–û–ø–∏—Å–∞–Ω–∏–µ",
            "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", "–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞",
            "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "OE –Ω–æ–º–µ—Ä", "–∞–Ω–∞–ª–æ–≥–∏", "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
        ]
        selected_columns = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞", options=options, default=options)

        format_type = st.radio("–§–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞:", ["CSV", "Excel (.xlsx)", "Parquet"], index=0)
        if st.button("üöÄ –ù–∞—á–∞—Ç—å —ç–∫—Å–ø–æ—Ä—Ç"):
            output_path = Path("./auto_parts_export")
            output_path.mkdir(exist_ok=True)
            filename = f"auto_parts_{int(time.time())}"
            if format_type == "CSV":
                file_path = output_path / f"{filename}.csv"
                self.export_to_csv(selected_columns, str(file_path))
                with open(file_path, 'rb') as f:
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å CSV", f, file_path.name, "text/csv")
            elif format_type == "Excel (.xlsx)":
                file_path = output_path / f"{filename}.xlsx"
                self.export_to_excel(selected_columns, file_path)
                with open(file_path, 'rb') as f:
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å Excel", f, file_path.name, "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
            else:
                file_path = output_path / f"{filename}.parquet"
                self.export_to_parquet(selected_columns, str(file_path))
                with open(file_path, 'rb') as f:
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å Parquet", f, file_path.name, "application/octet-stream")

    def delete_by_brand(self, brand_norm: str) -> int:
        try:
            res = self.conn.execute("DELETE FROM parts_data WHERE brand_norm = ?", [brand_norm])
            return res.rowcount
        except Exception as e:
            logging.exception(e)
            return 0

    def delete_by_artikul(self, artikul_norm: str) -> int:
        try:
            res = self.conn.execute("DELETE FROM parts_data WHERE artikul_norm = ?", [artikul_norm])
            return res.rowcount
        except Exception as e:
            logging.exception(e)
            return 0

    def get_statistics(self):
        total_parts = self.get_total_records()
        total_oe = self.conn.execute("SELECT COUNT(*) FROM oe_data").fetchone()[0]
        total_brands = self.conn.execute("SELECT COUNT(DISTINCT brand) FROM parts_data").fetchone()[0]
        top_brands = self.conn.execute("SELECT brand, COUNT(*) as cnt FROM parts_data GROUP BY brand ORDER BY cnt DESC LIMIT 10").pl()
        categories = self.conn.execute("SELECT category, COUNT(*) as cnt FROM oe_data WHERE category IS NOT NULL GROUP BY category ORDER BY cnt DESC").pl()
        return {
            'total_parts': total_parts,
            'total_oe': total_oe,
            'total_brands': total_brands,
            'top_brands': top_brands,
            'categories': categories
        }

    def get_total_records(self):
        res = self.conn.execute("SELECT COUNT(*) FROM parts_data").fetchone()
        return res[0] if res else 0

    # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–π—Å–∞
    def load_price_file(self, file_bytes):
        try:
            df_price = pl.read_excel(io.BytesIO(file_bytes), engine='calamine')
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø—Ä–∞–π—Å–∞: {e}")
            return
        if all(c in df_price.columns for c in ['–∞—Ä—Ç–∏–∫—É–ª', '–±—Ä–µ–Ω–¥', '—Ü–µ–Ω–∞']):
            df_price = df_price.rename({'–∞—Ä—Ç–∏–∫—É–ª': 'artikul', '–±—Ä–µ–Ω–¥': 'brand', '—Ü–µ–Ω–∞': 'price'})
            df_price = df_price.with_columns([
                self.normalize_key(pl.col('artikul')).alias('artikul_norm'),
                self.normalize_key(pl.col('brand')).alias('brand_norm')
            ])
            self.prices_df = self.prices_df.vstack(df_price.select(['artikul_norm', 'brand_norm', 'price']))
            st.success("–ü—Ä–∞–π—Å –æ–±–Ω–æ–≤–ª–µ–Ω.")
        else:
            st.warning("–ü—Ä–∞–π—Å –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏: '–∞—Ä—Ç–∏–∫—É–ª', '–±—Ä–µ–Ω–¥', '—Ü–µ–Ω–∞'.")

    def apply_price_markup(self):
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω –ø–æ —Ç–µ–∫—É—â–µ–π –Ω–∞—Ü–µ–Ω–∫–µ –∏ –±—Ä–µ–Ω–¥—É
        if self.prices_df.is_empty():
            return
        df_prices = self.prices_df
        for row in df_prices.iter_rows():
            artikul_norm = row[0]
            brand_norm = row[1]
            price = row[2]
            markup = self.brand_markup.get(brand_norm, self.price_markup)
            final_price = price * markup
            self.conn.execute("""
                UPDATE parts_data SET price = ? WHERE artikul_norm = ? AND brand_norm = ?
            """, [final_price, artikul_norm, brand_norm])

    def set_brand_markup(self, brand: str, percent: float):
        normalized_brand = self.normalize_key(pl.Series([brand]))[0]
        self.brand_markup[normalized_brand] = 1 + percent / 100
        self.apply_price_markup()

    def set_global_markup(self, percent: float):
        self.price_markup = 1 + percent / 100
        self.apply_price_markup()

    def add_price_from_uploaded_file(self, file_bytes):
        self.load_price_file(file_bytes)
        self.apply_price_markup()

    def build_export_query(self, selected_columns: List[str]):
        standard_description = """–°–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: –Ω–æ–≤—ã–π (–≤ —É–ø–∞–∫–æ–≤–∫–µ).
–í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã ‚Äî –Ω–∞–¥–µ–∂–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è. 
–û–±–µ—Å–ø–µ—á—å—Ç–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –¥–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç—å –∏ –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ —Å –ø–æ–º–æ—â—å—é –Ω–∞—à–µ–≥–æ —à–∏—Ä–æ–∫–æ–≥–æ –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–µ–π.

–í –Ω–∞—à–µ–º –∫–∞—Ç–∞–ª–æ–≥–µ –≤—ã –Ω–∞–π–¥–µ—Ç–µ —Ç–æ—Ä–º–æ–∑–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, —Ñ–∏–ª—å—Ç—Ä—ã (–º–∞—Å–ª—è–Ω—ã–µ, –≤–æ–∑–¥—É—à–Ω—ã–µ, —Å–∞–ª–æ–Ω–Ω—ã–µ), —Å–≤–µ—á–∏ –∑–∞–∂–∏–≥–∞–Ω–∏—è, —Ä–∞—Å—Ö–æ–¥–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –∞–≤—Ç–æ—Ö–∏–º–∏—é, —ç–ª–µ–∫—Ç—Ä–∏–∫—É, –∞–≤—Ç–æ–º–∞—Å–ª–∞, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∞ —Ç–∞–∫–∂–µ –¥—Ä—É–≥–∏–µ –∫–æ–º–ø–ª–µ–∫—Ç—É—é—â–∏–µ, –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. 

–ú—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –±—ã—Å—Ç—Ä—É—é –¥–æ—Å—Ç–∞–≤–∫—É, –≤—ã–≥–æ–¥–Ω—ã–µ —Ü–µ–Ω—ã –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –¥–ª—è –ª—é–±–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ ‚Äî –∞–≤—Ç–æ–ª—é–±–∏—Ç–µ–ª—è, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –∏–ª–∏ –∞–≤—Ç–æ—Å–µ—Ä–≤–∏—Å–∞. 

–í—ã–±–∏—Ä–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –ª—É—á—à–µ–µ ‚Äî –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç –≤–µ–¥—É—â–∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π."""
        columns_map = [
            ("–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", 'r.artikul AS "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞"'),
            ("–ë—Ä–µ–Ω–¥", 'r.brand AS "–ë—Ä–µ–Ω–¥"'),
            ("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", 'COALESCE(r.representative_name, r.analog_representative_name) AS "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"'),
            ("–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", 'COALESCE(r.representative_applicability, r.analog_representative_applicability) AS "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å"'),
            ("–û–ø–∏—Å–∞–Ω–∏–µ", "CONCAT(COALESCE(r.description, ''), dt.text) AS \"–û–ø–∏—Å–∞–Ω–∏–µ\""),
            ("–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", 'COALESCE(r.representative_category, r.analog_representative_category) AS "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞"'),
            ("–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", 'r.multiplicity AS "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"'),
            ("–î–ª–∏–Ω–Ω–∞", 'COALESCE(r.length, r.analog_length) AS "–î–ª–∏–Ω–Ω–∞"'),
            ("–®–∏—Ä–∏–Ω–∞", 'COALESCE(r.width, r.analog_width) AS "–®–∏—Ä–∏–Ω–∞"'),
            ("–í—ã—Å–æ—Ç–∞", 'COALESCE(r.height, r.analog_height) AS "–í—ã—Å–æ—Ç–∞"'),
            ("–í–µ—Å", 'COALESCE(r.weight, r.analog_weight) AS "–í–µ—Å"'),
            ("–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "COALESCE(CASE WHEN r.dimensions_str IS NULL OR r.dimensions_str = '' OR UPPER(TRIM(r.dimensions_str)) = 'XX' THEN NULL ELSE r.dimensions_str END, r.analog_dimensions_str) AS \"–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞\""),
            ("OE –Ω–æ–º–µ—Ä", 'r.oe_list AS "OE –Ω–æ–º–µ—Ä"'),
            ("–∞–Ω–∞–ª–æ–≥–∏", 'r.analog_list AS "–∞–Ω–∞–ª–æ–≥–∏"'),
            ("–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", 'r.image_url AS "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"')
        ]
        if not selected_columns:
            selected_exprs = [expr for _, expr in columns_map]
        else:
            selected_exprs = [expr for name, expr in columns_map if name in selected_columns]
            if not selected_exprs:
                selected_exprs = [expr for _, expr in columns_map]

        ctes = f"""
        WITH DescriptionTemplate AS (
            SELECT CHR(10) || CHR(10) || $${standard_description}$$ AS text
        ),
        PartDetails AS (
            SELECT
                cr.artikul_norm,
                cr.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(o.oe_number, '''', ''), '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]', '', 'g'), ', ') AS oe_list,
                ANY_VALUE(o.name) AS representative_name,
                ANY_VALUE(o.applicability) AS representative_applicability,
                ANY_VALUE(o.category) AS representative_category
            FROM cross_references cr
            JOIN oe_data o ON cr.oe_number_norm = o.oe_number_norm
            GROUP BY cr.artikul_norm, cr.brand_norm
        ),
        AllAnalogs AS (
            SELECT
                cr1.artikul_norm,
                cr1.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(p2.artikul, '''', ''), '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]', '', 'g'), ', ') as analog_list
            FROM cross_references cr1
            JOIN cross_references cr2 ON cr1.oe_number_norm = cr2.oe_number_norm
            JOIN parts_data p2 ON cr2.artikul_norm = p2.artikul_norm AND cr2.brand_norm = p2.brand_norm
            WHERE (cr1.artikul_norm != p2.artikul_norm OR cr1.brand_norm != p2.brand_norm)
            GROUP BY cr1.artikul_norm, cr1.brand_norm
        ),
        RankedData AS (
            SELECT
                p.artikul,
                p.brand,
                p.description,
                p.multiplicity,
                p.length,
                p.width,
                p.height,
                p.weight,
                p.dimensions_str,
                p.image_url,
                pd.representative_name,
                pd.representative_applicability,
                pd.representative_category,
                pd.oe_list,
                aa.analog_list,
                p_analog.length AS analog_length,
                p_analog.width AS analog_width,
                p_analog.height AS analog_height,
                p_analog.weight AS analog_weight,
                p_analog.dimensions_str AS analog_dimensions_str,
                p_analog.representative_name AS analog_representative_name,
                p_analog.representative_applicability AS analog_representative_applicability,
                p_analog.representative_category AS analog_representative_category,
                ROW_NUMBER() OVER(PARTITION BY p.artikul_norm, p.brand_norm ORDER BY pd.representative_name DESC NULLS LAST, pd.oe_list DESC NULLS LAST) as rn
            FROM parts_data p
            LEFT JOIN PartDetails pd ON p.artikul_norm = pd.artikul_norm AND p.brand_norm = pd.brand_norm
            LEFT JOIN AllAnalogs aa ON p.artikul_norm = aa.artikul_norm AND p.brand_norm = aa.brand_norm
            LEFT JOIN (
                SELECT artikul_norm, brand_norm, length, width, height, weight, dimensions_str, representative_name, representative_applicability, representative_category
                FROM parts_data
            ) p_analog ON p.artikul_norm = p_analog.artikul_norm AND p.brand_norm = p_analog.brand_norm
        )
        """

        select_clause = ",\n            ".join(selected_exprs)
        query = ctes + f"""
        SELECT
            {select_clause}
        FROM RankedData r
        CROSS JOIN DescriptionTemplate dt
        WHERE r.rn = 1
        ORDER BY r.brand, r.artikul
        """
        return query

    def assign_category_by_name(self, search_name: str, category_name: str, similarity_threshold: float = 0.5):
        """
        –ò—â–µ—Ç –≤ –±–∞–∑–µ —Ç–æ–≤–∞—Ä—ã —Å –ø–æ—Ö–æ–∂–∏–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç –∏–º —É–∫–∞–∑–∞–Ω–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é.
        """
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        res = self.conn.execute("SELECT DISTINCT name FROM oe_data WHERE name IS NOT NULL").fetchall()
        names = [row[0] for row in res]
        matched_names = []
        for name in names:
            ratio = difflib.SequenceMatcher(None, name.lower(), search_name.lower()).ratio()
            if ratio >= similarity_threshold:
                matched_names.append(name)
        if not matched_names:
            st.info("–ü–æ—Ö–æ–∂–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
            return
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
        for name in matched_names:
            self.conn.execute("""
                UPDATE oe_data SET category = ? WHERE name = ?
            """, [category_name, name])
        st.success(f"–ù–∞–∑–≤–∞–Ω–∏—è, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ '{search_name}', –æ–±–Ω–æ–≤–ª–µ–Ω—ã –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é '{category_name}'. –û–±–Ω–æ–≤–ª–µ–Ω–æ {len(matched_names)} –∑–∞–ø–∏—Å–µ–π.")

    def show_export_interface(self):
        st.header("üì§ –£–º–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")
        total_records = self.get_total_records()
        st.info(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞: {total_records:,}")
        if total_records == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞.")
            return
        options = [
            "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", "–ë—Ä–µ–Ω–¥", "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", "–û–ø–∏—Å–∞–Ω–∏–µ",
            "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", "–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞",
            "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "OE –Ω–æ–º–µ—Ä", "–∞–Ω–∞–ª–æ–≥–∏", "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
        ]
        selected_columns = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞", options=options, default=options)

        format_type = st.radio("–§–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞:", ["CSV", "Excel (.xlsx)", "Parquet"], index=0)
        if st.button("üöÄ –ù–∞—á–∞—Ç—å —ç–∫—Å–ø–æ—Ä—Ç"):
            output_path = Path("./auto_parts_export")
            output_path.mkdir(exist_ok=True)
            filename = f"auto_parts_{int(time.time())}"
            if format_type == "CSV":
                file_path = output_path / f"{filename}.csv"
                self.export_to_csv(selected_columns, str(file_path))
                with open(file_path, 'rb') as f:
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å CSV", f, file_path.name, "text/csv")
            elif format_type == "Excel (.xlsx)":
                file_path = output_path / f"{filename}.xlsx"
                self.export_to_excel(selected_columns, file_path)
                with open(file_path, 'rb') as f:
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å Excel", f, file_path.name, "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
            else:
                file_path = output_path / f"{filename}.parquet"
                self.export_to_parquet(selected_columns, str(file_path))
                with open(file_path, 'rb') as f:
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å Parquet", f, file_path.name, "application/octet-stream")

    def delete_by_brand(self, brand_norm: str) -> int:
        try:
            res = self.conn.execute("DELETE FROM parts_data WHERE brand_norm = ?", [brand_norm])
            return res.rowcount
        except Exception as e:
            logging.exception(e)
            return 0

    def delete_by_artikul(self, artikul_norm: str) -> int:
        try:
            res = self.conn.execute("DELETE FROM parts_data WHERE artikul_norm = ?", [artikul_norm])
            return res.rowcount
        except Exception as e:
            logging.exception(e)
            return 0

    def get_statistics(self):
        total_parts = self.get_total_records()
        total_oe = self.conn.execute("SELECT COUNT(*) FROM oe_data").fetchone()[0]
        total_brands = self.conn.execute("SELECT COUNT(DISTINCT brand) FROM parts_data").fetchone()[0]
        top_brands = self.conn.execute("SELECT brand, COUNT(*) as cnt FROM parts_data GROUP BY brand ORDER BY cnt DESC LIMIT 10").pl()
        categories = self.conn.execute("SELECT category, COUNT(*) as cnt FROM oe_data WHERE category IS NOT NULL GROUP BY category ORDER BY cnt DESC").pl()
        return {
            'total_parts': total_parts,
            'total_oe': total_oe,
            'total_brands': total_brands,
            'top_brands': top_brands,
            'categories': categories
        }

# –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
def main():
    st.title("üöó AutoParts Catalog - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 10+ –º–ª–Ω –∑–∞–ø–∏—Å–µ–π")
    catalog = HighVolumeAutoPartsCatalog()

    st.sidebar.title("üß≠ –ú–µ–Ω—é")
    menu = st.sidebar.radio("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:", ["–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö", "–≠–∫—Å–ø–æ—Ä—Ç", "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω–∞–º–∏", "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏"])

    if menu == "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö":
        st.header("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã Excel –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–∞—Ç–∞–ª–æ–≥–∞.\n\n"
                "–ú–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∂–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ ‚Äî –æ–Ω–∏ –±—É–¥—É—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã.\n\n"
                "–¢–∏–ø—ã —Ñ–∞–π–ª–æ–≤:\n"
                "- –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (OE, –∞—Ä—Ç–∏–∫—É–ª, –±—Ä–µ–Ω–¥, –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ)\n"
                "- –ö—Ä–æ—Å—Å—ã (OE -> –∞—Ä—Ç–∏–∫—É–ª)\n"
                "- –®—Ç—Ä–∏—Ö-–∫–æ–¥—ã –∏ –∫—Ä–∞—Ç–Ω–æ—Å—Ç—å\n"
                "- –í–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã\n"
                "- –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        col1, col2 = st.columns(2)
        with col1:
            oe_file = st.file_uploader("–û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (OE)", type=['xlsx', 'xls'])
            cross_file = st.file_uploader("–ö—Ä–æ—Å—Å—ã", type=['xlsx', 'xls'])
            barcode_file = st.file_uploader("–®—Ç—Ä–∏—Ö-–∫–æ–¥—ã", type=['xlsx', 'xls'])
        with col2:
            dimensions_file = st.file_uploader("–í–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã", type=['xlsx', 'xls'])
            images_file = st.file_uploader("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", type=['xlsx', 'xls'])

        uploaded_files = {
            'oe': oe_file,
            'cross': cross_file,
            'barcode': barcode_file,
            'dimensions': dimensions_file,
            'images': images_file
        }

        if st.button("üöÄ –ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É"):
            file_paths = {}
            for key, uploaded in uploaded_files.items():
                if uploaded:
                    path = catalog.data_dir / f"{key}_{int(time.time())}_{uploaded.name}"
                    with open(path, 'wb') as f:
                        f.write(uploaded.getvalue())
                    file_paths[key] = str(path)
            if file_paths:
                stats = catalog.merge_all_data_parallel(file_paths)
                st.success(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {stats['processing_time']:.2f} —Å–µ–∫. –í –±–∞–∑–µ {stats['total_records']:,} –∑–∞–ø–∏—Å–µ–π.")
            else:
                st.warning("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–π–ª –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")

    elif menu == "–≠–∫—Å–ø–æ—Ä—Ç":
        catalog.show_export_interface()

    elif menu == "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞":
        st.header("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–∞–ª–æ–≥—É")
        stats = catalog.get_statistics()
        st.metric("–í—Å–µ–≥–æ –∞—Ä—Ç–∏–∫—É–ª–æ–≤", stats['total_parts'])
        st.metric("OE –Ω–æ–º–µ—Ä–∞", stats['total_oe'])
        st.metric("–ë—Ä–µ–Ω–¥—ã", stats['total_brands'])
        st.subheader("–¢–æ–ø –±—Ä–µ–Ω–¥–æ–≤")
        if not stats['top_brands'].is_empty():
            st.dataframe(stats['top_brands'].to_pandas())
        st.subheader("–ö–∞—Ç–µ–≥–æ—Ä–∏–∏")
        if not stats['categories'].is_empty():
            st.bar_chart(stats['categories'].to_pandas().set_index('category'))

    elif menu == "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω–∞–º–∏":
        st.header("üõ†Ô∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω–∞–º–∏")
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ü–µ–Ω
        uploaded_price_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∞–π—Å (–∞—Ä—Ç–∏–∫—É–ª, –±—Ä–µ–Ω–¥, —Ü–µ–Ω–∞)", type=['xlsx', 'xls'])
        if uploaded_price_file:
            catalog.add_price_from_uploaded_file(uploaded_price_file.read())

        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—â–µ–≥–æ –ø—Ä–æ—Ü–µ–Ω—Ç–∞
        markup = st.number_input("–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞ (%)", min_value=0.0, max_value=100.0, value=0.0)
        if st.button("–ü—Ä–∏–º–µ–Ω–∏—Ç—å –æ–±—â—É—é –Ω–∞—Ü–µ–Ω–∫—É"):
            catalog.set_global_markup(markup)

        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞—Ü–µ–Ω–∫–∏ –ø–æ –±—Ä–µ–Ω–¥–∞–º
        st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞—Ü–µ–Ω–∫–∏ –ø–æ –±—Ä–µ–Ω–¥–∞–º")
        brand_name = st.text_input("–ë—Ä–µ–Ω–¥")
        brand_markup_percent = st.number_input("–ù–∞—Ü–µ–Ω–∫–∞ (%)", min_value=0.0, max_value=100.0, value=0.0)
        if st.button("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –¥–ª—è –±—Ä–µ–Ω–¥–∞"):
            if brand_name:
                catalog.set_brand_markup(brand_name, brand_markup_percent)

    elif menu == "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏":
        st.header("üóëÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏")
        option = st.radio("–î–µ–π—Å—Ç–≤–∏—è", ["–£–¥–∞–ª–∏—Ç—å –ø–æ –±—Ä–µ–Ω–¥—É", "–£–¥–∞–ª–∏—Ç—å –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É", "–î–æ–±–∞–≤–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é"])
        if option == "–£–¥–∞–ª–∏—Ç—å –ø–æ –±—Ä–µ–Ω–¥—É":
            brands = []
            res = catalog.conn.execute("SELECT DISTINCT brand FROM parts_data WHERE brand IS NOT NULL").fetchall()
            for row in res:
                brands.append(row[0])
            selected_brand = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –±—Ä–µ–Ω–¥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è", brands)
            if selected_brand:
                norm_brand = catalog.normalize_key(pl.Series([selected_brand]))[0]
                count = catalog.delete_by_brand(norm_brand)
                st.success(f"–£–¥–∞–ª–µ–Ω–æ {count} –∑–∞–ø–∏—Å–µ–π –ø–æ –±—Ä–µ–Ω–¥—É '{selected_brand}'")
        elif option == "–£–¥–∞–ª–∏—Ç—å –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É":
            artikul_input = st.text_input("–ê—Ä—Ç–∏–∫—É–ª")
            if artikul_input:
                norm_artikul = catalog.normalize_key(pl.Series([artikul_input]))[0]
                count = catalog.delete_by_artikul(norm_artikul)
                st.success(f"–£–¥–∞–ª–µ–Ω–æ {count} –∑–∞–ø–∏—Å–µ–π –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É '{artikul_input}'")
        elif option == "–î–æ–±–∞–≤–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é":
            search_name_input = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞")
            category_name_input = st.text_input("–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞")
            similarity_threshold = st.slider("–ü–æ—Ä–æ–≥ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏", 0.0, 1.0, 0.5, step=0.05)
            if st.button("–ü—Ä–∏—Å–≤–æ–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é"):
                if search_name_input and category_name_input:
                    catalog.assign_category_by_name(search_name_input, category_name_input, similarity_threshold)
                else:
                    st.warning("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –æ–±–∞ –ø–æ–ª—è: –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è.")

if __name__ == "__main__":
    main()
