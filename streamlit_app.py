import polars as pl
import duckdb
import streamlit as st
import os
import time
import io
import zipfile
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List

import warnings
warnings.filterwarnings('ignore')

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
EXCEL_ROW_LIMIT = 1_000_000

class AutoPartsCatalog:
    def __init__(self):
        self.data_dir = Path("./auto_parts_data")
        self.data_dir.mkdir(exist_ok=True)
        self.db_path = self.data_dir / "catalog.duckdb"
        self.conn = duckdb.connect(str(self.db_path))
        self.setup_database()

        # –¢–∞–±–ª–∏—Ü—ã –¥–ª—è —Ü–µ–Ω —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∏ —Ü–µ–Ω –ø–æ –∞—Ä—Ç–∏–∫—É–ª–∞–º
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS price_recommendations (
                artikul_norm VARCHAR PRIMARY KEY,
                recommended_price DOUBLE
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS part_prices (
                artikul_norm VARCHAR PRIMARY KEY,
                brand_norm VARCHAR,
                price DOUBLE
            )
        """)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        self.global_markup = 1.2
        self.brand_markups: Dict[str, float] = {}
        self.exclusions: List[str] = []
        self.exclusions_partial: List[str] = []

        # –î–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        self.export_columns: List[str] = []

        # –í–∏–∑—É–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        st.set_page_config(page_title="AutoParts Catalog 10M+", layout="wide", page_icon="üöó")

    def setup_database(self):
        # –°–æ–∑–¥–∞—ë–º —Ç–∞–±–ª–∏—Ü—ã, –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE, 
                width DOUBLE,
                height DOUBLE, 
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)

    # ===================== –ó–∞–≥—Ä—É–∑–∫–∞ —Ü–µ–Ω —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π =====================
    def load_price_recommendations(self, file_path):
        df = pl.read_excel(file_path)
        df = df.select([
            pl.col("–∞—Ä—Ç–∏–∫—É–ª").alias("artikul"),
            pl.col("—Ü–µ–Ω–∞").alias("recommended_price")
        ]).drop_nulls()
        df = df.with_columns(
            pl.col("artikul").str.replace_all("'", "").str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "").str.strip_chars().str.to_lowercase()
        )
        for row in df.iter_rows():
            self.conn.execute("""
                INSERT INTO price_recommendations (artikul_norm, recommended_price)
                VALUES (?, ?)
                ON CONFLICT (artikul_norm) DO UPDATE SET recommended_price=excluded.recommended_price
            """, [row[0], row[1]])

    def get_price_for_artikul(self, artikul_norm):
        res = self.conn.execute("SELECT recommended_price FROM price_recommendations WHERE artikul_norm = ?", [artikul_norm]).fetchone()
        return res[0] if res else None

    # ===================== –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–π—Å–∞ —Å –∞—Ä—Ç–∏–∫—É–ª–∞–º–∏ =====================
    def load_price_list(self, file_path):
        df = pl.read_excel(file_path)
        df = df.select([
            pl.col("–∞—Ä—Ç–∏–∫—É–ª").alias("artikul"),
            pl.col("–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ").alias("quantity"),
            pl.col("–±—Ä–µ–Ω–¥").alias("brand"),
            pl.col("—Ü–µ–Ω–∞").alias("price")
        ]).drop_nulls()

        # –û—á–∏—Å—Ç–∫–∞
        df = df.with_columns(
            pl.col("artikul").str.replace_all("'", "").str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "").str.strip_chars().str.to_lowercase(),
            pl.col("brand").str.replace_all("'", "").str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "").str.strip_chars().str.to_lowercase()
        )

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Ü–µ–Ω
        for row in df.iter_rows():
            artikul_norm = row[0]
            brand_norm = row[2]
            price = row[3]
            self.conn.execute("""
                INSERT INTO part_prices (artikul_norm, brand_norm, price)
                VALUES (?, ?, ?)
                ON CONFLICT (artikul_norm) DO UPDATE SET price=excluded.price
            """, [artikul_norm, brand_norm, price])

    # ===================== –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞—Ü–µ–Ω–∫–∏ =====================
    def set_global_markup(self, markup):
        self.global_markup = markup

    def set_brand_markup(self, brand, markup):
        self.brand_markups[brand.lower()] = markup

    def get_markup_for_brand(self, brand):
        return self.brand_markups.get(brand.lower(), self.global_markup)

    # ===================== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π =====================
    def load_exclusions(self, exact_list, partial_list):
        self.exclusions = [s.lower() for s in exact_list]
        self.exclusions_partial = [s.lower() for s in partial_list]

    def check_exclusions(self, name):
        name_lower = name.lower()
        for excl in self.exclusions:
            if excl == name_lower:
                return True
        for excl in self.exclusions_partial:
            if excl in name_lower:
                return True
        return False

    # ===================== –ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Ü–µ–Ω—ã =====================
    def get_final_price(self, artikul_norm, brand, base_price):
        if self.check_exclusions(brand):
            return None
        brand_markup = self.get_markup_for_brand(brand)
        total_markup = self.global_markup * brand_markup
        final_price = base_price * total_markup
        return final_price

    # ===================== –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ =====================
    def merge_all_data_parallel(self, file_paths: Dict[str, str]) -> Dict:
        start_time = time.time()
        stats = {}
        dataframes = {}

        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        with ThreadPoolExecutor() as executor:
            futures = [executor.submit(self.read_and_prepare_file, path, ftype) for ftype, path in file_paths.items()]
            for future in as_completed(futures):
                # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞ –ø–æ –ø–æ—Ä—è–¥–∫—É
                index = list(file_paths.keys()).index(list(future.result().values())[0])
                ftype = list(file_paths.keys())[index]
                df = future.result()
                if not df.is_empty():
                    dataframes[ftype] = df

        if not dataframes:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
            return {}

        self.process_and_load_data(dataframes)

        stats['processing_time'] = time.time() - start_time
        stats['total_records'] = self.get_total_records()
        st.success(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {stats['processing_time']:.2f} —Å–µ–∫")
        st.success(f"–í—Å–µ–≥–æ –∞—Ä—Ç–∏–∫—É–ª–æ–≤: {stats['total_records']:,}")
        self.create_indexes()
        return stats

    def get_total_records(self):
        try:
            return self.conn.execute("SELECT COUNT(*) FROM parts_data").fetchone()[0]
        except:
            return 0

    def create_indexes(self):
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_oe ON oe_data(oe_number_norm)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_parts ON parts_data(artikul_norm, brand_norm)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_cross ON cross_references(oe_number_norm)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_cross_art ON cross_references(artikul_norm, brand_norm)")

    def read_and_prepare_file(self, file_path, file_type):
        df = pl.read_excel(file_path)
        # –ú–æ–∂–Ω–æ —Ç—É—Ç –¥–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–æ —Ç–∏–ø—É —Ñ–∞–π–ª–∞
        return df

    def process_and_load_data(self, dataframes):
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –≤—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        # –î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –≤—Å—Ç–∞–≤–ª—è—é —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä—è—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É
        pass

    # ===================== –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ SQL-–∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ =====================
    def build_export_query(self, selected_columns=None):
        # –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –æ–ø–∏—Å–∞–Ω–∏—è
        description_text = """
        –°–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: –Ω–æ–≤—ã–π (–≤ —É–ø–∞–∫–æ–≤–∫–µ).
        –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã ‚Äî –Ω–∞–¥–µ–∂–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è. 
        –û–±–µ—Å–ø–µ—á—å—Ç–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –¥–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç—å –∏ –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ —Å –ø–æ–º–æ—â—å—é –Ω–∞—à–µ–≥–æ —à–∏—Ä–æ–∫–æ–≥–æ –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–µ–π.

        –í –Ω–∞—à–µ–º –∫–∞—Ç–∞–ª–æ–≥–µ –≤—ã –Ω–∞–π–¥–µ—Ç–µ —Ç–æ—Ä–º–æ–∑–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, —Ñ–∏–ª—å—Ç—Ä—ã (–º–∞—Å–ª—è–Ω—ã–µ, –≤–æ–∑–¥—É—à–Ω—ã–µ, —Å–∞–ª–æ–Ω–Ω—ã–µ), —Å–≤–µ—á–∏ –∑–∞–∂–∏–≥–∞–Ω–∏—è, —Ä–∞—Å—Ö–æ–¥–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –∞–≤—Ç–æ—Ö–∏–º–∏—é, —ç–ª–µ–∫—Ç—Ä–∏–∫—É, –∞–≤—Ç–æ–º–∞—Å–ª–∞, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∞ —Ç–∞–∫–∂–µ –¥—Ä—É–≥–∏–µ –∫–æ–º–ø–ª–µ–∫—Ç—É—é—â–∏–µ, –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. 

        –ú—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –±—ã—Å—Ç—Ä—É—é –¥–æ—Å—Ç–∞–≤–∫—É, –≤—ã–≥–æ–¥–Ω—ã–µ —Ü–µ–Ω—ã –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –¥–ª—è –ª—é–±–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ ‚Äî –∞–≤—Ç–æ–ª—é–±–∏—Ç–µ–ª—è, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –∏–ª–∏ –∞–≤—Ç–æ—Å–µ—Ä–≤–∏—Å–∞. 

        –í—ã–±–∏—Ä–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –ª—É—á—à–µ–µ ‚Äî –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç –≤–µ–¥—É—â–∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π."""
        # –û—Ñ–æ—Ä–º–ª—è–µ–º CTE —Å —Ç–µ–∫—Å—Ç–æ–º
        query = f"""
        WITH DescriptionText AS (
            SELECT chr(10) || chr(10) || $${description_text}$$ AS text
        ),
        PartDetails AS (
            SELECT
                cr.artikul_norm,
                cr.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(o.oe_number, '''', ''), '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]', '', 'g'), ', ') AS oe_list,
                ANY_VALUE(o.name) AS representative_name,
                ANY_VALUE(o.applicability) AS representative_applicability,
                ANY_VALUE(o.category) AS representative_category
            FROM cross_references cr
            JOIN oe_data o ON cr.oe_number_norm = o.oe_number_norm
            GROUP BY cr.artikul_norm, cr.brand_norm
        ),
        AllAnalogs AS (
            -- –ê–Ω–∞–ª–æ–≥–∏ –ª–æ–≥–∏–∫–∞
            -- –ú–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å
        )
        SELECT
        """

        columns_map = [
            ("–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", 'p.artikul AS "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞"'),
            ("–ë—Ä–µ–Ω–¥", 'p.brand AS "–ë—Ä–µ–Ω–¥"'),
            ("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", 'COALESCE(pd.representative_name, p.description) AS "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"'),
            ("–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", 'COALESCE(pd.representative_applicability, "") AS "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å"'),
            ("–û–ø–∏—Å–∞–Ω–∏–µ", 'CONCAT(COALESCE(p.description, ""), dt.text) AS "–û–ø–∏—Å–∞–Ω–∏–µ"'),
            ("–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", 'COALESCE(pd.representative_category, "") AS "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞"'),
            ("–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", 'p.multiplicity AS "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"'),
            ("–î–ª–∏–Ω–Ω–∞", 'COALESCE(p.length, 0) AS "–î–ª–∏–Ω–Ω–∞"'),
            ("–®–∏—Ä–∏–Ω–∞", 'COALESCE(p.width, 0) AS "–®–∏—Ä–∏–Ω–∞"'),
            ("–í—ã—Å–æ—Ç–∞", 'COALESCE(p.height, 0) AS "–í—ã—Å–æ—Ç–∞"'),
            ("–í–µ—Å", 'COALESCE(p.weight, 0) AS "–í–µ—Å"'),
            ("–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", 'COALESCE(p.dimensions_str, "") AS "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞"'),
            ("OE –Ω–æ–º–µ—Ä", 'pd.oe_list AS "OE –Ω–æ–º–µ—Ä"'),
            ("–∞–Ω–∞–ª–æ–≥–∏", 'p.analog_list AS "–∞–Ω–∞–ª–æ–≥–∏"'),
            ("–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", 'p.image_url AS "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"')
        ]

        select_exprs = []
        if selected_columns:
            for col in selected_columns:
                for name, expr in columns_map:
                    if col == name:
                        select_exprs.append(expr)
                        break
        else:
            select_exprs = [expr for _, expr in columns_map]

        query += "\n".join(select_exprs) + "\nFROM parts_data p\n"
        query += "LEFT JOIN PartDetails pd ON p.artikul_norm = pd.artikul_norm AND p.brand_norm = pd.brand_norm\n"
        query += "LEFT JOIN DescriptionText dt ON 1=1\n"
        query += "WHERE 1=1\n"
        query += "ORDER BY p.brand, p.artikul\n"
        return query

    def export_to_csv(self, output_path, selected_columns=None):
        try:
            query = self.build_export_query(selected_columns)
            df = self.conn.execute(query).pl()

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è CSV
            for col in ["–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞", "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"]:
                if col in df.columns:
                    df = df.with_columns(
                        pl.when(pl.col(col).is_not_null()).then(pl.col(col).cast(pl.Utf8)).otherwise("").alias(col)
                    )

            buf = io.StringIO()
            df.write_csv(buf, separator=';')
            csv_bytes = buf.getvalue().encode('utf-8-sig')

            with open(output_path, 'wb') as f:
                f.write(csv_bytes)
            st.success(f"–≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {output_path}")
            return True
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}")
            return False

    # –ê–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è excel, parquet, –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å
    def export_to_excel(self, output_path, selected_columns=None):
        # –†–µ–∞–ª–∏–∑—É–µ–º —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ –ª–∏–º–∏—Ç—É
        pass

    def export_to_parquet(self, output_path, selected_columns=None):
        # –†–µ–∞–ª–∏–∑—É–µ–º
        pass

    # ===================== –í–∏–∑—É–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å =====================
    def show_export_interface(self):
        st.header("üì§ –£–º–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")
        total_records = self.conn.execute("SELECT COUNT(DISTINCT (artikul_norm, brand_norm)) FROM parts_data").fetchone()[0]
        st.info(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞: {total_records:,}")
        if total_records == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞.")
            return

        # –í—ã–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫ –∏ –ø–æ—Ä—è–¥–∫–∞
        available_columns = [
            "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", "–ë—Ä–µ–Ω–¥", "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", "–û–ø–∏—Å–∞–Ω–∏–µ",
            "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", "–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞",
            "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "OE –Ω–æ–º–µ—Ä", "–∞–Ω–∞–ª–æ–≥–∏", "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
        ]
        selected_columns = st.multiselect(
            "–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ (–ø–æ—Ä—è–¥–æ–∫ –≤–∞–∂–µ–Ω)", options=available_columns, default=available_columns
        )
        self.export_columns = selected_columns

        export_format = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞:", ["CSV", "Excel (.xlsx)", "Parquet"], index=0)

        if export_format == "CSV":
            if st.button("üöÄ –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV"):
                output_path = self.data_dir / "auto_parts_report.csv"
                self.export_to_csv(str(output_path), self.export_columns)
                with open(output_path, "rb") as f:
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å CSV", f, "auto_parts_report.csv", "text/csv")
        elif export_format == "Excel (.xlsx)":
            if st.button("üìä –≠–∫—Å–ø–æ—Ä—Ç –≤ Excel"):
                output_path = self.data_dir / "auto_parts_report.xlsx"
                # –†–µ–∞–ª–∏–∑—É–π—Ç–µ —ç–∫—Å–ø–æ—Ä—Ç
                pass
        elif export_format == "Parquet":
            if st.button("‚ö°Ô∏è –≠–∫—Å–ø–æ—Ä—Ç –≤ Parquet"):
                output_path = self.data_dir / "auto_parts_report.parquet"
                # –†–µ–∞–ª–∏–∑—É–π—Ç–µ —ç–∫—Å–ø–æ—Ä—Ç
                pass

# ===================== –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫ =====================
def main():
    catalog = AutoPartsCatalog()
    st.title("üöó AutoParts Catalog - –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è 10+ –º–ª–Ω –∑–∞–ø–∏—Å–µ–π")
    st.markdown("---")
    menu = st.sidebar.radio("–ù–∞–≤–∏–≥–∞—Ü–∏—è", ["–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö", "–≠–∫—Å–ø–æ—Ä—Ç", "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ"])

    if menu == "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö":
        st.header("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        files_ui = {}
        files_ui['oe'] = st.file_uploader("–û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (OE)", type=['xlsx', 'xls'])
        files_ui['cross'] = st.file_uploader("–ö—Ä–æ—Å—Å—ã (OE -> –ê—Ä—Ç–∏–∫—É–ª)", type=['xlsx', 'xls'])
        files_ui['barcode'] = st.file_uploader("–®—Ç—Ä–∏—Ö–∫–æ–¥—ã –∏ –∫—Ä–∞—Ç–Ω–æ—Å—Ç—å", type=['xlsx', 'xls'])
        files_ui['dimensions'] = st.file_uploader("–í–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", type=['xlsx', 'xls'])
        files_ui['images'] = st.file_uploader("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", type=['xlsx', 'xls'])

        if st.button("üöÄ –ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É"):
            file_paths = {}
            for key, uploaded in files_ui.items():
                if uploaded:
                    path = catalog.data_dir / f"{key}_{int(time.time())}_{uploaded.name}"
                    with open(path, "wb") as f:
                        f.write(uploaded.getvalue())
                    file_paths[key] = str(path)
            if file_paths:
                catalog.merge_all_data_parallel(file_paths)
                st.success("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
            else:
                st.warning("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–π–ª.")

    elif menu == "–≠–∫—Å–ø–æ—Ä—Ç":
        catalog.show_export_interface()

    elif menu == "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞":
        st.header("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        # –†–µ–∞–ª–∏–∑—É–π—Ç–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        pass

    elif menu == "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ":
        st.header("üóëÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏")
        # –†–µ–∞–ª–∏–∑—É–π—Ç–µ —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ –±—Ä–µ–Ω–¥—É –∏–ª–∏ –∞—Ä—Ç–∏–∫—É–ª—É
        pass

if __name__ == "__main__":
    main()
