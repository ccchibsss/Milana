import polars as pl
import duckdb
import streamlit as st
import os
import time
import io
import zipfile
import json
from pathlib import Path

EXCEL_ROW_LIMIT = 1_000_000

class AutoPartsCatalog:
    def __init__(self):
        self.data_dir = Path("./auto_parts_data")
        self.data_dir.mkdir(exist_ok=True)
        self.db_path = self.data_dir / "catalog.duckdb"
        self.conn = duckdb.connect(str(self.db_path))
        self._setup_database()
        self._create_indexes()
        st.set_page_config(
            page_title="AutoParts Catalog 10M+",
            layout="wide",
            page_icon="üöó"
        )

    def _setup_database(self):
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR,
                category VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE,
                width DOUBLE,
                height DOUBLE,
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                price_with_markup DOUBLE,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS recommended_prices (
                artikul_norm VARCHAR PRIMARY KEY,
                price DOUBLE
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS price_list (
                artikul VARCHAR,
                brand VARCHAR,
                quantity INTEGER,
                price DOUBLE,
                PRIMARY KEY (artikul, brand)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS markup_settings (
                id INTEGER PRIMARY KEY,
                total_markup DOUBLE,
                brand_markup JSON
            )
        """)
        if not self.conn.execute("SELECT 1 FROM markup_settings").fetchone():
            self.conn.execute("INSERT INTO markup_settings (id, total_markup, brand_markup) VALUES (1, 0, '{}')")

    def _create_indexes(self):
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_oe ON oe_data(oe_number_norm)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_parts ON parts_data(artikul_norm, brand_norm)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_cross ON cross_references(oe_number_norm)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_cross_art ON cross_references(artikul_norm, brand_norm)")

    @staticmethod
    def normalize_key(series: pl.Series) -> pl.Series:
        return (
            series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
            .str.to_lowercase()
        )

    @staticmethod
    def clean_values(series: pl.Series) -> pl.Series:
        return (
            series
            .fill_null("")
            .cast(pl.Utf8)
            .str.replace_all("'", "")
            .str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\-\s]", "")
            .str.replace_all(r"\s+", " ")
            .str.strip_chars()
        )

    def detect_columns(self, actual_cols, expected_cols):
        mapping = {}
        col_variants = {
            'oe_number': ['oe –Ω–æ–º–µ—Ä', 'oe', '–æe', '–Ω–æ–º–µ—Ä', 'code', 'OE'],
            'artikul': ['–∞—Ä—Ç–∏–∫—É–ª', 'article', 'sku'],
            'brand': ['–±—Ä–µ–Ω–¥', 'brand', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', 'manufacturer'],
            'name': ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ', 'name', '–æ–ø–∏—Å–∞–Ω–∏–µ', 'description'],
            'applicability': ['–ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', 'vehicle', 'applicability'],
            'barcode': ['—à—Ç—Ä–∏—Ö-–∫–æ–¥', 'barcode', '—à—Ç—Ä–∏—Ö–∫–æ–¥', 'ean', 'eac13'],
            'multiplicity': ['–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å —à—Ç', '–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å', 'multiplicity'],
            'length': ['–¥–ª–∏–Ω–∞ (—Å–º)', '–¥–ª–∏–Ω–∞', 'length', '–¥–ª–∏–Ω–Ω–∞'],
            'width': ['—à–∏—Ä–∏–Ω–∞ (—Å–º)', '—à–∏—Ä–∏–Ω–∞', 'width'],
            'height': ['–≤—ã—Å–æ—Ç–∞ (—Å–º)', '–≤—ã—Å–æ—Ç–∞', 'height'],
            'weight': ['–≤–µ—Å (–∫–≥)', '–≤–µ—Å, –∫–≥', '–≤–µ—Å', 'weight'],
            'image_url': ['—Å—Å—ã–ª–∫–∞', 'url', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 'image', '–∫–∞—Ä—Ç–∏–Ω–∫–∞'],
            'dimensions_str': ['–≤–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã', '—Ä–∞–∑–º–µ—Ä—ã', 'dimensions', 'size']
        }
        actual_lower = {c.lower(): c for c in actual_cols}
        for exp_col in expected_cols:
            variants = [v.lower() for v in col_variants.get(exp_col, [exp_col])]
            for var in variants:
                for act_lower, act_orig in actual_lower.items():
                    if var in act_lower:
                        mapping[act_orig] = exp_col
                        break
                if exp_col in mapping.values():
                    break
        return mapping

    def read_and_prepare_file(self, path, file_type):
        try:
            df = pl.read_excel(path, engine='calamine')
            if df.is_empty():
                return pl.DataFrame()
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {path}: {e}")
            return pl.DataFrame()
        schemas = {
            'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
            'barcode': ['brand', 'artikul', 'barcode', 'multiplicity'],
            'dimensions': ['artikul', 'brand', 'length', 'width', 'height', 'weight', 'dimensions_str'],
            'images': ['artikul', 'brand', 'image_url'],
            'cross': ['oe_number', 'artikul', 'brand']
        }
        expected_cols = schemas.get(file_type, [])
        col_mapping = self.detect_columns(df.columns, expected_cols)
        if not col_mapping:
            return pl.DataFrame()
        df = df.rename(col_mapping)
        if 'artikul' in df.columns:
            df = df.with_columns(artikul=self.clean_values(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand=self.clean_values(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number=self.clean_values(pl.col('oe_number')))
        key_cols = [c for c in ['oe_number', 'artikul', 'brand'] if c in df.columns]
        if key_cols:
            df = df.unique(subset=key_cols, keep='first')
        if 'artikul' in df.columns:
            df = df.with_columns(artikul_norm=self.normalize_key(pl.col('artikul')))
        if 'brand' in df.columns:
            df = df.with_columns(brand_norm=self.normalize_key(pl.col('brand')))
        if 'oe_number' in df.columns:
            df = df.with_columns(oe_number_norm=self.normalize_key(pl.col('oe_number')))
        return df

    def upsert_data(self, table_name, df, pk):
        if df.is_empty():
            return
        df_unique = df.unique(keep='first')
        self.conn.register(f"temp_{table_name}_{int(time.time())}", df_unique.to_arrow())
        pk_str = ", ".join([f'"{col}"' for col in pk])
        update_cols = [col for col in df_unique.columns if col not in pk]
        if update_cols:
            update_clause = ", ".join([f'"{col}"=excluded."{col}"' for col in update_cols])
            conflict_action = f"DO UPDATE SET {update_clause}"
        else:
            conflict_action = "DO NOTHING"
        sql = f"""
            INSERT INTO {table_name}
            SELECT * FROM "temp_{table_name}_{int(time.time())}"
            ON CONFLICT ({pk_str}) {conflict_action}
        """
        try:
            self.conn.execute(sql)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ {table_name}: {e}")
        finally:
            self.conn.unregister(f"temp_{table_name}_{int(time.time())}")

    def process_and_load_data(self, dataframes):
        st.info("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        total_steps = 3
        progress = st.progress(0)
        step_idx = 0

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ OE
        if 'oe' in dataframes:
            step_idx += 1
            progress.progress(step_idx / total_steps, text=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ OE ({step_idx}/{total_steps})")
            df_oe = dataframes['oe'].filter(pl.col('oe_number_norm') != "")
            oe_df = df_oe.select(['oe_number_norm', 'oe_number', 'name', 'applicability']).unique(subset=['oe_number_norm'])
            if 'name' in oe_df.columns:
                oe_df = oe_df.with_columns(self.determine_category_vectorized(pl.col('name')))
            else:
                oe_df = oe_df.with_columns(category=pl.lit('–†–∞–∑–Ω–æ–µ'))
            self.upsert_data('oe_data', oe_df, ['oe_number_norm'])
            cross_df = df_oe.filter(pl.col('artikul_norm') != "").select(['oe_number_norm', 'artikul_norm', 'brand_norm']).unique()
            self.upsert_data('cross_references', cross_df, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ cross
        if 'cross' in dataframes:
            step_idx += 1
            progress.progress(step_idx / total_steps, f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫—Ä–æ—Å—Å–æ–≤ ({step_idx}/{total_steps})")
            df_cross = dataframes['cross'].filter(
                (pl.col('oe_number_norm') != "") & (pl.col('artikul_norm') != "")
            )
            self.upsert_data('cross_references', df_cross, ['oe_number_norm', 'artikul_norm', 'brand_norm'])

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ parts
        step_idx += 1
        progress.progress(step_idx / total_steps, f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—Ä—Ç–∏–∫—É–ª–∞ ({step_idx}/{total_steps})")
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∞—Ä—Ç–∏–∫—É–ª–∞
        parts_df = None
        files_order = ['oe', 'barcode', 'images', 'dimensions']
        key_files = {f: dataframes[f] for f in files_order if f in dataframes}
        if key_files:
            combined = pl.concat([df.select(['artikul', 'artikul_norm', 'brand', 'brand_norm']) for df in key_files.values()]).filter(pl.col('artikul_norm') != "").unique(subset=['artikul', 'brand'])
            parts_df = combined

            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ñ–∞–π–ª–∞–º
            for ftype in files_order:
                df = key_files.get(ftype)
                if not df or df.is_empty():
                    continue
                if 'artikul_norm' not in df.columns:
                    continue
                join_cols = [col for col in df.columns if col not in ['artikul', 'artikul_norm', 'brand', 'brand_norm']]
                existing_cols = set(parts_df.columns)
                join_cols = [col for col in join_cols if col not in existing_cols]
                if not join_cols:
                    continue
                df_select = df.select(['artikul_norm', 'brand_norm'] + join_cols).unique(subset=['artikul_norm', 'brand_norm'])
                parts_df = parts_df.join(df_select, on=['artikul_norm', 'brand_norm'], how='left', coalesce=True)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤, –æ–ø–∏—Å–∞–Ω–∏–µ
        if parts_df and not parts_df.is_empty():
            # multiplicity
            if 'multiplicity' not in parts_df.columns:
                parts_df = parts_df.with_columns(pl.lit(1).cast(pl.Int32).alias('multiplicity'))
            else:
                parts_df = parts_df.with_columns(pl.col('multiplicity').fill_null(1).cast(pl.Int32))
            # —Ä–∞–∑–º–µ—Ä—ã
            for c in ['length', 'width', 'height']:
                if c not in parts_df.columns:
                    parts_df = parts_df.with_columns(pl.lit(None).cast(pl.Float64).alias(c))
            if 'dimensions_str' not in parts_df.columns:
                parts_df = parts_df.with_columns(dimensions_str=pl.lit(None).cast(pl.Utf8))
            # —Å–æ–∑–¥–∞–µ–º dimensions_str
            parts_df = parts_df.with_columns([
                pl.col('length').cast(pl.Utf8).fill_null('').alias('_length_str'),
                pl.col('width').cast(pl.Utf8).fill_null('').alias('_width_str'),
                pl.col('height').cast(pl.Utf8).fill_null('').alias('_height_str')
            ])
            parts_df = parts_df.with_columns(
                pl.when(
                    (pl.col('dimensions_str').is_not_null()) &
                    (pl.col('dimensions_str') != '') &
                    (pl.col('dimensions_str').cast(pl.Utf8).str.upper() != 'XX')
                )
                .then(pl.col('dimensions_str'))
                .otherwise(
                    pl.concat_str([pl.col('_length_str'), pl.lit('x'), pl.col('_width_str'), pl.lit('x'), pl.col('_height_str')], separator='')
                ).alias('dimensions_str')
            )
            parts_df = parts_df.drop(['_length_str', '_width_str', '_height_str'])

            # –æ–ø–∏—Å–∞–Ω–∏–µ
            if 'artikul' not in parts_df.columns:
                parts_df = parts_df.with_columns(artikul=pl.lit(''))
            if 'brand' not in parts_df.columns:
                parts_df = parts_df.with_columns(brand=pl.lit(''))
            parts_df = parts_df.with_columns([
                pl.col('artikul').cast(pl.Utf8).fill_null('').alias('_artikul'),
                pl.col('brand').cast(pl.Utf8).fill_null('').alias('_brand'),
                pl.col('multiplicity').cast(pl.Utf8).alias('_multiplicity')
            ])
            parts_df = parts_df.with_columns(
                pl.concat_str([
                    '–ê—Ä—Ç–∏–∫—É–ª: ', pl.col('_artikul'),
                    ', –ë—Ä–µ–Ω–¥: ', pl.col('_brand'),
                    ', –ö—Ä–∞—Ç–Ω–æ—Å—Ç—å: ', pl.col('_multiplicity'), ' —à—Ç.'
                ], separator='').alias('description')
            )
            parts_df = parts_df.drop(['_artikul', '_brand', '_multiplicity'])

            # –í —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫
            final_cols = [
                'artikul_norm', 'brand_norm', 'artikul', 'brand', 'multiplicity', 'barcode',
                'length', 'width', 'height', 'weight', 'image_url', 'dimensions_str', 'description'
            ]
            select_exprs = [pl.col(c) for c in final_cols if c in parts_df.columns]
            parts_df = parts_df.select(select_exprs)

            # –í—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ü–µ–Ω–∞–º–∏
            self.upsert_data('parts_data', parts_df, ['artikul_norm', 'brand_norm'])

        progress.progress(1.0)
        time.sleep(1)
        st.success("üíæ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

    def determine_category_vectorized(self, series):
        categories = {
            '–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä': '–ê–≤—Ç–æ—ç–ª–µ–∫—Ç—Ä–∏–∫–∞',
            '—Ñ–∏–ª—å—Ç—Ä': '–§–∏–ª—å—Ç—Ä—ã',
            '—Å–≤–µ—á–∞': '–ê–≤—Ç–æ—ç–ª–µ–∫—Ç—Ä–∏–∫–∞',
            '–º–∞—Å–ª–æ': '–ú–∞—Å–ª–∞',
            '—Ç–æ—Ä–º–æ–∑': '–¢–æ—Ä–º–æ–∑–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã'
        }
        def category_for_name(name):
            name_lower = name.lower()
            for key, cat in categories.items():
                if key in name_lower:
                    return cat
            return '–†–∞–∑–Ω–æ–µ'
        return series.apply(category_for_name)

    def merge_all_data_parallel(self, paths: dict):
        start_time = time.time()
        import concurrent.futures
        dataframes = {}
        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = {
                executor.submit(self.read_and_prepare_file, p, t): t
                for t, p in paths.items()
            }
            for f in concurrent.futures.as_completed(futures):
                t = futures[f]
                df = f.result()
                if not df.is_empty():
                    dataframes[t] = df
        if not dataframes:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
            return
        self.process_and_load_data(dataframes)
        st.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫.")

    def get_total_records(self):
        try:
            return self.conn.execute("SELECT COUNT(*) FROM parts_data").fetchone()[0]
        except:
            return 0

    def get_statistics(self):
        stats = {}
        try:
            stats['total_parts'] = self.get_total_records()
            if stats['total_parts'] == 0:
                return {
                    'total_parts': 0, 'total_oe': 0, 'total_brands': 0,
                    'top_brands': None, 'categories': None
                }
            stats['total_oe'] = self.conn.execute("SELECT COUNT(*) FROM oe_data").fetchone()[0]
            stats['total_brands'] = self.conn.execute("SELECT COUNT(DISTINCT brand) FROM parts_data WHERE brand IS NOT NULL").fetchone()[0]
            brs = self.conn.execute("SELECT brand, COUNT(*) FROM parts_data WHERE brand IS NOT NULL GROUP BY brand ORDER BY COUNT(*) DESC LIMIT 10").fetchall()
            stats['top_brands'] = pl.DataFrame(brs, schema=["brand", "count"])
            cats = self.conn.execute("SELECT category, COUNT(*) FROM oe_data WHERE category IS NOT NULL GROUP BY category ORDER BY COUNT(*) DESC").fetchall()
            stats['categories'] = pl.DataFrame(cats, schema=["category", "count"])
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            stats = {}
        return stats

    def load_price_recommendation(self, file_bytes):
        df = pl.read_excel(io.BytesIO(file_bytes))
        if '–∞—Ä—Ç–∏–∫—É–ª' not in df.columns or '—Ü–µ–Ω–∞' not in df.columns:
            st.error("–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å '–∞—Ä—Ç–∏–∫—É–ª' –∏ '—Ü–µ–Ω–∞'")
            return
        df = df.select([
            pl.col('–∞—Ä—Ç–∏–∫—É–ª').alias('artikul'),
            pl.col('—Ü–µ–Ω–∞').cast(pl.Float64)
        ])
        for row in df.iter_rows():
            artikul, price = row
            norm_series = self.normalize_key(pl.Series([artikul]))
            artikul_norm = norm_series[0] if len(norm_series) > 0 else ''
            self.conn.execute("""
                INSERT INTO recommended_prices (artikul_norm, price)
                VALUES (?, ?)
                ON CONFLICT (artikul_norm) DO UPDATE SET price=excluded.price
            """, [artikul_norm, price])
        st.success("–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ —Ü–µ–Ω—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")

    def load_price_list(self, file_bytes):
        df = pl.read_excel(io.BytesIO(file_bytes))
        required_cols = ['–∞—Ä—Ç–∏–∫—É–ª', '–±—Ä–µ–Ω–¥', '–∫–æ–ª-–≤–æ', '—Ü–µ–Ω–∞']
        if not all(c in df.columns for c in required_cols):
            st.error("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–æ–∫ '–∞—Ä—Ç–∏–∫—É–ª', '–±—Ä–µ–Ω–¥', '–∫–æ–ª-–≤–æ', '—Ü–µ–Ω–∞'")
            return
        df = df.select([
            pl.col('–∞—Ä—Ç–∏–∫—É–ª'),
            pl.col('–±—Ä–µ–Ω–¥'),
            pl.col('–∫–æ–ª-–≤–æ').cast(pl.Int32),
            pl.col('—Ü–µ–Ω–∞').cast(pl.Float64)
        ])
        for row in df.iter_rows():
            artikul, brand, qty, price = row
            norm_artikul = self.normalize_key(pl.Series([artikul]))[0]
            norm_brand = self.normalize_key(pl.Series([brand]))[0]
            self.conn.execute("""
                INSERT INTO price_list (artikul, brand, quantity, price)
                VALUES (?, ?, ?, ?)
                ON CONFLICT (artikul, brand) DO UPDATE SET quantity=excluded.quantity, price=excluded.price
            """, [artikul, brand, qty, price])
        st.success("–ü—Ä–∞–π—Å-–ª–∏—Å—Ç –∑–∞–≥—Ä—É–∂–µ–Ω.")

    def set_markups(self, total_markup, brand_markups):
        self.conn.execute("""
            UPDATE markup_settings SET total_markup=?, brand_markup=?
            WHERE id=1
        """, [total_markup, json.dumps(brand_markups)])
        st.success("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞—Ü–µ–Ω–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã.")

    def get_markups(self):
        row = self.conn.execute("SELECT total_markup, brand_markup FROM markup_settings WHERE id=1").fetchone()
        if row:
            total_markup, brand_markup_json = row
            brand_markup = json.loads(brand_markup_json) if brand_markup_json else {}
            return total_markup, brand_markup
        return 0, {}

    def apply_markup(self, price, brand_norm=''):
        total_markup, brand_markup = self.get_markups()
        markup = total_markup
        if brand_norm and brand_norm in brand_markup:
            markup += brand_markup[brand_norm]
        return price * (1 + markup / 100)

    def build_export_query(self, selected_columns=None):
        desc_text = """–°–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: –Ω–æ–≤—ã–π (–≤ —É–ø–∞–∫–æ–≤–∫–µ).
–í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–≤—Ç–æ—Ç–æ–≤–∞—Ä—ã ‚Äî –Ω–∞–¥–µ–∂–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è. 
–û–±–µ—Å–ø–µ—á—å—Ç–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –¥–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç—å –∏ –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–∞—à–µ–≥–æ –∞–≤—Ç–æ —Å –ø–æ–º–æ—â—å—é –Ω–∞—à–µ–≥–æ —à–∏—Ä–æ–∫–æ–≥–æ –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–µ–π.

–í –Ω–∞—à–µ–º –∫–∞—Ç–∞–ª–æ–≥–µ –≤—ã –Ω–∞–π–¥–µ—Ç–µ —Ç–æ—Ä–º–æ–∑–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, —Ñ–∏–ª—å—Ç—Ä—ã (–º–∞—Å–ª—è–Ω—ã–µ, –≤–æ–∑–¥—É—à–Ω—ã–µ, —Å–∞–ª–æ–Ω–Ω—ã–µ), —Å–≤–µ—á–∏ –∑–∞–∂–∏–≥–∞–Ω–∏—è, —Ä–∞—Å—Ö–æ–¥–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –∞–≤—Ç–æ—Ö–∏–º–∏—é, —ç–ª–µ–∫—Ç—Ä–∏–∫—É, –∞–≤—Ç–æ–º–∞—Å–ª–∞, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∞ —Ç–∞–∫–∂–µ –¥—Ä—É–≥–∏–µ –∫–æ–º–ø–ª–µ–∫—Ç—É—é—â–∏–µ, –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. 

–ú—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –±—ã—Å—Ç—Ä—É—é –¥–æ—Å—Ç–∞–≤–∫—É, –≤—ã–≥–æ–¥–Ω—ã–µ —Ü–µ–Ω—ã –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –¥–ª—è –ª—é–±–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ ‚Äî –∞–≤—Ç–æ–ª—é–±–∏—Ç–µ–ª—è, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –∏–ª–∏ –∞–≤—Ç–æ—Å–µ—Ä–≤–∏—Å–∞. 

–í—ã–±–∏—Ä–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –ª—É—á—à–µ–µ ‚Äî –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç –≤–µ–¥—É—â–∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π."""
        ctes = f"""
        WITH DescriptionTemplate AS (
            SELECT CHR(10) || CHR(10) || $${desc_text}$$ AS text
        ),
        PartDetails AS (
            SELECT
                cr.artikul_norm,
                cr.brand_norm,
                STRING_AGG(DISTINCT regexp_replace(regexp_replace(o.oe_number, '''', ''), '[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]', '', 'g'), ', ') AS oe_list,
                ANY_VALUE(o.name) AS representative_name,
                ANY_VALUE(o.applicability) AS representative_applicability,
                ANY_VALUE(o.category) AS representative_category
            FROM cross_references cr
            JOIN oe_data o ON cr.oe_number_norm = o.oe_number_norm
            GROUP BY cr.artikul_norm, cr.brand_norm
        ),
        AllAnalogs AS (
            -- –ê–Ω–∞–ª–æ–≥–∏ –ª–æ–≥–∏–∫–∞
            SELECT * FROM (VALUES (NULL))
        ),
        -- –æ—Å—Ç–∞–ª—å–Ω—ã–µ CTE –æ–ø—É—â–µ–Ω—ã –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏, –≤—Å—Ç–∞–≤—å—Ç–µ —Å—é–¥–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ–ª–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        RankedData AS (
            -- –ª–æ–≥–∏–∫–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        )
        """
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
        if not selected_columns:
            select_exprs = [
                'p.artikul AS "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞"',
                'p.brand AS "–ë—Ä–µ–Ω–¥"',
                'COALESCE(p.representative_name, p.analog_representative_name) AS "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"',
                'COALESCE(p.representative_applicability, p.analog_representative_applicability) AS "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å"',
                'CONCAT(COALESCE(p.description, \'\'), dt.text) AS "–û–ø–∏—Å–∞–Ω–∏–µ"',
                'COALESCE(p.representative_category, p.analog_representative_category) AS "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞"',
                'p.multiplicity AS "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"',
                'COALESCE(p.length, p.analog_length) AS "–î–ª–∏–Ω–Ω–∞"',
                'COALESCE(p.width, p.analog_width) AS "–®–∏—Ä–∏–Ω–∞"',
                'COALESCE(p.height, p.analog_height) AS "–í—ã—Å–æ—Ç–∞"',
                'COALESCE(p.weight, p.analog_weight) AS "–í–µ—Å"',
                'COALESCE(CASE WHEN p.dimensions_str IS NULL OR p.dimensions_str = \'\' OR UPPER(TRIM(p.dimensions_str)) = \'XX\' THEN NULL ELSE p.dimensions_str END, p.analog_dimensions_str) AS "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞"',
                'p.oe_list AS "OE –Ω–æ–º–µ—Ä"',
                'p.analog_list AS "–∞–Ω–∞–ª–æ–≥–∏"',
                'p.image_url AS "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"',
                'p.price_with_markup AS "–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π"'
            ]
        else:
            select_exprs = []
            for col_name in selected_columns:
                if col_name == "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞":
                    select_exprs.append('p.artikul AS "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞"')
                elif col_name == "–ë—Ä–µ–Ω–¥":
                    select_exprs.append('p.brand AS "–ë—Ä–µ–Ω–¥"')
                elif col_name == "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ":
                    select_exprs.append('COALESCE(p.representative_name, p.analog_representative_name) AS "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"')
                elif col_name == "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å":
                    select_exprs.append('COALESCE(p.representative_applicability, p.analog_representative_applicability) AS "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å"')
                elif col_name == "–û–ø–∏—Å–∞–Ω–∏–µ":
                    select_exprs.append('CONCAT(COALESCE(p.description, \'\'), dt.text) AS "–û–ø–∏—Å–∞–Ω–∏–µ"')
                elif col_name == "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞":
                    select_exprs.append('COALESCE(p.representative_category, p.analog_representative_category) AS "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞"')
                elif col_name == "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å":
                    select_exprs.append('p.multiplicity AS "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"')
                elif col_name == "–î–ª–∏–Ω–Ω–∞":
                    select_exprs.append('COALESCE(p.length, p.analog_length) AS "–î–ª–∏–Ω–Ω–∞"')
                elif col_name == "–®–∏—Ä–∏–Ω–∞":
                    select_exprs.append('COALESCE(p.width, p.analog_width) AS "–®–∏—Ä–∏–Ω–∞"')
                elif col_name == "–í—ã—Å–æ—Ç–∞":
                    select_exprs.append('COALESCE(p.height, p.analog_height) AS "–í—ã—Å–æ—Ç–∞"')
                elif col_name == "–í–µ—Å":
                    select_exprs.append('COALESCE(p.weight, p.analog_weight) AS "–í–µ—Å"')
                elif col_name == "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞":
                    select_exprs.append('COALESCE(CASE WHEN p.dimensions_str IS NULL OR p.dimensions_str = \'\' OR UPPER(TRIM(p.dimensions_str)) = \'XX\' THEN NULL ELSE p.dimensions_str END, p.analog_dimensions_str) AS "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞"')
                elif col_name == "OE –Ω–æ–º–µ—Ä":
                    select_exprs.append('p.oe_list AS "OE –Ω–æ–º–µ—Ä"')
                elif col_name == "–∞–Ω–∞–ª–æ–≥–∏":
                    select_exprs.append('p.analog_list AS "–∞–Ω–∞–ª–æ–≥–∏"')
                elif col_name == "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ":
                    select_exprs.append('p.image_url AS "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"')
                elif col_name == "–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π":
                    select_exprs.append('p.price_with_markup AS "–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π"')
        select_clause = ", ".join(select_exprs)
        query = f"""
        {ctes}
        SELECT {select_clause}
        FROM RankedData p
        CROSS JOIN DescriptionTemplate dt
        WHERE p.rn=1
        ORDER BY p.brand, p.artikul
        """
        return query

    def export_csv(self, output_path, selected_columns=None, exclude_names=None):
        total = self.conn.execute(
            "SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)"
        ).fetchone()[0]
        if total == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False
        query = self.build_export_query(selected_columns)
        df = self.conn.execute(query).pl()

        if exclude_names:
            pattern = '|'.join(exclude_names)
            df = df.filter(~pl.col("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ").str.contains(pattern, case=False))
        if '–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π' in df.columns:
            df = df.with_columns(
                pl.col('–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π').apply(lambda p: self.apply_markup(p)).alias('–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π')
            )
        for c in ["–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞", "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å"]:
            if c in df.columns:
                df = df.with_columns(
                    pl.when(pl.col(c).is_not_null())
                    .then(pl.col(c).cast(pl.Utf8))
                    .otherwise("")
                    .alias(c)
                )
        buf = io.StringIO()
        df.write_csv(buf, separator=';')
        with open(output_path, 'wb') as f:
            f.write(b'\xef\xbb\xbf')
            f.write(buf.getvalue().encode('utf-8'))
        st.success(f"–≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {output_path}")
        return True

    def export_excel(self, output_path, selected_columns=None, exclude_names=None):
        total = self.conn.execute(
            "SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)"
        ).fetchone()[0]
        if total == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False, None
        num_files = (total + EXCEL_ROW_LIMIT - 1) // EXCEL_ROW_LIMIT
        progress = st.progress(0)
        files = []
        base_query = self.build_export_query(selected_columns)
        for i in range(num_files):
            offset = i * EXCEL_ROW_LIMIT
            query = f"{base_query} LIMIT {EXCEL_ROW_LIMIT} OFFSET {offset}"
            df = self.conn.execute(query).pl()

            if exclude_names:
                pattern = '|'.join(exclude_names)
                df = df.filter(~pl.col("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ").str.contains(pattern, case=False))
            if '–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π' in df.columns:
                df = df.with_columns(
                    pl.col('–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π').apply(lambda p: self.apply_markup(p)).alias('–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π')
                )
            fname = output_path.with_name(f"{output_path.stem}_part_{i+1}.xlsx")
            df.write_excel(str(fname))
            files.append(fname)
            progress.progress((i+1)/num_files)
        if len(files) > 1:
            zip_path = output_path.with_suffix('.zip')
            with zipfile.ZipFile(zip_path, 'w') as zf:
                for f in files:
                    zf.write(f, arcname=f.name)
                    os.remove(f)
            final_path = zip_path
        else:
            final_path = files[0]
            if final_path != output_path:
                os.rename(final_path, output_path)
                final_path = output_path
        st.success(f"–≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {final_path}")
        return True, final_path

    def export_parquet(self, output_path, selected_columns=None, exclude_names=None):
        total = self.conn.execute(
            "SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)"
        ).fetchone()[0]
        if total == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return False
        query = self.build_export_query(selected_columns)
        df = self.conn.execute(query).pl()

        if exclude_names:
            pattern = '|'.join(exclude_names)
            df = df.filter(~pl.col("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ").str.contains(pattern, case=False))
        if '–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π' in df.columns:
            df = df.with_columns(
                pl.col('–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π').apply(lambda p: self.apply_markup(p)).alias('–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π')
            )
        df.write_parquet(output_path)
        st.success(f"–≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {output_path}")
        return True

    def apply_markup(self, price):
        total, brand = self.get_markups()
        markup = total
        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –ø–æ –±—Ä–µ–Ω–¥–∞–º
        return price * (1 + markup / 100)

    def show_export_interface(self):
        st.header("üì§ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")
        total = self.conn.execute(
            "SELECT COUNT(*) FROM (SELECT DISTINCT artikul_norm, brand_norm FROM parts_data)"
        ).fetchone()[0]
        if total == 0:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞.")
            return
        options = [
            "–ê—Ä—Ç–∏–∫—É–ª –±—Ä–µ–Ω–¥–∞", "–ë—Ä–µ–Ω–¥", "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", "–û–ø–∏—Å–∞–Ω–∏–µ",
            "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", "–î–ª–∏–Ω–Ω–∞", "–®–∏—Ä–∏–Ω–∞", "–í—ã—Å–æ—Ç–∞",
            "–í–µ—Å", "–î–ª–∏–Ω–Ω–∞/–®–∏—Ä–∏–Ω–∞/–í—ã—Å–æ—Ç–∞", "OE –Ω–æ–º–µ—Ä", "–∞–Ω–∞–ª–æ–≥–∏", "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π"
        ]
        selected_columns = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞", options=options, default=options)
        exclude_input = st.text_input("–ò—Å–∫–ª—é—á–∏—Ç—å –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —á–µ—Ä–µ–∑ |")
        exclude_names = [n.strip() for n in exclude_input.split('|')] if exclude_input else []

        format_opt = st.radio("–§–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞", ["CSV", "Excel (.xlsx)", "Parquet"], index=0)

        st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞—Ü–µ–Ω–∫–∏")
        total_markup = st.slider("–û–±—â–∞—è –Ω–∞—Ü–µ–Ω–∫–∞ (%)", 0, 100, 0)
        brand_markups = {}
        if st.checkbox("–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –Ω–∞—Ü–µ–Ω–∫–∏ –ø–æ –±—Ä–µ–Ω–¥–∞–º"):
            brands = self.conn.execute("SELECT DISTINCT brand, brand_norm FROM parts_data WHERE brand IS NOT NULL").fetchall()
            for b, bn in brands:
                mark = st.slider(f"–ù–∞—Ü–µ–Ω–∫–∞ –¥–ª—è –±—Ä–µ–Ω–¥–∞ '{b}'", 0, 100, 0)
                brand_markups[bn] = mark
        if st.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"):
            self.set_markups(total_markup, brand_markups)

        if st.button("üöÄ –ù–∞—á–∞—Ç—å —ç–∫—Å–ø–æ—Ä—Ç"):
            output_path = self.data_dir / "auto_parts_export"
            if format_opt == "CSV":
                out_file = output_path.with_suffix('.csv')
                self.export_csv(str(out_file), selected_columns, exclude_names)
                with open(out_file, "rb") as f:
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å CSV", f, "auto_parts_report.csv", "text/csv")
            elif format_opt == "Excel (.xlsx)":
                out_file = output_path.with_suffix('.xlsx')
                self.export_excel(out_file, selected_columns, exclude_names)
                with open(out_file, "rb") as f:
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å XLSX", f, out_file.name, "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
            elif format_opt == "Parquet":
                out_file = str(output_path.with_suffix('.parquet'))
                self.export_parquet(out_file, selected_columns, exclude_names)
                with open(out_file, "rb") as f:
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å Parquet", f, "auto_parts_report.parquet", "application/octet-stream")

    def delete_by_brand(self, brand_norm):
        try:
            count = self.conn.execute("SELECT COUNT(*) FROM parts_data WHERE brand_norm=?", [brand_norm]).fetchone()[0]
            self.conn.execute("DELETE FROM parts_data WHERE brand_norm=?", [brand_norm])
            self.conn.execute("DELETE FROM cross_references WHERE (artikul_norm, brand_norm) NOT IN (SELECT artikul_norm, brand_norm FROM parts_data)")
            return count
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –ø–æ –±—Ä–µ–Ω–¥—É: {e}")
            return 0

    def delete_by_artikul(self, artikul_norm):
        try:
            count = self.conn.execute("SELECT COUNT(*) FROM parts_data WHERE artikul_norm=?", [artikul_norm]).fetchone()[0]
            self.conn.execute("DELETE FROM parts_data WHERE artikul_norm=?", [artikul_norm])
            self.conn.execute("DELETE FROM cross_references WHERE (artikul_norm, brand_norm) NOT IN (SELECT artikul_norm, brand_norm FROM parts_data)")
            return count
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –ø–æ –∞—Ä—Ç–∏–∫—É–ª–∞: {e}")
            return 0

# --- –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å ---
def main():
    st.title("üöó AutoParts Catalog - 10+ –º–ª–Ω –∑–∞–ø–∏—Å–µ–π")
    st.markdown("–ú–æ—â–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–µ–π.")
    catalog = AutoPartsCatalog()

    menu = st.sidebar.radio("–ú–µ–Ω—é", ["–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö", "–≠–∫—Å–ø–æ—Ä—Ç", "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ"])

    if menu == "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö":
        st.subheader("–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤")
        col1, col2 = st.columns(2)
        with col1:
            file_oe = st.file_uploader("–û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (OE)", type=['xlsx', 'xls'])
            file_cross = st.file_uploader("–ö—Ä–æ—Å—Å—ã (OE -> –ê—Ä—Ç–∏–∫—É–ª)", type=['xlsx', 'xls'])
            file_barcode = st.file_uploader("–®—Ç—Ä–∏—Ö-–∫–æ–¥—ã", type=['xlsx', 'xls'])
        with col2:
            file_dim = st.file_uploader("–í–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã", type=['xlsx', 'xls'])
            file_img = st.file_uploader("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", type=['xlsx', 'xls'])
        files_map = {
            'oe': file_oe,
            'cross': file_cross,
            'barcode': file_barcode,
            'dimensions': file_dim,
            'images': file_img
        }
        if st.button("üöÄ –ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É"):
            paths = {}
            for ftype, uploaded in files_map.items():
                if uploaded:
                    filename = f"{ftype}_{int(time.time())}_{uploaded.name}"
                    path = catalog.data_dir / filename
                    with open(path, "wb") as f:
                        f.write(uploaded.getvalue())
                    paths[ftype] = str(path)
            if paths:
                catalog.merge_all_data_parallel(paths)
            else:
                st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–π–ª.")
    elif menu == "–≠–∫—Å–ø–æ—Ä—Ç":
        catalog.show_export_interface()
    elif menu == "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞":
        st.header("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        stats = catalog.get_statistics()
        st.metric("–ê—Ä—Ç–∏–∫—É–ª–æ–≤", stats.get('total_parts', 0))
        st.metric("OE", stats.get('total_oe', 0))
        st.metric("–ë—Ä–µ–Ω–¥–æ–≤", stats.get('total_brands', 0))
        st.subheader("–¢–æ–ø –±—Ä–µ–Ω–¥–æ–≤")
        st.dataframe(stats.get('top_brands', None))
        st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
        st.bar_chart(stats.get('categories', None))
    elif menu == "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ":
        st.header("üóëÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
        op = st.radio("–î–µ–π—Å—Ç–≤–∏–µ", ["–£–¥–∞–ª–∏—Ç—å –ø–æ –±—Ä–µ–Ω–¥—É", "–£–¥–∞–ª–∏—Ç—å –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É"])
        if op == "–£–¥–∞–ª–∏—Ç—å –ø–æ –±—Ä–µ–Ω–¥—É":
            brands = catalog.conn.execute("SELECT DISTINCT brand, brand_norm FROM parts_data WHERE brand IS NOT NULL").fetchall()
            if brands:
                b_list = [b for b, bn in brands]
                selected_b = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –±—Ä–µ–Ω–¥", b_list)
                bn_row = catalog.conn.execute("SELECT brand_norm FROM parts_data WHERE brand=?", [selected_b]).fetchone()
                bn = bn_row[0] if bn_row else ''
                count_del = catalog.delete_by_brand(bn)
                st.success(f"–£–¥–∞–ª–µ–Ω–æ {count_del} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –±—Ä–µ–Ω–¥–∞ {selected_b}")
            else:
                st.info("–ù–µ—Ç –±—Ä–µ–Ω–¥–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è.")
        else:
            arti = st.text_input("–ê—Ä—Ç–∏–∫—É–ª –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
            if arti:
                norm_series = catalog.normalize_key(pl.Series([arti]))
                arti_norm = norm_series[0] if len(norm_series) > 0 else ''
                count_del = catalog.delete_by_artikul(arti_norm)
                st.success(f"–£–¥–∞–ª–µ–Ω–æ {count_del} –∑–∞–ø–∏—Å–µ–π –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É {arti}")

if __name__ == "__main__":
    main()
